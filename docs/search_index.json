[["index.html", "OPTAIN CS-10 Chapter 1 Introduction", " OPTAIN CS-10 Moritz Shore, Csilla Farkas 2023-05-23 Chapter 1 Introduction The aim of page is to: Document the process of setting up the SWAT+ and SWAP models for Kraakstad, Norway (Case study 10 / CS10) Link and execute all workflow steps in parallel with the documentation using Rmarkdown Make the model setup process reproducible Be a reference to the CS10 modelers, as well as other OPTAIN modelers. If all goes well, the compiling of this Gitbook will also compile the OPTAIN model setup in tandem, from source input files to final model setup (..and beyond!) The model setup follows the OPTAIN modelling protocols for SWAP (Farkas et al. 2023) and SWAT+ (Schürz et al. 2022). We begin with SWAT+ and the model setup with SWATBuildR, in section 2. References "],["model-setup-with-swatbuildr.html", "Chapter 2 Model setup with SWATBuildR 2.1 Processing input data 2.2 Calculating Contiguous Object Connectivity 2.3 Generate SWAT+ input 2.4 Add point source inputs 2.5 Create SWAT+ sqlite database", " Chapter 2 Model setup with SWATBuildR The OPTAIN project is using the COCOA approach (Schürz et al. 2022) and as such, needs to use the SWATBuildR package to build the SWAT+ model setup and calculate the connectivity between HRUs in the catchment. This chapter covers this process. We will define the location of our project here: project_path &lt;- &#39;model_data/cs10_setup&#39; And give it the name: project_name &lt;- &#39;optain-cs10&#39; We will need the following packages for this chapter: require(mapview) require(sf) require(raster) require(dplyr) require(ggplot2) This is a custom theme we will use in many GGplots: sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) If this flag is set to true, the following code will actually run buildR, which could take about 4 hours! run_code = FALSE 2.1 Processing input data SWATBuildR reads input data, performs some checks on it, and saves it in a compatible format for subsequent calculations. Little documentation exists on the creation of these data sets. IF you have any information to add, feel free to do so! 2.1.1 High-Resolution Digital Elevation Model (DEM) The high-resolution DEM is the basis for calculation of water connectivity, among other things. Most of the documentation of the creation of the DEM for CS10 has been lost to the sands of time. All we know is that it is located here: dem_path &lt;- &quot;model_data/input/elevation/dtm3_ns_v5.tif&quot; plot(raster(dem_path)) Figure 2.1: CS10 Digital elevation model (DEM). To our knowledge, it has a 10 meter resolution. 1 meter resolution was available but there seems to have been issues with using it. It is definitely preferable to use the 1m DEM as certain important information can be lost with (max allowed resolution) of 10m. An example of a hydrologically effective landscape features being lost due to coarse DEM resolution. From (Schürz et al. 2022) 2.1.2 Processing the Basin Boundary The basin boundary has presumably been created using the defined outlet point of the catchment and the DEM. No more is currently known about this file other than that it is located here: bound_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; Figure 2.2: CS10 Basin, caclulated from the DEM. We will begin using the SWATBuildR Package by initializing its functions. (Note: this package is currently unfinished, which is why this step is necessary). The following code and commentary is from version 1.5.12, written by Christoph Schuerz. Another note: there are currently issues with White box which are being resolved. source(&#39;model_data/swat_buildR/init.R&#39;) BuildR recommends all layers to be in the same CRS, if we set project_layer to FALSE, it will throw an error when this is not the case: The input layers might be in different coordinate reference systems (CRS). It is recommended to project all layers to the same CRS and check them before using them as model inputs. The model setup process checks if the layer CRSs differ from the one of the basin boundary. By setting ‘proj_layer &lt;- TRUE’ the layer is projected if the CRS is different. If FALSE different CRS trigger an error. project_layer &lt;- TRUE We read in and check the basin boundary and run some checks bound &lt;- read_sf(bound_path) %&gt;% select() set_proj_crs(bound, data_path) check_polygon_topology(layer = bound, data_path = data_path, label = &#39;basin&#39;, n_feat = 1, checks = c(F,T,T,T,F,F,F,F)) 2.1.3 Processing the Land layer Our land layer is located here: land_path &lt;- &quot;model_data/input/land/CS10_LU.shp&quot; Documentation on its creation does not exist. Figure 2.3: Land use map of CS10 by Farm ID We had an issue with the classification of the land uses. For the OPTAIN project, all agricultural fields must have a unique ID, and our land uses only had the ID of the given farm KGB (which had many different fields). To remedy this, new IDs were generated with the format a_###f_# where a_ represents the farm, and f_ represents the respective field of that farm. The farm names needed to be shortened because the SWAT+ model often cannot handle long ID names (longer than 16 characters) Note, the potential measures polygons were not counted as individual fields, which is why SP_ID does not match up with field_ID – This is by design. readr::read_csv(&quot;model_data/farm_id/a_f_id.csv&quot;, show_col_types = F) %&gt;% head() ## # A tibble: 6 × 4 ## KGB type_fr sp_id field_ID ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 213/8/1 a_084 1 a_084f_1 ## 2 213/7/6 a_083 2 a_083f_1 ## 3 213/7/6 a_083 3 a_083f_2 ## 4 213/7/4 a_082 4 a_082f_1 ## 5 213/65/1 a_081 5 a_081f_1 ## 6 213/65/1 a_081 6 a_081f_2 This was done in a simple QGIS workflow of dissolving by farm, splitting from single part to multipart, and then adding an iterating ID per farm field. This workflow could be replicated in R, and then shown here. It is under consideration… BuildR will now run some checks on our land layer. land &lt;- read_sf(land_path) %&gt;% check_layer_attributes(., type_to_lower = FALSE) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;land&#39;, type = &#39;vector&#39;) check_polygon_topology(layer = land, data_path = data_path, label = &#39;land&#39;, area_fct = 0.00, cvrg_frc = 99.9, checks = c(T,F,T,T,T,T,T,T)) BuildR splits the land layer into HRU (land) and reservoir (water) objects split_land_layer(data_path) 2.1.4 Processing the Channels No documentation exists on the creation of the channels layer, all we know is that it is located here: channel_path &lt;- &#39;model_data/input/line/cs10_channels.shp&#39; channels &lt;- read_sf(channel_path) %&gt;% dplyr::select(&quot;type&quot;) channel_map &lt;- ggplot() + geom_sf(data = bound_sf) + geom_sf(data = channels, mapping = aes(color = type)) + sf_theme channel_map (#fig:channel_plot)CS10 Channels, including surface and subsurface channels BuildR runs some checks: channel &lt;- read_sf(channel_path) %&gt;% check_layer_attributes(., type_to_lower = TRUE) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;channel&#39;, type = &#39;vector&#39;) check_line_topology(layer = channel, data_path = data_path, label = &#39;channel&#39;, length_fct = 0, can_cross = FALSE) BuildR then checks the connectivity between the channels and reservoirs. For this we need to define our id_cha_out and id_res_out. “Variable id_cha_out sets the outlet point of the catchment. Either define a channel OR a reservoir as the final outlet. If channel then assign id_cha_out with the respective id from the channel layer. If reservoir then assign the respective id from the land layer to id_res_out, otherwise leave as NULL” id_cha_out &lt;- 37 id_res_out &lt;- NULL Running connectivity checks between channels and reservoirs: check_cha_res_connectivity(data_path, id_cha_out, id_res_out) Checking if any defined channel ids for drainage from land objects do not exist check_land_drain_ids(data_path) 2.1.5 Processing the DEM BuildR loads and checks the DEM, and saves it. dem &lt;- rast(dem_path) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;dem&#39;, type = &#39;raster&#39;) check_raster_coverage(rst = dem, vct_layer = &#39;land&#39;, data_path = data_path, label = &#39;dem&#39;, cov_frc = 0.95) save_dem_slope_raster(dem, data_path) 2.1.6 Processing soil data Our soil map is located here: soil_layer_path &lt;- &#39;model_data/input/soil/soil_layer.tif&#39; No documentation exists on its creation. Figure 2.4: CS10 Soil map The soil data and look-up path are located here: soil_lookup_path &lt;- &#39;model_data/input/soil/soil_lookup.csv&#39; soil_data_path &lt;- &#39;model_data/input/soil/UserSoil_Krakstad.csv&#39; Not much documentation exists here either. Some can be found in the excel sheet: ~/model_data/input/soil/swatsoil2.xlsx BuildR reads in the soil data, performs checks, processes, and saves. NOTE: THIS IS CURRENTLY BROKEN, WITH ERROR MESSAGE: Warning: Cannot find coordinate operations from EPSG:2583 [“unnamed”,EDATUM[“”], CS[Cartesian,2], AXIS[“(E)”, east, ORDER[1], LENGTHUNIT[“unknown”, 1]], AXIS[“(N)”, north, ORDER[2], LENGTHUNIT[“unknown”, 1]]]’ (GDAL error 6) Error: [project] Cannot do this transformation soil &lt;- rast(soil_layer_path) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;soil&#39;, type = &#39;raster&#39;) check_raster_coverage(rst = soil, vct_layer = &#39;hru&#39;, data_path = data_path, label = &#39;soil&#39;, cov_frc = 0.75) # BROKEN save_soil_raster(soil, data_path) BuildR then generates a table with aggregated elevation, slope, soil for HRU units. # waiting on soil fix. #aggregate_hru_dem_soil(data_path) Read and prepare the soil input tables and a soil/hru id table and write them into data_path/tables.sqlite #build_soil_data(soil_lookup_path, soil_data_path, data_path) 2.2 Calculating Contiguous Object Connectivity SWATBuildR follows COCOA. This section contains the calculations. You can read more about it in the protocol (ref). 2.2.1 Calculating land unit connectivity Preparing raster layers based on the DEM and the surface channel objects that will be used in the calculation of the land object connectivity. #prepare_terrain_land(data_path) The connection of each land object to neighboring land and water objects is calculated based on the flow accumulation and the D8 flow pointer along the object edge # calculate_land_connectivity(data_path) 2.2.1.1 Eliminate land object connections with small flow fractions: For each land object the flow fractions are compared to connection with the largest flow fraction of that land object. Connections are removed if their fraction is smaller than frc_thres relative to the largest one. This is necessary to: Simplify the connectivity network To reduce the risk of circuit routing between land objects. Circuit routing will be checked. If an error due to circuit routing is triggered, then ‘frc_thres’ must be increased to remove connectivities that may cause this issue. frc_thres &lt;- 0.3 The remaining land object connections are analyzed for infinite loop routing. For each land unit the connections are propagated and checked if the end up again in the same unit. #reduce_land_connections(data_path, frc_thres) %&gt;% # check_infinite_loops(., data_path, &#39;Land&#39;) If infinite loops were identified this routine tries to resolve the issues by selectively removing connections between land units in order to get rid of all infinite loops. # resolve_loop_issues(data_path) 2.2.2 Calculating channel/reservoir connectivity 2.2.2.1 Calculating the water object connectivity the function returns the cha and res con_out tables in SWAT+ database format and writes them into data_path/tables.sqlite #build_water_object_connectivity(data_path) 2.2.3 Checking the water objects for infinite loops From the cha_res_con_out tables id_from/id_to links are generated and checked for infinite loop routing. #prepare_water_links(data_path) %&gt;% # check_infinite_loops(., data_path, &#39;Water&#39;, Inf) 2.2.4 Terrain properties Calculate terrain properties such as elevation, slope, catchment area, channel width/depth for channel and reservoir objects and write them into data_path/tables.sqlite #prepare_terrain_water(data_path) 2.3 Generate SWAT+ input 2.3.1 Generate land object SWAT+ input tables Build the landuse.lum and a landuse/hru id table and write them into data_path/tables.sqlite #build_landuse(data_path) Build the HRU SWAT+ input files and write them into data_path/tables.sqlite #build_hru_input(data_path) Add wetlands to the HRUs and build the wetland input files and write them into data_path/tables.sqlite (TODO fix this!) wetland_landuse &lt;- c(&#39;wehb&#39;, &#39;wetf&#39;, &#39;wetl&#39;, &#39;wetn&#39;) #add_wetlands(data_path, wetland_landuse) 2.3.2 Generate water object SWAT+ input tables Build the SWAT+ cha input files and write them into data_path/tables.sqlite #build_cha_input(data_path) Build the SWAT+ res input files and write them into data_path/tables.sqlite #build_res_input(data_path) Build SWAT+ routing unit con_out based on ‘land_connect_fraction’. #build_rout_con_out(data_path) Build the SWAT+ rout_unit input files and write them into data_path/tables.sqlite #build_rout_input(data_path) Build the SWAT+ LSU input files and write them into data_path/tables.sqlite #build_ls_unit_input(data_path) 2.3.3 Build aquifer input Build the SWAT+ aquifer input files for a single aquifer for the entire catchment. The connectivity to the channels with geomorphic flow must be added after writing the txt input files. This is not implemented in the script yet. #build_single_aquifer_files(data_path) 2.4 Add point source inputs The point source locations are provided with a point vector layer in the path ‘point_path’. point_path &lt;- &#39;model_data/input/point/cs10_pointsource.shp&#39; point_sf &lt;- read_sf(point_path) point_map &lt;- mapview(point_sf, zcol = &quot;GRAD_P&quot;, cex = &quot;GRAD_N&quot;) mapview(bound_sf, alpha.regions = 0.2)+point_map Map of point sources, colored by (assumed) phosphorous and size by (assumed) Nitrogren Maximum distance of a point source to a channel or a reservoir to be included as a point source object (recall) in the model setup: max_point_dist &lt;- 500 #meters Point source records can automatically be added from files in the same folder as the point source location layer. To be identified as point source data the files must be named as &lt;name&gt;_&lt;interval&gt;.csv, where &lt;name&gt; must be the name of a point int the vector layer and &lt;interval&gt; must be one of const, yr, mon, or day depending on the time intervals in the input data. #add_point_sources(point_path, data_path, max_point_dist) 2.5 Create SWAT+ sqlite database 2.5.1 Write the SWAT+Editor project database The database will be located the ‘project_path’. After writing the database it can be opened and edited with the SWAT+Editor. #create_swatplus_database(project_path, project_name) The next step involves you entering the SWAT+ Editor and parameterizing the model from there. These are the steps we have taken, with screenshots since it is currently not possible to replicate this process in R. 2.5.2 TODO Switch to SWAT+Editor for further model parametrization and continue with the step below after writing the SWAT+ projects’ text input files 2.5.3 Link aquifers and channels with geomorphic flow A SWATbuildR model setup only has one single aquifer (in its current version). This aquifer is linked with all channels through a channel- aquifer-link file (aqu_cha.lin) in order to maintain recharge from the aquifer into the channels using the geomorphic flow option of SWAT+ The required input file cannot be written with the SWAT+Editor. Therefore it has to be generated in a step after writing the model text input files with the SWAT+Editor. Path of the TxtInOut folder (project folder where the SWAT+ text files are written with the SWAT+Editor) txt_path &lt;- &#39;../swat_runs/txtinouit/&#39; #link_aquifer_channels(txt_path) References "],["climate-inputs-and-weather-generator.html", "Chapter 3 Climate inputs and weather generator 3.1 Preperation and loading data 3.2 Creating the weather generator 3.3 Adding weather data to SWAT project", " Chapter 3 Climate inputs and weather generator 3.1 Preperation and loading data To add weather and climate data to our SWAT project, we will use svatools 3.1.1 Required packages require(euptf2) # devtools::install_github(&quot;tkdweber/euptf2&quot;) require(svatools) # devtools::install_github(&quot;biopsichas/svatools&quot;) require(readr) require(readxl) require(sf) require(mapview) 3.1.2 Load in the file(s) and load svatools template First we load in the template and fill it in with our values and rename it to “cs10_weather_data.xlsx”. This is not done within R. #temp_path &lt;- system.file(&quot;extdata&quot;, &quot;weather_data.xlsx&quot;, package = &quot;svatools&quot;) # /// fill out this template and save &quot;cs10_weather_data.xlsx&quot; /// We are using the following projection for this project: epgs_code = 25832 Now we can load it in with Svatools. met_lst &lt;- load_template(template_path = &quot;model_data/input/met/cs10_weather_data.xlsx&quot;, epgs_code) ## [1] &quot;Loading data from template.&quot; ## [1] &quot;Reading station ID1 data.&quot; ## [1] &quot;Loading of data is finished.&quot; 3.1.3 Proof the station plot_weather(met_lst, &quot;PCP&quot;, &quot;month&quot;, &quot;sum&quot;) Checking the location of the station: basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; basin &lt;- st_transform(st_read(basin_path), epgs_code) %&gt;% mutate(NAME = &quot;Basin&quot;) ## Reading layer `cs10_basin&#39; from data source ## `C:\\Users\\NIBIO\\Documents\\GitLab\\github\\optain-cs10\\model_data\\input\\shape\\cs10_basin.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1 feature and 1 field ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 603439.4 ymin: 6607792 xmax: 610984.4 ymax: 6622017 ## Projected CRS: ETRS89 / UTM zone 32N stations &lt;- st_transform(met_lst$stations, epgs_code) mapview(stations) + mapview(basin) 3.2 Creating the weather generator We only have one weather station, which means only one weather generator. wgn &lt;- prepare_wgn(met_lst, TMP_MAX = met_lst$data$ID1$TMP_MAX, TMP_MIN = met_lst$data$ID1$TMP_MIN, PCP = met_lst$data$ID1$PCP, RELHUM = met_lst$data$ID1$RELHUM, WNDSPD = met_lst$data$ID1$WNDSPD, MAXHHR = met_lst$data$ID1$MAXHHR, SLR = met_lst$data$ID1$SLR) ## [1] &quot;Coordinate system checked and transformed to EPSG:4326.&quot; ## [1] &quot;Working on station ID1:WS_AAS&quot; write.csv(wgn$wgn_st, &quot;model_data/input/met/as_wgn_st.csv&quot;, row.names = FALSE, quote = FALSE) write.csv(wgn$wgn_data, &quot;model_data/input/met/as_wgn_data.csv&quot;, row.names = FALSE, quote = FALSE) 3.3 Adding weather data to SWAT project Currently, this throws an error – Cannot tell why, but might not be an issue because it still writes the files that we need. Because the buildR script is currently broken, we cannot evaluate the following code db_path &lt;- &quot;model_data/cs10_setup/optain-cs10/...sqlite&quot; add_weather(db_path, met_lst, wgn) "],["crop-map-generation.html", "Chapter 4 Crop Map Generation 4.1 Calculating Farm Area 4.2 Generating the Crop Rotation 4.3 Extrapolating the Crop Rotation 4.4 Merging crop rotation with BuildR output", " Chapter 4 Crop Map Generation As OPTAIN takes into account the individual field, we need to know what is growing on which field and when. Unfortunately our data foundation for this task is quite lackluster, but we will try out best to do so. in the following chapter. require(sf) require(dplyr) require(readr) require(stringr) require(readxl) require(reshape2) require(tibble) require(DT) require(ggplot2) sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) 4.1 Calculating Farm Area We have data based on what area certain crops have per farm. In order to relate this to our spatial land use map, we require the area of both the farms, and the fields within these farms. The basis for these calculations comes from the BuildR output: (REF header here) lu_sf &lt;- read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land.shp&quot;) lu_map &lt;- ggplot() + geom_sf(lu_sf, mapping = aes(fill = type)) + theme(legend.position = &quot;left&quot;) + sf_theme + theme(legend.position = &quot;none&quot;) print(lu_map) Figure 4.1: Land use map for CS10, colored by type. 4.1.1 Calculate Field Area and Assign IDs This stage has been completed in QGIS. An R-implementation is being considered. farm_area_sf &lt;- read_sf(&quot;model_data/input/crop/buildr_output_dissolved_into_farms.shp&quot;) field_lu_plot &lt;- ggplot() + geom_sf(farm_area_sf, mapping = aes(fill = farm_id)) + theme(legend.position = &quot;none&quot;) +sf_theme print(field_lu_plot) Figure 4.2: Farms in the CS10 catchment, colored by ID 4.1.2 Calculate Field area and assign IDs This stage has been completed in QGIS. An R-implementation is being considered. 4.2 Generating the Crop Rotation The following algorithm takes information from reporting farm, and their respective area in the SWAT+ setup, and combines these data sets to generate a plausible crop rotation. Note, pasture refers to meadow (TODO: fix!) crop_data &lt;- read_excel(&quot;model_data/input/crop/crop_rotation_source_data.xlsx&quot;) datatable(crop_data) 4.2.1 Tidy up the source data: We are converting the source data to tidy format in the following code snippet crop_names &lt;- melt( crop_data, id.vars = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;, &quot;pastrure_area_daa&quot;), measure.vars = c(&quot;crop1&quot;, &quot;crop2&quot;, &quot;crop3&quot;), variable.name = &quot;crop_status&quot;, value.name = &quot;crop&quot; ) %&gt;% tibble() crop_area &lt;- melt( crop_data, id.vars = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;, &quot;pastrure_area_daa&quot;), measure.vars = c(&quot;percent_crop1&quot;, &quot;percent_crop2&quot;, &quot;percent_crop3&quot;), variable.name = &quot;crop_status&quot;, value.name = &quot;area&quot; ) %&gt;% tibble() crop_area$crop_status &lt;- crop_area$crop_status %&gt;% str_remove(&quot;percent_&quot;) # unstable, TODO fix! colnames(crop_area)[6] &lt;- &quot;crop_area_percent&quot; crop_tidy &lt;- left_join(crop_names, crop_area, by = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;,&quot;pastrure_area_daa&quot;, &quot;crop_status&quot;)) %&gt;% dplyr::select(-crop_status) # unstable, TODO fix! colnames(crop_tidy)[3] &lt;- &quot;farm_area_daa&quot; crop_tidy$crop_area_percent &lt;- (crop_tidy$crop_area_percent/100) %&gt;% round(2) crop_tidy &lt;- crop_tidy %&gt;% filter(crop %&gt;% is.na() == FALSE) datatable(crop_tidy) 4.2.2 Tidy up the BuildR output The maps shown in secttion (ref) were exported to CSV in QGIS. An R-implementation might be written in here sometime (#TODO). This CSV data needs to be re-formatted to be tidy as well. fields &lt;- read_csv( &quot;model_data/input/crop/buildr_landuse_dissolved_into_fields.csv&quot;, show_col_types = F ) %&gt;% dplyr::select(type, farm_id, field_area_daa) farms &lt;- read_csv( &quot;model_data/input/crop/buildr_landuse_dissolved_into_farms.csv&quot;, show_col_types = F ) %&gt;% dplyr::select(type, farm_id , farm_area_) # fix farm_area_ # unstable, fix! colnames(farms)[3] &lt;- &quot;farm_area_daa&quot; farms$farm_area_daa &lt;- farms$farm_area_daa %&gt;% round(2) fields$field_area_daa &lt;- fields$field_area_daa %&gt;% round(2) reported_data &lt;- left_join(fields, farms, by = &quot;farm_id&quot;) # unstable, fix! colnames(reported_data)[1] &lt;- &quot;field_id&quot; reported_data &lt;- reported_data %&gt;% dplyr::select(field_id, farm_id, field_area_daa, farm_area_daa) datatable(reported_data) 4.2.3 Field classification We are now ready to perform our classification. We will store our results in this predefined dataframe: classed_fields &lt;- tibble( farm_id = NA, field_id = NA, farm_area_daa = NA, field_area_daa = NA, crop = NA, year = NA ) The following reporting farms will be used for the classification: farms &lt;- reported_data$farm_id %&gt;% unique() For the following years (AKA we have data for these years): years &lt;- c(2016, 2017, 2018, 2019) This for loop runs through all the fields and classifies them. The output will not be printed in this file, but will be saved in a log file, that you can dig out of the repository, located here: #TODO add the log file generation The following code is not executed: # For every year for(c_year in years){ # For every farm, for (farm in farms) { # Do the following: # Filter the source and buildR data to the given year and farm crop_filter &lt;- crop_tidy %&gt;% filter(year == c_year) %&gt;% filter(farm_id == farm) hru_filter &lt;- reported_data %&gt;% filter(farm_id == farm) # Behavior if the farm has no reporting data: if(crop_filter$farm_id %&gt;% length() == 0){ # Set all fields to winter wheat. farm_fields &lt;- hru_filter %&gt;% dplyr::select(farm_id, field_id, farm_area_daa, field_area_daa) %&gt;% arrange(desc(field_area_daa)) farm_fields$crop = &quot;wwht&quot; farm_fields$year = c_year # Add them to the results dataframe classed_fields &lt;- rbind(classed_fields, farm_fields) # and skipping to next farm next() } # Behavior, if there is reporting data on the farm&quot; # Find the area of the farm (all elements in vector are the same, so ok # to just grab the first) farm_area_hru &lt;- hru_filter$farm_area_daa[1] # Find the discrepency between &quot;SHOULD BE&quot; crop area and &quot;CURRENTLY IS&quot; # crop area. We want this as an absolute value. area_dis &lt;- (hru_filter$farm_area_daa[1]-crop_filter$farm_area_daa[1]) %&gt;% round(2) %&gt;% abs() # Some farms have no reported area. in this case we set it to some random # negative number. if(area_dis %&gt;% is.na()){area_dis = -999} # Extracting the area of the pasture and farm (all vectors same value, ok # to just take the first) past_area &lt;- crop_filter$pastrure_area_daa[1] farm_area &lt;- crop_filter$farm_area_daa[1] # Extract relevant fields farm_fields &lt;- hru_filter %&gt;% dplyr::select(farm_id, field_id, farm_area_daa, field_area_daa) %&gt;% # and sort by area arrange(desc(field_area_daa)) # pre-define crops column to be NA farm_fields$crop = NA # Bollean check to see if the field has been meadow in a previous year. If # it has, then we want to continue planting meadow on it (This was a # decision we made.) has_meadow &lt;- (classed_fields %&gt;% filter(field_id %in% farm_fields$field_id) %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_id) %&gt;% pull() %&gt;% length() &gt; 0) ### Determining how much land the crops should cover. # This if statement checks if is currently a pasture portion for the reporting # farm, or if there has been in the past. if((past_area %&gt;% is.na() == FALSE) | has_meadow) { # If it does, or did, then this special routine is enacted: # Determines the pasture percentage of the total farm past_percent &lt;- (past_area/farm_area) # If it cannot be calculated, then it is set to 0 if(past_percent %&gt;% is.na()){past_percent = 0} # The crop fractions provided to us in the source data did not account for # the pasture fraction. therefore we need to reduce the other crop # fractions by this amount. adjuster &lt;- past_percent / length(crop_filter$crop) crop_filter$crop_area_percent &lt;- crop_filter$crop_area_percent-adjuster # creating a new entry for pasture and adding it to the now updated crop\\ # fractions list past_row &lt;- tibble( farm_id = farm, year = c_year, farm_area_daa = crop_filter$farm_area_daa[1], pastrure_area_daa = past_area, crop = &quot;meadow&quot;, crop_area_percent = past_percent ) crop_filter &lt;- rbind(crop_filter, past_row) # Creating an updated &quot;SHOULD BE&quot; and &quot;CURRENTLY IS&quot; crop coverage # datafram crop_coverage &lt;- tibble( crop = crop_filter$crop, to_cover = (farm_area_hru*crop_filter$crop_area_percent)) # Setting the initial actual coverage to 0 (&quot;CURRENTLY IS&quot;) crop_coverage$actual &lt;- 0 # This extracts the fields that were previously pasture fields. previously_pasture &lt;- classed_fields %&gt;% filter(field_id %in% farm_fields$field_id) %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_id) %&gt;% pull() if (length(previously_pasture) &gt; 0) { # if some of the fields were previously pasture, they are set to pasture # again: farm_fields$crop[ which(farm_fields$field_id %in% previously_pasture)] = &quot;meadow&quot; # We save the amount of area the meadow crops cover, so that we can # adjust the other &quot;SHOULD BE&quot; fractions correctly. custom_actual &lt;- farm_fields %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_area_daa) %&gt;% pull() %&gt;% sum(na.rm = T) crop_coverage$actual[which(crop_coverage$crop==&quot;meadow&quot;)] = custom_actual } # Now the algorithm continues as normal. # This is the routine that is run, when no meadow is detected: }else{ # creating a tibble which tracks how much land the crops SHOULD cover crop_coverage &lt;- tibble( crop = crop_filter$crop, to_cover = (farm_area_hru * crop_filter$crop_area_percent) ) # pre define crop coverage to 0 (CURRENT) crop_coverage$actual &lt;- 0 } ### Crop classification # Now we know how much land the crops SHOULD cover, it is time to assign # the fields accordingly. We do this with a WHILE loop, which keeps going # until we have classified every field. while(any(farm_fields$crop %&gt;% is.na())){ # run through all the crops in the crop coverage list and determine or # update their current coverage for (c_crop in crop_coverage$crop) { # figure out which index we are in the list crop_index &lt;- which(c_crop == crop_coverage$crop) # Determine/UPDATE the current actual coverage of the crop (sum) crop_coverage$actual[crop_index] &lt;- farm_fields %&gt;% filter(crop == c_crop) %&gt;% dplyr::select(field_area_daa) %&gt;% sum() } # Calculate the difference between crop SHOULD BE coverage and ACTUAL # coverage crop_coverage$diff &lt;- crop_coverage$actual-crop_coverage$to_cover # determine the maximum difference. max_diff &lt;- crop_coverage$diff %&gt;% min(na.rm = T) # determine which crop has the maximum difference max_diff_crop &lt;- crop_coverage$crop[which(crop_coverage$diff == max_diff) %&gt;% min(na.rm = T)] # determine the biggest field left un-classified. biggest_na_field &lt;- farm_fields %&gt;% filter(crop %&gt;% is.na()) %&gt;% arrange(desc(field_area_daa)) %&gt;% nth(1) # Set the field which is still NA, and also the biggest to the crop with # the maximum difference (This code seems weird, and could be # improved?) farm_fields$crop[which((farm_fields$crop %&gt;% is.na() == TRUE) &amp; farm_fields$field_area_daa == biggest_na_field$field_area_daa )] &lt;- max_diff_crop } # After having classified the field, we update the actual coverage value # the respective crop that has been classified. for (c_crop in crop_coverage$crop) { crop_index &lt;- which(c_crop == crop_coverage$crop) crop_coverage$actual[crop_index] &lt;- farm_fields %&gt;% filter(crop == c_crop) %&gt;% dplyr::select(field_area_daa) %&gt;% sum() } # save the year of this crop assignment in the dataframe farm_fields$year = c_year # add the classification to the result dataframe classed_fields &lt;- rbind(classed_fields, farm_fields) # and move onto the next farm } # for every year } # Remove the first NA line classed_fields &lt;- classed_fields[-1, ] Now we can have a look at our results (not evaluated) datatable(classed_fields) We can now extract the crop rotation from this datatable. (not evaluated) # the format of our final dataframe final_df &lt;- tibble(field = NA, y_2016 = NA, y_2017 = NA, y_2018 = NA, y_2019 = NA) # fields we need to extract field_ids &lt;- classed_fields$field_id %&gt;% unique() We will do this with this for loop (note this is bad R-practice. Should be done with something like pivot_longer (TODO)) for(c_field_id in field_ids) { crop_rotation &lt;- tibble(field = c_field_id) crops &lt;- classed_fields %&gt;% filter(field_id == c_field_id) %&gt;% dplyr::select(crop) %&gt;% t() %&gt;% as_tibble(.name_repair = &quot;minimal&quot;) colnames(crops) &lt;- c(&quot;y_2016&quot;, &quot;y_2017&quot;, &quot;y_2018&quot;, &quot;y_2019&quot;) crop_rotation &lt;- cbind(crop_rotation, crops) %&gt;% as_tibble() final_df &lt;- rbind(final_df, crop_rotation) } # Remove the first NA line crop_rotation &lt;- final_df[-1,] # Save the crop rotation in CSV format write_csv(x = crop_rotation, file = &quot;model_data/input/crop/cs10_crop_rotation.csv&quot;) Here is a look at the crop rotation: The crop rotation looks like this: ## Loading required package: gifski (#fig:cro_gif)Generated crop rotation for CS10. 4.3 Extrapolating the Crop Rotation For OPTAIN, the crop rotation needs to span from 1988 to 2020. We currently have 2016 to 2019. crop_rotation &lt;- read_csv(&quot;model_data/input/crop/cs10_crop_rotation.csv&quot;, show_col_types = F) rotation &lt;- crop_rotation %&gt;% dplyr::select(-field) crop_rotation_extrapolated &lt;- cbind(crop_rotation, rotation, rotation, rotation, rotation, rotation, rotation, rotation, rotation$y_2016) # set the column names to be in the correct format colnames(crop_rotation_extrapolated) &lt;- c(&quot;field&quot;, paste0(&quot;y_&quot;, 1988:2020)) 4.4 Merging crop rotation with BuildR output The output of SWATbuildR “land.shp” needs to be connected to the newly generated crop map. lu &lt;- read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land.shp&quot;) # temporary rename to field, for the left join # unstable, fix! colnames(lu)[2] = &quot;field&quot; # join the crop rotation and land use layer by their field ID lu_cr &lt;- left_join(lu, crop_rotation_extrapolated, by = &quot;field&quot; ) # reset column name. # unstable, fix! colnames(lu_cr)[2] = &quot;lu&quot; # Write the new shape file write_sf(lu_cr, &quot;model_data/input/crop/land_with_cr.shp&quot;) "],["landuse.lum-update.html", "Chapter 5 Landuse.lum update 5.1 Data preperation 5.2 Assigning landuse values 5.3 Writing Changes", " Chapter 5 Landuse.lum update This section covers the modifications made to the landuse file. For some reason it was written in a tutorial fashion, as if the reader were new to R. require(tidyverse) require(reshape2) require(sf) require(DT) require(dplyr) # require dplyr last to overwrite plyr count() require(ggplot2) ggplot theme(s): sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) 5.1 Data preperation We read in the land use file. We want to set skip = 1 to ignore the SWAT+editor text, and set header = T. We then convert it to a tibble format for better printing to console landuse_lum &lt;- read.table(&quot;model_data/cs10_setup/temp_landuse.lum&quot;, skip = 1, header = T) %&gt;% tibble::as_tibble() (#tab:lum_tab)The landuse.lum file, post BuildR name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp a_001f_1_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_2_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_3_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_4_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_5_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_6_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_7_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_002f_1_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_002f_2_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_002f_3_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null Our field_id for our cropland does not match name of landuse_lum so we need to parse it out, we can do this many ways, but a safe way is to split it by “_” and combine the first 3 splits with that same underscore: splitted &lt;- landuse_lum$name %&gt;% str_split(&quot;_&quot;) landuse_lum$field_id &lt;- paste(splitted %&gt;% map(1), splitted %&gt;% map(2), splitted %&gt;% map(3), sep = &quot;_&quot;) head(landuse_lum$field_id) ## [1] &quot;a_001f_1&quot; &quot;a_001f_2&quot; &quot;a_001f_3&quot; &quot;a_001f_4&quot; &quot;a_001f_5&quot; &quot;a_001f_6&quot; But wait, this does not work for our “non-fields”. So lets find out which ones they are, and set them to NA not_fields &lt;- which(!grepl(x=landuse_lum$field_id, &quot;a_&quot;)) landuse_lum$field_id[not_fields] &lt;- NA So, what non-fields do we have? landuse_lum$name[not_fields] ## [1] &quot;frst_lum&quot; &quot;past_lum&quot; &quot;rngb_lum&quot; &quot;urml_lum&quot; &quot;utrn_lum&quot; &quot;wetf_lum&quot; Good to know. We’ll keep that in mind. 5.2 Assigning landuse values 5.2.1 Setting cal_group There is no info on this column, so we are going to leave it as null. 5.2.2 Setting plnt_com This step will be done by SWATFarmR ??. 5.2.3 Setting mgt This step will be done by SWATFarmR ??. 5.2.4 Setting urb_ro We want to set the urb_ro column for all urban land uses (in our case this would be urml and utrn) to usgs_reg. We can do this like so: landuse_lum$urb_ro[which(landuse_lum$name %in% c(&quot;urml_lum&quot;, &quot;utrn_lum&quot;))] &lt;- &quot;usgs_reg&quot; (#tab:urbro_tab)Changing urb_ro in the landuse file name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp field_id urml_lum null null null null null null usgs_reg null null null null null null NA utrn_lum null null null null null null usgs_reg null null null null null null NA Very good. In our case this was only two – could be done by hand, but that will not be the case for all of our land uses. 5.2.5 Setting urban This one is easy, we set our urban column to an urban parameter set of the same name. The rest we leave as null landuse_lum$urban[which(landuse_lum$name ==&quot;utrn_lum&quot;)] &lt;- &quot;utrn&quot; landuse_lum$urban[which(landuse_lum$name ==&quot;urml_lum&quot;)] &lt;- &quot;urml&quot; (#tab:urbro_tab2)Changing urban in the landuse file name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp field_id urml_lum null null null null null urml usgs_reg null null null null null null NA Lets make sure other land uses still have null: Table 5.1: Landuse frst in the landuse file name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp field_id frst_lum null nopl_comm null null null null null null null null null null null NA Looks good. 5.2.6 Setting Manning’s n (ovn) Lets get the easy ones out of the way landuse_lum$ov_mann[which(landuse_lum$name ==&quot;past_lum&quot;)] &lt;- &quot;densegrass&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;rngb_lum&quot;)] &lt;- &quot;rangeland_20cover&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;urml_lum&quot;)] &lt;- &quot;urban_rubble&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;urtn_lum&quot;)] &lt;- &quot;urban_asphalt&quot; Don’t fall asleep yet! For wetf we want forest_heavy but with a higher value. this means we need to add a new entry. And since we are doing this fancy Rmarkdown stuff, we’re going to do it with code! Lets jump in and get at this file. (REMEBER TO REMOVE THE TEMP) ovn_table_path &lt;- &quot;model_data/cs10_setup/temp_ovn_table.lum&quot; ovn_table &lt;- readLines(ovn_table_path) Good, now where is this forest heavy entry? and what does the format look like? index &lt;- grepl(x=ovn_table, &quot;forest_heavy&quot;) %&gt;% which %&gt;% min() ovn_table[c(2, index)] %&gt;% print() ## [1] &quot;name ovn_mean ovn_min ovn_max description&quot; ## [2] &quot;forest_heavy 0.80000 0.70000 0.90000 Forest_heavy&quot; Now, lets make our own and add it in. But only if it doesn’t exist it (Like if the script has been run before…) if(grepl(x = ovn_table, &quot;forest_heavy_cs10&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;forest_heavy_cs10 0.90000 0.80000 0.95000 Forest_heavy_mod&quot; ovn_table &lt;- c(ovn_table, entry) } Did it work? Yes: ovn_table %&gt;% tail() ## [1] &quot;forest_med 0.60000 0.50000 0.70000 Forest_medimum_good&quot; ## [2] &quot;forest_heavy 0.80000 0.70000 0.90000 Forest_heavy&quot; ## [3] &quot;urban_asphalt 0.01100 0.01100 0.01100 Urban_asphalt&quot; ## [4] &quot;urban_concrete 0.01200 0.01200 0.01200 Urban_concrete&quot; ## [5] &quot;urban_rubble 0.02400 0.02400 0.02400 Urban_rubble&quot; ## [6] &quot;forest_heavy_cs10 0.90000 0.80000 0.95000 Forest_heavy_mod&quot; Now lets write this new table writeLines(ovn_table, con = ovn_table_path) And now we can enter our wetland class: landuse_lum$ov_mann[which(landuse_lum$name ==&quot;wetf_lum&quot;)] &lt;- &quot;forest_heavy_cs10&quot; For the fields we are going to need the crop rotation info which we created in section 4. crop_rotation &lt;- read_csv(&quot;model_data/input/crop/cs10_crop_rotation.csv&quot;, show_col_types = F) head(crop_rotation) ## # A tibble: 6 × 5 ## field y_2016 y_2017 y_2018 y_2019 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a_025f_1 wwht wwht wwht wwht ## 2 a_025f_3 wwht wwht wwht wwht ## 3 a_025f_4 wwht wwht wwht wwht ## 4 a_025f_2 wwht wwht wwht wwht ## 5 a_105f_1 wwht wwht wwht wwht ## 6 a_105f_4 wwht wwht wwht wwht We have decided to classify ovn based on the degree to which a crop rotation contains meadow, conventional crops (wwht, pota), and conservation crops (all others). To do this we need to analyze the crop rotation. Now it gets a little complicated, but trust me its not actually as bad as it looks. We need to count how many times certain crops show up in the crop rotation of a certain field. Lets do it for conventional crops first. Now, lets get into it conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() %&gt;% dplyr::rename(conv = n) head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field conv ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 What happened here? well we took our crop_rotation, and melted it by field using melt. This function converts the data into tidy format (Wickham 2014). This format makes it easier to apply the following operations on a field basis. What does this look like? crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% arrange(field) %&gt;% head() ## field variable value ## 1 a_001f_1 y_2016 wwht ## 2 a_001f_1 y_2017 wwht ## 3 a_001f_1 y_2018 wwht ## 4 a_001f_1 y_2019 wwht ## 5 a_001f_2 y_2016 wwht ## 6 a_001f_2 y_2017 wwht melt is a function from reshape2 which is why we required it. And what does that “.” mean in there, to the left of \"field\"?? That is a code word for the object to the left of the “pipe” (in this case crop_rotation). the pipe is “%&gt;%” and passes the object to the left of it, into the function to the right of it. look it up! When arranged by field, we can see that every field gets one entry per crop per year. This is a good format to count how many times we have a certain type of crop on a field. Next we group_by the field – this means all the following operations will be done on a field basis. Then we filter our value (which is the crop name). For conventional we needed to filter in any fields with the crop \"wwht\" or \"pota\". What does this look like? crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) ## # A tibble: 1,519 × 3 ## # Groups: field [415] ## field variable value ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 a_025f_1 y_2016 wwht ## 2 a_025f_3 y_2016 wwht ## 3 a_025f_4 y_2016 wwht ## 4 a_025f_2 y_2016 wwht ## 5 a_105f_1 y_2016 wwht ## 6 a_105f_4 y_2016 wwht ## 7 a_105f_2 y_2016 wwht ## 8 a_105f_3 y_2016 wwht ## 9 a_187f_1 y_2016 wwht ## 10 a_206f_1 y_2016 wwht ## # ℹ 1,509 more rows Looks good. We only have crops with winter wheat and potatoes, for every year. Exactly what we need, now we just need to count() them. conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field n ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 And we are back where we started! See, not that complicated. One last thing we need to do is rename n to conv, we do that like so: conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() %&gt;% dplyr::rename(conv = n) head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field conv ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 Now lets go ahead and do the same thing for the two other categories: cons andmeadow. meadow_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value == &quot;meadow&quot; ) %&gt;% dplyr::count() %&gt;% dplyr::rename(meadow = n) cons_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(!(value %in% c(&quot;wwht&quot;, &quot;pota&quot;, &quot;meadow&quot;))) %&gt;% dplyr::count() %&gt;% dplyr::rename(cons = n) cons_crop_count %&gt;% head() ## # A tibble: 6 × 2 ## # Groups: field [6] ## field cons ## &lt;chr&gt; &lt;int&gt; ## 1 a_002f_1 2 ## 2 a_002f_2 2 ## 3 a_002f_3 4 ## 4 a_002f_4 4 ## 5 a_002f_5 2 ## 6 a_002f_6 4 meadow_crop_count %&gt;% head() ## # A tibble: 6 × 2 ## # Groups: field [6] ## field meadow ## &lt;chr&gt; &lt;int&gt; ## 1 a_012f_3 4 ## 2 a_012f_6 4 ## 3 a_016f_3 4 ## 4 a_017f_2 4 ## 5 a_017f_3 4 ## 6 a_046f_1 4 meadow was a simple filter, all we have to do was grab crops with the meadow name. For cons crops, we just grabbed the remaining crops that were not conv or meadow. Great. We have these 3 separate, we need to combine them. we can do that with left_join and join by the field column which contains our IDs # create a base dataframe to join to crop_fractions &lt;- crop_rotation %&gt;% dplyr::select(field) %&gt;% distinct() # join our 3 data frames crop_fractions &lt;- left_join(crop_fractions, conv_crop_count, by = &quot;field&quot;) crop_fractions &lt;- left_join(crop_fractions, cons_crop_count, by = &quot;field&quot;) crop_fractions &lt;- left_join(crop_fractions, meadow_crop_count, by = &quot;field&quot;) # lets have a look crop_fractions %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 a_025f_1 4 NA NA ## 2 a_025f_3 4 NA NA ## 3 a_025f_4 4 NA NA ## 4 a_025f_2 4 NA NA ## 5 a_105f_1 4 NA NA ## 6 a_105f_4 4 NA NA That does not look good! when it seems like when the count is 0, it is returned as NA. Lets fix that… crop_fractions &lt;- crop_fractions %&gt;% mutate(cons = ifelse(is.na(cons), 0, cons)) crop_fractions &lt;- crop_fractions %&gt;% mutate(conv = ifelse(is.na(conv), 0, conv)) crop_fractions &lt;- crop_fractions %&gt;% mutate(meadow = ifelse(is.na(meadow), 0, meadow)) What are we doing here? we are mutating the the 3 columns using an ifelse statement. The statement is simple. If the value is.na then we set it to 0. Otherwise, we set it to the same value it had before. did it work? crop_fractions %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a_025f_1 4 0 0 ## 2 a_025f_3 4 0 0 ## 3 a_025f_4 4 0 0 ## 4 a_025f_2 4 0 0 ## 5 a_105f_1 4 0 0 ## 6 a_105f_4 4 0 0 Very good. Now we need to decide what to classify our fields at. lets Define some functions to do that. is_meadow &lt;- function(conv, cons, meadow) { ((meadow &gt; cons) &amp; (meadow &gt; conv)) %&gt;% return() } is_cons &lt;- function(conv, cons, meadow) { ((cons &gt;= meadow) &amp; (cons &gt; conv)) %&gt;% return() } is_conv &lt;- function(conv, cons, meadow) { ((conv &gt;= meadow) &amp; (conv &gt;= cons)) %&gt;% return() } The &amp; sign means that both conditions need to be met. and &gt;= you should know already. Lets use those functions in action. lets do the meadow first. We will create a new dataframe from crop fractions, named field_class. field_class &lt;- crop_fractions %&gt;% mutate(meadow = is_meadow(conv,cons,meadow)) head(field_class) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 a_025f_1 4 0 FALSE ## 2 a_025f_3 4 0 FALSE ## 3 a_025f_4 4 0 FALSE ## 4 a_025f_2 4 0 FALSE ## 5 a_105f_1 4 0 FALSE ## 6 a_105f_4 4 0 FALSE Ok, so none of those first 6 fields are meadow dominated. What about cons? (lets stick with our field_class dataframe and just keep adding on) field_class &lt;- field_class %&gt;% mutate(cons = is_cons(conv,cons,meadow)) head(field_class) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 a_025f_1 4 FALSE FALSE ## 2 a_025f_3 4 FALSE FALSE ## 3 a_025f_4 4 FALSE FALSE ## 4 a_025f_2 4 FALSE FALSE ## 5 a_105f_1 4 FALSE FALSE ## 6 a_105f_4 4 FALSE FALSE Nope, not cons either. Then it must be conv dominant. field_class &lt;- field_class %&gt;% mutate(conv = is_conv(conv,cons,meadow)) field_class %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 a_025f_1 TRUE FALSE FALSE ## 2 a_025f_3 TRUE FALSE FALSE ## 3 a_025f_4 TRUE FALSE FALSE ## 4 a_025f_2 TRUE FALSE FALSE ## 5 a_105f_1 TRUE FALSE FALSE ## 6 a_105f_4 TRUE FALSE FALSE Correct! now lets just double check that we didnt have any cases where all are TRUE or where all are FALSE field_class %&gt;% dplyr::select(conv, cons, meadow) %&gt;% isTRUE %&gt;% all() ## [1] FALSE field_class %&gt;% dplyr::select(conv, cons, meadow) %&gt;% isFALSE() %&gt;% all() ## [1] FALSE Looks good to me! carry on: We have our field classifications now, they will be very useful to us later. Lets pull them out of our dataframe to store them as a nice list. cons_fields &lt;- field_class %&gt;% filter(cons) %&gt;% dplyr::select(field) %&gt;% pull() conv_fields &lt;- field_class %&gt;% filter(conv) %&gt;% dplyr::select(field) %&gt;% pull() meadow_fields &lt;- field_class %&gt;% filter(meadow) %&gt;% dplyr::select(field) %&gt;% pull() cons_fields %&gt;% head() ## [1] &quot;a_182f_1&quot; &quot;a_182f_5&quot; &quot;a_182f_3&quot; &quot;a_182f_4&quot; &quot;a_182f_2&quot; &quot;a_182f_7&quot; conv_fields %&gt;% head() ## [1] &quot;a_025f_1&quot; &quot;a_025f_3&quot; &quot;a_025f_4&quot; &quot;a_025f_2&quot; &quot;a_105f_1&quot; &quot;a_105f_4&quot; meadow_fields %&gt;% head() ## [1] &quot;a_182f_6&quot; &quot;a_182f_8&quot; &quot;a_017f_3&quot; &quot;a_017f_2&quot; &quot;a_012f_3&quot; &quot;a_012f_6&quot; Fantastic! Now that was a big task, but it makes the next bit very easy. Lets assign the correct ovn to the types of fields we have classified: 5.2.6.1 Agricultural Fields: Meadow landuse_lum$ov_mann[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;shortgrass&quot; 5.2.6.2 Agricultural Fields: Conventional landuse_lum$ov_mann[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;convtill_nores&quot; 5.2.6.3 Agricultural Fields: Conservational landuse_lum$ov_mann[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;falldisk_res&quot; 5.2.6.4 Forest Initially we wanted certain classes for the forest land use based on how good the soil quality was. But to do this, we would need multiple land uses, which we do not have. We will just use forest_medium for all frst. We can only change this if we go back to buildR and define more generic forest classes. landuse_lum$ov_mann[which(landuse_lum$name ==&quot;frst_lum&quot;)] &lt;- &quot;forest_med&quot; 5.2.6.5 Summary With all that data processing out of the way, lets have a quick look at what we’ve done: landuse_lum$ov_mann[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;densegrass&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;rangeland_20cover&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urban_rubble&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;urtn_lum&quot;)] &lt;- &quot;urban_asphalt&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;forest_med&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;shortgrass&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;convtill_nores&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;falldisk_res&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;forest_heavy_cs10&quot; Sweet. Next column.. 5.2.7 Setting cn2: You know the drill – I am sure you know how to read this code by now. # Brush-brush-weed-grass_mixture_with_brush_the_major_element (poor) landuse_lum$cn2[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;brush_p&quot; # Woods (poor) landuse_lum$cn2[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;wood_p&quot; # Paved_streets_and_roads;_open_ditches_(incl._right-of-way) landuse_lum$cn2[which(landuse_lum$name == &quot;utrn_lum&quot;)] &lt;- &quot;paveroad&quot; # Paved_parking_lots_roofs_driveways_etc_(excl_right-of-way) landuse_lum$cn2[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urban&quot; # Woods (fair) landuse_lum$cn2[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;wood_f&quot; # Pasture_grassland_or_range-continuous_forage_for_grazing landuse_lum$cn2[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;pastg_g&quot; # Meadow-continuous_grass_protected_from_grazing_mowed_for_hay landuse_lum$cn2[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;pasth&quot; # Row_crops landuse_lum$cn2[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;rc_conterres_g&quot; # Row_crops landuse_lum$cn2[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;rc_strow_p&quot; 5.2.8 Setting cons_prac For this, we need to add some custom entries to the database: # TODO: remove temp!!!! cons_prac_path &lt;- &quot;model_data/cs10_setup/temp_cons_practice.lum&quot; cons_prac &lt;- readLines(cons_prac_path) cons_prac[1:3] %&gt;% print() ## [1] &quot;cons_practice.lum: written by SWAT+ editor v2.1.0 on 2023-03-31 08:13 for SWAT+ rev.60.5.4&quot; ## [2] &quot;name usle_p slp_len_max description&quot; ## [3] &quot;up_down_slope 1.00000 121.00000 Up_and_down_slope&quot; Let us do it in a way so that it is only added if it doesn’t exist yet: if(grepl(x = cons_prac, &quot;agri_conv&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_conv 1.00000 60.00000 no_convervation&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_part_conv&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_part_conv 0.85000 60.00000 75_percent_convential&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_half&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_half 0.70000 50.00000 50_percent_convential&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_part_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_part_cons 0.50000 30.00000 75_percent_consveration&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_cons 0.30000 30.00000 100_percent_consveration&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;past_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;past_cons 0.1 60 pasture&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;rngb_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;rngb_cons 0.2 60 rangeland&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;frst_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;frst_cons 0.1 60 forest&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;urml_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;urml_cons 1 60 cs10urban&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;utrn_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;utrn_cons 1 60 road&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_meadow&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_meadow 0.2 60 cs10_meadow&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;wetf_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;wetf_cons 0.05 30 wetlands&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_sed_pond&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_sed_pond 0.1 30 cs10_sed_pond&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_cons_wetl&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_cons_wetl 0.1 30 cs10_cons_wetl&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_buff_grass&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_buff_grass 0.25 10 cs10_buff_grass&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs_10_buff_wood&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs_10_buff_wood 0.2 10 cs_10_buff_wood&quot; cons_prac &lt;- c(cons_prac, entry) } And write the updated file: writeLines(cons_prac, con = cons_prac_path) Now we need to define the 0, 25, 50, 75, 100 percent conventional crops. We can use our old “crop_fractions” dataframe: head(crop_fractions) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a_025f_1 4 0 0 ## 2 a_025f_3 4 0 0 ## 3 a_025f_4 4 0 0 ## 4 a_025f_2 4 0 0 ## 5 a_105f_1 4 0 0 ## 6 a_105f_4 4 0 0 Lets derive which fields get classified as what p100_conv &lt;- crop_fractions %&gt;% filter(conv==4) %&gt;% dplyr::select(field) %&gt;% pull() p75_conv &lt;- crop_fractions %&gt;% filter(conv==3) %&gt;% dplyr::select(field) %&gt;% pull() p50_conv &lt;- crop_fractions %&gt;% filter(conv==2) %&gt;% dplyr::select(field) %&gt;% pull() p25_conv &lt;- crop_fractions %&gt;% filter(conv==1) %&gt;% dplyr::select(field) %&gt;% pull() p0_conv &lt;- crop_fractions %&gt;% filter(conv==0) %&gt;% dplyr::select(field) %&gt;% pull() Now we can assign the all the land uses # Agricultural landuse_lum$cons_prac[which(landuse_lum$field_id %in% p100_conv)] &lt;- &quot;agri_conv&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p75_conv)] &lt;- &quot;agri_part_conv&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p50_conv)] &lt;- &quot;agri_half&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p25_conv)] &lt;- &quot;agri_part_cons&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p0_conv)] &lt;- &quot;agri_cons&quot; # Meadow landuse_lum$cons_prac[which(field_class$meadow)] &lt;- &quot;cs10_meadow&quot; # Generic landuse_lum$cons_prac[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;past_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;rngb_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;frst_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urml_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;utrn_lum&quot;)] &lt;- &quot;utrn_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;wetf_cons&quot; # Measures landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_sed_pond&quot;)] &lt;- &quot;cs10_sed_pond&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_cons_wetl&quot;)] &lt;- &quot;cs10_cons_wetl&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_buff_grass&quot;)] &lt;- &quot;cs10_buff_grass&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs_10_buff_wood&quot;)] &lt;- &quot;cs_10_buff_wood&quot; 5.2.9 Setting tile This column has already been completed by SWATbuildR in 2.3 5.2.10 Setting sep This column has been left as null 5.2.11 Setting vfs This column has been left as null 5.2.12 Setting grww This column has been left as null 5.2.13 Setting bmp This column has been left as null 5.3 Writing Changes We are done with our modifications however, we need to remove the field_id column landuse_lum &lt;- landuse_lum %&gt;% dplyr::select(-field_id) lets take a look at the final product: datatable(landuse_lum) Lets write out changes new_lum_path &lt;- &quot;model_data/cs10_setup/temp_lum_new.lum&quot; write.table(landuse_lum, file = new_lum_path, sep = &quot;\\t&quot;, quote = F, row.names = F) But wait – we need to keep that pesky header, otherwise the FarmR will be very angry with us. lum_lines &lt;- readLines(new_lum_path) header &lt;- &quot;header header header HEADER, delete me and you will regret it FOREVER!&quot; lum_lines2&lt;- c(header,lum_lines) writeLines(text = lum_lines2, con = new_lum_path) Done! References "],["field-site-loggers.html", "Chapter 6 Field Site Loggers 6.1 Introduction 6.2 Loading in observed data. 6.3 Data Re-structuring 6.4 Data quality check 6.5 Data Cleanup", " Chapter 6 Field Site Loggers 6.1 Introduction This chapter is concerned with preparing the measured data for the OPTAIN field scale models. This will involve loading the data, transforming it into a usable format, and performing a quality check. The end of this chapter will result in data fit for use in SWAP as well as a tangentially related antecedent precipitation index (API) model for use in the SWAT+ modelling work, related to scheduling management operations using SWATFarmR . 6.1.1 Prerequisites The following packages are required for this chapter: require(readr) # for reading in data require(dplyr) # for manipulating data require(stringr) # for manipulating text require(purrr) # for extracting from nested lists require(plotly) # for diagnostic plotting require(lubridate) # for date conversions require(reshape2) # for data conversions require(wesanderson) # plot colors require(DT) # for tables require(mapview) # for plotting require(sf) # for shapefiles show_rows &lt;- 500 # number of rows to show in the tables As well as the following custom functions: source(&quot;model_data/code/plot_logger.R&quot;) 6.1.2 Locations The loggers are described as such: Logger3(600409): deep rocky sand (forest?) Logger4(600410): Tormods farm (agri) Logger2(600411): John Ivar lower area (agri) Logger1(600413): John Ivar behind the barn (agri) And are located here: field_sites_shp &lt;- read_sf(&quot;model_data/field_sites/geospatial/cs10_field_sites.shp&quot;) basin_shp &lt;- read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;) basin_map &lt;- mapview(basin_shp, alpha.regions = .1, legend = FALSE) field_map &lt;- mapview(field_sites_shp, legend = FALSE) basin_map+field_map 6.2 Loading in observed data. To start we need to load in our data from the GroPoint Profile data loggers. The logger is approx. 1m long and has 7 temperature sensors, and 6 soil moisture sensors located along its segments. “GroPoint Profile Multi Segment Soil Moisture &amp; Temperature Profiling Probe Operation Manual” (2023) Figure 6.1: GroPoint Segment Schematic (GroPoint Profile User Manual, Page 6) Figure 6.2: GroPoint Temperature sensor depths. We use GPLP-4 with seven temperature sensors. (GroPoint Profile User Manual, Page 6) The logger data has been modified before import into R. The file names have been changed and column headers have been added. Missing values stored as “Error” have been removed. Columns headers have assigned with variable name, depth, and unit of depth separated by an underscore. path &lt;- &quot;model_data/field_sites/loggers/data/&quot; Loading the data and modifying it is performed with readr and dplyr. # list of files datasheets &lt;- list.files(path, full.names = TRUE) # loading in the data, with an ID column &#39;source&#39; data &lt;- read_csv(datasheets, show_col_types = F, id = &quot;source&quot;) # translating the source text to logger ID data$site &lt;- data$source %&gt;% str_remove(path) %&gt;% str_remove(&quot;.csv&quot;) %&gt;% str_split(&quot;_&quot;) %&gt;% map(1) %&gt;% unlist() # removing the source column data &lt;- data %&gt;% select(-source) # removing duplicates data &lt;- data %&gt;% distinct() Data preview We need to force the date time column into the correct format. We can do this with the following command. (Please note, this has been done for region: United States this code may need to be adjusted to fit your regional settings) data$datetime &lt;- data$datetime %&gt;% as_datetime(format = &quot;%m/%d/%Y %H:%M&quot;, tz = &quot;CET&quot;) 6.3 Data Re-structuring For ease of use in R, we will re-structure the data to be in “tidy” format (Read more: (Wickham 2014)). # grab the measured variable column headers mea_var &lt;- colnames(data)[3:15] # melt (tidy) the data by datetime and site. data_melt &lt;- data %&gt;% melt(id.vars = c(&quot;datetime&quot;, &quot;site&quot;), measure.vars = mea_var) # parse out the variable data_melt$var &lt;- data_melt$variable %&gt;% str_split(&quot;_&quot;) %&gt;% map(1) %&gt;% unlist() # parse out the depth of measurement data_melt$depth &lt;- data_melt$variable %&gt;% str_split(&quot;_&quot;) %&gt;% map(2) %&gt;% unlist() # remove duplicates data_melt &lt;- data_melt %&gt;% distinct() Data preview: 6.3.1 Summarize to Daily Means As SWAP is a daily time step model, we do not need the hourly resolution, and can therefore simplify our analysis. We first need to parse out a (daily) date column data_melt$date &lt;- data_melt$datetime %&gt;% as.Date() And then group the data by this date, followed by an averaging function. daily_data &lt;- data_melt %&gt;% group_by(date, var, depth, site) %&gt;% summarise(value = round(mean(value, na.rm = TRUE), 1), .groups = &quot;drop_last&quot;) %&gt;% ungroup() Additional notes: The values are rounded to one decimal place, as this is is the precision limit of the Logger. The averaging is performed per date, variable, depth, and site (for obvious reasons) The functionality of .groups = \"drop_last\" is unknown to me, however it does not change anything other than quieting a warning message. The data is “ungrouped” at the end, to avoid problems with plotting the data (with plotly) later on. Data preview: 6.3.2 Including Air Temperature and Precipitation Data We are going to add the temp / precipitation data from out SWAT+ setup to give us insight on temperature and moisture dynamics for the data quality check. Also important to note, is that all this code is only necessary, because of the strange format SWAT+ uses for its weather input. If it would just use a standard format (like everything else) then all this would not be needed. Reading in the SWAT source precipitation data: # TODO remove temp swat_pcp &lt;- read.table(&quot;model_data/cs10_setup/temp_sta_id1.pcp&quot;, skip = 3, header = F, sep = &quot;\\t&quot;) %&gt;% as_tibble() swat_tmp &lt;- read.table(&quot;model_data/cs10_setup/temp_sta_id1.tmp&quot;, skip = 3, header = F, sep = &quot;\\t&quot;) %&gt;% as_tibble() Data preview: Parsing out the first and last day and year. FIRST_DOY &lt;- swat_pcp[1,][[2]]-1 FIRST_YEAR &lt;- swat_pcp[1,][[1]] LAST_DOY &lt;- swat_pcp[length(swat_pcp$V1),][[2]]-1 LAST_YEAR &lt;- swat_pcp[length(swat_pcp$V1),][[1]] Converting these to an R format. Note that R uses a 0-based index only for dates. (very confusing!). first_day &lt;- as.Date(FIRST_DOY, origin = paste0(FIRST_YEAR, &quot;-01-01&quot;)) last_day &lt;- as.Date(LAST_DOY, origin = paste0(LAST_YEAR, &quot;-01-01&quot;)) date_range &lt;- seq(from = first_day, to = last_day, by = &quot;day&quot;) Creating a dataframe from the source data pcp_data &lt;- tibble(date = date_range, pcp = swat_pcp$V3) And filtering it to only cover the same range as our logger data. pcp_data &lt;- pcp_data %&gt;% filter(date %in% daily_data$date) And we do the same for temperature FIRST_DOY &lt;- swat_tmp[1,][[2]]-1 FIRST_YEAR &lt;- swat_tmp[1,][[1]] LAST_DOY &lt;- swat_tmp[length(swat_tmp$V1),][[2]]-1 LAST_YEAR &lt;- swat_tmp[length(swat_tmp$V1),][[1]] first_day &lt;- as.Date(FIRST_DOY, origin = paste0(FIRST_YEAR, &quot;-01-01&quot;)) last_day &lt;- as.Date(LAST_DOY, origin = paste0(LAST_YEAR, &quot;-01-01&quot;)) date_range &lt;- seq(from = first_day, to = last_day, by = &quot;day&quot;) tmp_data &lt;- tibble(date = date_range, tmp_max = swat_tmp$V3, tmp_min = swat_tmp$V4) tmp_data &lt;- tmp_data %&gt;% filter(date %in% daily_data$date) Figure 6.3: Temperature range and precipitation for CS10 Note: this does (yet) include data from 2023, even though the loggers do. 6.4 Data quality check In this section we are proofing the quality of the data. 6.4.1 Logger 600409 “Deep Rocky Sand” plot_logger(daily_data, &quot;600409&quot;, pcp_data, tmp_data) Figure 6.4: Logger 600409 6.4.1.1 Soil Temperature Remove first few days with unrealistic data 6.4.1.2 Soil Moisture Content First few days need to be removed 150mm depth seems usable before Sept2022 – However Attila (instrument installer) mentioned this sensor might be in the air (which would explain low moisture levels), so potentially useless 350mm depth seems usable, breaks around/after Sept2022 450mm depth seems useless, constantly getting wetter for an entire year, spiking post Sept2022 600mm depth seems useless, constantly getting wetter for an entire year, spiking post Sept2022 750mm depth seems useless, constantly getting wetter for an entire year. 900mm depth seems useless, constantly getting wetter for an entire year. 6.4.2 Logger 600410 “Tormods farm” plot_logger(daily_data, &quot;600410&quot;, pcp_data, tmp_data) 6.4.2.1 Soil Temperature Remove first few days with unrealistic data 6.4.2.2 Soil Moisture Content First few days need to be removed Depth 150 probably useless, as sensor is in free air. (According to Attila) Logger seems to have broken Sometime after Sept1 2022, for all depth levels. Data pre-Sept22 seems usable (Csilla ?) 6.4.3 Logger 600411 “John Ivar Lower Area” plot_logger(daily_data, &quot;600411&quot;, pcp_data, tmp_data) 6.4.3.1 Soil Temperature Need to remove the first few days Need to remove the malfunction in April-May 2022 6.4.3.2 Soil Moisture Content First few days need to be removed Need to remove the malfunction in April-May 2022 Depths 150 is out of soil, must be removed. Logger seems to have broken August 16th, 2022, data seems useful before that date (Csilla, agree?) 6.4.4 Logger 600413 “John Ivar behind the barn” plot_logger(daily_data, &quot;600413&quot;, pcp_data, tmp_data) 6.4.4.1 Soil Temperature Remove the first few days with unrealistic data 6.4.4.2 Soil Moisture Content Remove first few days Remove 150mm, seems out of earth Hard to tell when sensor broke, earliest June 24th, latest Sept.12 2022, what do you think Csilla ? 6.5 Data Cleanup 6.5.1 General cleanup Removing the first few days of the simulation start_date &lt;- &quot;2021-11-01&quot; data_clean &lt;- daily_data %&gt;% filter(date &gt; start_date) To be on the safe side, I am setting the logger cut-off date to XXXXXXXXX This could be tuned “per logger” later on if one feels confident. cut_off_date &lt;- &quot;2022-08-14&quot; data_clean &lt;- data_clean %&gt;% filter(date &lt; cut_off_date) And we will do the same for the climate data pcp_data_clean &lt;- pcp_data %&gt;% filter(date &gt; start_date) pcp_data_clean &lt;- pcp_data_clean %&gt;% filter(date &lt; cut_off_date) tmp_data_clean &lt;- tmp_data %&gt;% filter(date &gt; start_date) tmp_data_clean &lt;- tmp_data_clean %&gt;% filter(date &lt; cut_off_date) We will remove the 150 mm depth SMC measurement, as it was not submerged properly in the soil, and is therefore of little use. The temperature reading might be useful, so we will keep it. data_clean &lt;- data_clean %&gt;% filter(depth != &quot;150&quot;) 6.5.2 Logger specific cleanup 6.5.2.1 Logger 600409 No logger specific cleanup required 6.5.2.2 Logger 600410 No logger specific cleanup required 6.5.2.3 Logger 600411 Logger 600411 needs temperature and SMC data removed from 2022-04-21 to 2022-05-10. It seems like the logger was removed from the soil. # define time range to remove remove_dates &lt;- seq(from = as.Date(&quot;2022-04-21&quot;), to = as.Date(&quot;2022-05-10&quot;), by = &quot;day&quot;) # set values in that site and date range to NA data_clean &lt;- data_clean %&gt;% mutate(value = case_when((site == &quot;600411&quot; &amp; date %in% remove_dates) ~ NA, .default = value)) 6.5.2.4 Logger 600413 No logger specific cleanup required 6.5.3 Saving the cleaned data We can now save our cleaned up data. We will do so in CSV form write_csv(data_clean, file = &quot;model_data/field_sites/loggers/clean/logger_data_daily_clean.csv&quot;) write_csv(pcp_data_clean, file = &quot;model_data/field_sites/loggers/clean/pcp_data_clean.csv&quot;) References "],["field-site-properties.html", "Chapter 7 Field Site Properties 7.1 Determining the HSG group of the logger sites", " Chapter 7 Field Site Properties For various use-cases across the modelling project, we will require certain information about the locations of the field sites. This section gathers that information and makes it available to other sections. 7.1 Determining the HSG group of the logger sites require(sf) require(readr) require(DT) require(dplyr) require(mapview) require(ggplot2) sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) We load the soil map of the area. soil_map &lt;- &quot;model_data/input/soil/soil_map_UTM32N.shp&quot; soil_map_shp &lt;- read_sf(soil_map) We will crop it by the watershed. Therefore we load the water shed cs10_basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; cs10_basin &lt;- read_sf(cs10_basin_path) And crop by by basin. (current not sure of the effect of constant geometry) st_agr(soil_map_shp) = &quot;constant&quot; st_agr(cs10_basin) = &quot;constant&quot; soil_map_shp &lt;- st_intersection(soil_map_shp, cs10_basin) ggplot()+geom_sf(soil_map_shp, mapping = aes(fill = SNAM))+sf_theme Figure 7.1: Soil map of CS10, in shapefile format We need information on the soil itself. This is found in our user table. We only need the hydrologic soil group (HSG) HYDGRP, but we will keep the soil depth SOL_ZMX and soil texture TEXTURE just in case usersoil_path &lt;- &quot;model_data/input/soil/UserSoil_Krakstad.csv&quot; usersoil &lt;- read_csv(usersoil_path, show_col_types = F) usersoil &lt;- usersoil %&gt;% dplyr::select(OBJECTID, SNAM, SOL_ZMX, HYDGRP, TEXTURE) We can combine the two data sets with a left join by soil ID / Object ID usersoil$SOIL_ID &lt;- usersoil$OBJECTID usersoil &lt;- usersoil %&gt;% dplyr::select(-OBJECTID) soil_propery_map &lt;- dplyr::left_join(soil_map_shp, usersoil, by = &quot;SOIL_ID&quot;) ggplot()+geom_sf(soil_propery_map, mapping = aes(fill = HYDGRP))+sf_theme Figure 7.2: HSG of CS10 Now we need the locations of the field sites. These are stored as points in the following file: cs10_field_sites_path &lt;- &quot;model_data/field_sites/geospatial/cs10_field_sites.shp&quot; cs10_field_sites &lt;- read_sf(cs10_field_sites_path) We are going to join the attributes using a spatial join field_sites_attr &lt;- st_join(cs10_field_sites, soil_propery_map) Here is the result: We have two sites covering C and D, and one site covering B. This covers all existing HSGs in the catchment, which is good. We can save this information in a dataframe. field_site_attr_df &lt;- st_drop_geometry(field_sites_attr) datatable(field_site_attr_df) We will write this into our output folder write_csv(x = field_site_attr_df, file = &quot;model_data/field_sites/output/field_site_attr_df.csv&quot;) And save our soil property map, and field site points (with attributes) # TODO fix the weird ID column mess you made write_sf(field_sites_attr, &quot;model_data/field_sites/output/field_site_attr_map.shp&quot;) ## Warning in CPL_write_ogr(obj, dsn, layer, driver, ## as.character(dataset_options), : GDAL Message 6: Normalized/laundered field ## name: &#39;Id&#39; to &#39;Id_1&#39; write_sf(soil_propery_map, &quot;model_data/field_sites/output/soil_propery_map.shp&quot;) "],["antecedent-precipitation-index.html", "Chapter 8 Antecedent Precipitation Index 8.1 Introduction 8.2 Antecedent Precipitation Index", " Chapter 8 Antecedent Precipitation Index 8.1 Introduction SWATFarmR cannot account for operations based on values modeled by SWAT+, such as soil moisture. This is because FarmR does not run the model itself, and creates all the management schedules based on measured climate data. This means we need to relate our measured climate data to our soil moisture. A method to do this is called the Antecedent Precipitation Index. Which is what we will creating for CS10 in this part As a heads up, this section requires Python! 8.1.1 Pre-requirements Required R packages require(readr) require(dplyr) require(hydroGOF) library(reticulate) # Required Python packages: &quot;pandas&quot;, &quot;numpy&quot;, &quot;scipy&quot; source(&quot;model_data/code/plot_api.R&quot;) source(&quot;model_data/code/calc_api.R&quot;) 8.1.2 Setting up Python for R/Rmarkdown A quick (non-evaluated) code block to show how a python environment can be created for use in Rmarkdown/Reticulate. # install miniconda install_miniconda() # create conda environment for your project conda_create(&quot;cs10_env&quot;, # environment name packages = c(&quot;pandas&quot;, &quot;numpy&quot;, &quot;scipy&quot;), # required packages conda = &quot;auto&quot;) conda_python(&quot;cs10_env&quot;) # returns the .exe python path for your environment # you need to change your R environment path in this file usethis::edit_r_environ() # RETICULATE_PYTHON=&quot;YOUR ENVIRONMENT PATH/python.exe&quot; # restart your R session. py_config() # If everything is right, you should get something like: # python: YOUR ENVIRONMENT PATH/python.exe libpython: YOUR ENVIRONMENT # PATH/python39.dll pythonhome: YOUR ENVIRONMENT PATH version: 3.9.7 (default, # Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] Architecture: 64bit numpy: # YOUR ENVIRONMENT PATH\\lib\\site-packages\\numpy numpy_version: 1.21.2 # After this it should work :) 8.2 Antecedent Precipitation Index From (Patrignani 2020) The API is a well-known, parsimonious, recursive model for predicting soil moisture solely based on precipitation records. The API is commonly implemented using daily precipitation records, but it is possible to work at finer temporal scales (e.g. hourly) if both precipitation (model input) and soil moisture (for validation purposes) are available. The equation describing the simple version of the model is: \\[API_{t} = \\alpha API_{t-1} + P_t\\] \\(API_t\\): Soil water content at time \\(t\\) (today) \\(API_{t-1}\\): Soil water content at time \\(t-1\\) (yesterday) \\(\\alpha\\): Loss coefficient. Range between 0 and 1 \\(P_t\\): Precipitation at time \\(t\\) (today) 8.2.1 Loading data We will load out data from the previous steps. (ref) logger_data &lt;- read_csv(&quot;model_data/field_sites/loggers/clean/logger_data_daily_clean.csv&quot;, show_col_types=F) # we only need SMC for this part logger_data = logger_data %&gt;% filter(var == &quot;smc&quot;) site_attributes &lt;- read_csv(&quot;model_data/field_sites/output/field_site_attr_df.csv&quot;, show_col_types = F) pcp_data &lt;- read_csv(&quot;model_data/field_sites/loggers/clean/pcp_data_clean.csv&quot;, show_col_types = F) head(site_attributes) ## # A tibble: 5 × 8 ## id SOIL_ID SNAM.x Id SNAM.y SOL_ZMX HYDGRP TEXTURE ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 600413 23 ST 0 Sea_dep_thick 800 C sandy loam ## 2 600411 1 AB 0 ARdy 1000 C loamy_sand ## 3 600410 20 RGah 0 GLum-ar 1000 B sandy_loam ## 4 600409 13 Humic 0 GLch-hu-sl 1000 D silty_clay_loam ## 5 600408 13 Humic 0 GLch-hu-sl 1000 D silty_clay_loam We will calculate the model per Hydrologic soil group (HSG), and start with B, which was measured by logger 600410. b_data &lt;- logger_data %&gt;% filter(site == &quot;600410&quot;) 8.2.2 Calculating the total water content We need to calculate the total water content of the profile from the depth measurements from the following depths: unique(b_data$depth) ## [1] &quot;300&quot; &quot;450&quot; &quot;600&quot; &quot;750&quot; &quot;900&quot; Following code is modified from Patrignani (2020) (read more) This calculation could be improved by considering the soil horizons. a &lt;- b_data %&gt;% filter(depth == &quot;300&quot;) %&gt;% select(value) b &lt;- b_data %&gt;% filter(depth == &quot;450&quot;) %&gt;% select(value) c &lt;- b_data %&gt;% filter(depth == &quot;600&quot;) %&gt;% select(value) d &lt;- b_data %&gt;% filter(depth == &quot;750&quot;) %&gt;% select(value) e &lt;- b_data %&gt;% filter(depth == &quot;900&quot;) %&gt;% select(value) total_water_content &lt;- a * .35 + # from 0 to 35 b * .15 + # from 35 to 50 c * .15 + # from 55 to 65 d * .15 + # from 65 to 80 e * .20 # from 80 to 100 b_col &lt;- tibble(b_data %&gt;% filter(depth == &quot;300&quot;) %&gt;% select(date), total_water_content) And we can add the precipitation data fro step 01. (REF) b_col &lt;- left_join(b_col, pcp_data, by = &quot;date&quot;) plot_api(obs = b_col, title = &quot;HSG B total water content&quot;) Saving the data write_csv(b_col, &quot;model_data/field_sites/output/twc.csv&quot;) 8.2.3 Defining API model “Create a function containing both the API and the gamma model so that we can optimize the function parameters:” The value is capped at the max val of the source data (50). This could be improved by capping per site def api_model(P,alpha=0.97,ini=36.8): api = [ini] for t in range(1,len(P)): append_val = api[t-1]*alpha + P[t] if append_val &gt; 50: append_val = 50 api.append(append_val) return api Load in our data: import pandas as pd df = pd.read_csv(&#39;model_data/field_sites/output/twc.csv&#39;) df = df[[&#39;date&#39;,&#39;value&#39;,&#39;pcp&#39;]] df.head() ## date value pcp ## 0 2021-11-02 36.880 0.3 ## 1 2021-11-03 36.930 1.1 ## 2 2021-11-04 36.995 0.0 ## 3 2021-11-05 37.025 0.0 ## 4 2021-11-06 37.030 0.7 As a test, we will calculate the predictions with an alpha of 0.95: import pandas as pd from scipy.optimize import curve_fit # guess storage_guessed = api_model(df[&#39;pcp&#39;], alpha=0.95, ini=36.8) # write to text dates = df[[&#39;date&#39;]] guess = pd.DataFrame(dates.values.tolist(),storage_guessed) guess.to_csv(&quot;model_data/field_sites/output/first_guess.csv&quot;) Now we can have a look at it in R guess &lt;- read_csv(&quot;model_data/field_sites/output/first_guess.csv&quot;, show_col_types = F, skip = 1, col_names = c(&quot;val&quot;, &quot;date&quot;)) plot_api(api_vals = guess, obs = b_col, title = &quot;HSG-B API guessed&quot;) Now we optimize the alpha parameter using scipy curve_fit. par_opt, par_cov = curve_fit(api_model, df[&#39;pcp&#39;], df[&#39;value&#39;], p0=[0.95,100]) print(&#39;Annual mean alpha value is&#39;,round(par_opt[0],2)) ## Annual mean alpha value is 0.98 Now we can calculate the API using the optimized parameter. import numpy as np storage_optimized = api_model(df[&#39;pcp&#39;],par_opt[0],par_opt[1]) # Mean Absolute Error MAE = np.mean(np.abs(df[&#39;value&#39;] - storage_optimized)) print(&#39;Mean annual error =&#39;, round(MAE,1),&#39;cm&#39;) ## Mean annual error = 9.1 cm And save the data for R dates = df[[&#39;date&#39;]] api_optimized = pd.DataFrame(dates.values.tolist(),storage_optimized) api_optimized.to_csv(&quot;model_data/field_sites/output/api_optimized.csv&quot;) And have a look in R: optimized &lt;- read_csv(&quot;model_data/field_sites/output/api_optimized.csv&quot;, show_col_types = F, skip = 1, col_names = c(&quot;val&quot;, &quot;date&quot;)) plot_api(api_vals = optimized, obs = b_col, title = &quot;HSG-B API optimized&quot;) Not great, but also not surprising considering no account for snow melt has been done, nor do we have good logger data. We will define this workflow in a function calc_api.R and run it for the next loggers. calc_api(logger_id = &quot;600409&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set ## HSG Alpha ## &quot;D&quot; &quot;1&quot; This is the logger with broken data, so no surprise that the optimizer failed. calc_api(logger_id = &quot;600410&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set ## HSG Alpha ## &quot;B&quot; &quot;0.99&quot; calc_api(logger_id = &quot;600411&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set ## HSG Alpha ## &quot;C&quot; &quot;0.99&quot; calc_api(logger_id = &quot;600413&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set ## HSG Alpha ## &quot;C&quot; &quot;0.99&quot; It seems all the values are around 0.99… so analysis not very (yet) useful. BUG: allow to calculate with NA values in the validation data (400611) References "],["model-parameterization.html", "Chapter 9 Model Parameterization 9.1 Parameter Changes", " Chapter 9 Model Parameterization These sections are not detailed enough to warrant their own dedicated chapter. 9.1 Parameter Changes These sections mostly involve changing a few parameter values and are therefore in small sub sections without R-code. Some of this might change later. 9.1.1 Channel parameters revised This an ongoing unresolved issue #49 9.1.2 Crop parameters verified The results of this step can be found in file plannts.plt and discussions to this topic can be found in issue #27. Detailed documentation is currently unavailable. 9.1.3 Soil physical parameters in final form This has been mentioned in issue #28, and has been closed without discussion. Seems like the parameters are in final form, but no further documentation exists at this point. 9.1.4 Soil chemical parameters in final form This step will be verified in issue #46. It has been discussed in #19. The following changes have been implemented, but are subject to change: Changes made to soilnut1 of nutrients.sol Parameter Default CS10 lab_p 5 20 nitrate 7 8.5 inorgp 3.5 35.1 watersol_p 0.15 0.4 An R implementation is being considered. 9.1.5 Impoundment parameters defined Has been postponed by #20, will be dealt with in #54 9.1.6 Water diversions defined Deemed as not relevant for this catchment 9.1.7 Point sources parameters added Is an ongoing issue in #21 9.1.8 Tile drainage parameters defined Is an ongoing issue in #53 9.1.9 Atmospheric deposition defined Has been done with svatools. An R implementation is being considered. 9.1.10 Additional settings verified Refers to chapter 3.11 in The Protocol and files parameters.bsn codes.bsn Has been discussed in #22 More in-depth documentation and an R implementation are being considered The following changes have been made: Changes made to codes.bsn Parameter Default CS10 pet 1 2 rte_cha 0 1 cn 0 1 tiledrain 0 1 soil_p 0 1 atmo_dep a m (TODO: Fill in more data from the GitLab Issue) "],["swatfarmr-management-schedules.html", "Chapter 10 SWATFarmR Management Schedules 10.1 Pre-processing crop rotation 10.2 Initializing FarmR 10.3 Calculating Antecedent Precipitation Index 10.4 Scheduling Operations", " Chapter 10 SWATFarmR Management Schedules WIP: Every field in the OPTAIN SWAT setup needs its own management. We will use the SWATFarmR package to define it. Pre-requirements The creation of management schedules with SWATfarmR requires a SWAT+ model setup created by the SWATbuildR package. We also require the following packages: require(DT) require(ggplot2) require(readr) require(stringr) require(magrittr) #remotes::install_github(&quot;chrisschuerz/SWATfarmR&quot;) require(SWATfarmR) 10.1 Pre-processing crop rotation This OPTAIN-provided workflow pre-processes our crop rotation data into a SWATfarmR compatible format. Authored by Michael Strauch, modified by Moritz Shore It requires the following packages require(sf) require(tidyverse) require(lubridate) require(reshape2) require(remotes) require(dplyr) require(data.table) require(DT) And the following functions: source(&#39;model_data/swat_farmR/functions_write_SWATfarmR_input.R&#39;) 10.1.1 Input files 10.1.1.1 Land use map with crop information This map has the following requirements: The map must contain the land use of each hru. In case of cropland, the names must be unique for each field (e.g., ‘field_1’, ‘field_2’, etc.) The map must also contain crop infos for the period 1988 to 2020 (or 2021 if crop info available). This requires an extrapolation of the available crop sequences (the sequences derived from remote-sensing based crop classification or local data). The extrapolated crop sequence for 33 years will be also used for running climate scenarios and must not contain any gaps. That means, gaps have to be closed manually! The year columns must be named y_1988, y_1989, etc. The crop infos for each year must match the crop_mgt names in the management operation schedules (provided in a .csv file, see below) see also section 4.1 of the modelling protocol (ref) lu_shp &lt;- &#39;model_data/input/crop/land_with_cr.shp&#39; lu &lt;- st_drop_geometry(read_sf(lu_shp)) datatable( lu %&gt;% head(50), extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE) ) 10.1.1.2 Management operation schedules for each crop ..or, if available, crop-management type. All schedules must be compiled in one csv file (see example in demo data and study also section 4.2 of the modelling protocol) ‘crop_mgt’ must start with the 4-character SWAT crop names (any further management specification is optional). Each schedule must contain a ‘skip’ line to indicate the change of years. The ‘skip’ line should never be the last line of a schedule. mgt_csv &lt;- &#39;model_data/input/management/mgt_crops_CS10.csv&#39; mgt_crop &lt;- read.csv(mgt_csv) datatable(mgt_crop, extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE)) 10.1.1.3 Management operation schedules for generic land-use classes Usually all non-cropland classes with vegetation cover here, all schedules must be provided already in the SWATfarmR input format. lu_generic_csv &lt;- &#39;model_data/input/management/farmR_generic_CS10.csv&#39; # generic land use management .csv table mgt_generic &lt;- read.csv(lu_generic_csv) datatable(mgt_generic) 10.1.2 Settings 10.1.2.1 Simulation Period start_y &lt;- 2010 # starting year (consider at least 3 years for warm-up!) end_y &lt;- 2020 # ending year 10.1.2.2 Prefix of cropland hrus all names of hrus with a crop rotation must begin with this prefix in column ‘lu’ of your land use hru_crops &lt;- &#39;a_&#39; 10.1.2.3 Multi-year farmland grass Did you define any multi-year farmland grass schedules? ‘y’ (yes), ‘n’ (no) m_yr_sch_existing &lt;- &#39;n&#39; If yes, define also the following variables. If not, skip next four lines crop_myr &lt;- &#39;past&#39; # name of your farmland grass Maximum number of years farmland grass can grow before it is killed (should be &lt;8) max_yr &lt;- 5 Do your multi-year farmland grass schedules consider the type of the following crop (summer or winter crop)? (e.g., a ’_1.5yr’ schedule with a kill op in spring allows for planting a summer crop immediately afterwards) If yes, you must define your summer crops crop_s &lt;- c(&#39;sgbt&#39;,&#39;csil&#39;,&#39;barl&#39;) Do your summer crop schedules usually start with an operation in autumn (e.g. tillage)? To combine them with farmland grass, it is necessary that you provide ‘half-year-schedules’ (‘half-year-schedules’ are additional summer crop schedules without operations in autumn) The adapted schedules should be added to the crop management table with suffix ’_0.5yr’ (e.g. ‘csil_0.5yr’) If additional ‘half-year-schedules’ are not needed, because your normal summer crop schedules do not start in autumn, type ‘n’ additional_h_yr_sch_existing &lt;- &#39;n&#39; # &#39;y&#39; (yes), &#39;n&#39; (no) 10.1.3 Checks Check for correct positioning of ‘skip’ line check_skip &lt;- check_skip_position() Check for date conflicts in single crop schedules check_date_conflicts1() Build schedules for crop sequences (Messages disabled) rota_schedules &lt;- build_rotation_schedules() Check for date conflicts in the full rotation schedule. (Messages disabled) check_date_conflicts2() Solve minor date conflicts (where only a few days/weeks are overlapping) (Messages disabled) rota_schedules &lt;- solve_date_conflicts() Check again for date conflicts (Messages disabled) check_date_conflicts2() 10.1.4 Writing input data Write the SWAT farmR input table write_farmR_input() The output of this pre-processing stage is loaded in from this file: farmR_input &lt;- readr::read_csv(&quot;model_data/input/management/farmR_input.csv&quot;, show_col_types = F) datatable( head(farmR_input, 50), extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE) ) TODO: Figure out why this is an issue. Are you using the wrong buildR output as source material for the crop generation? We need to fix the land use name, which entails replacing the _drn_lum with just _lum for all the agricultural fields. This is because land.shp from buildR does not include the _drn for some reason # TODO: Figure out why this is an issue. Are you using the wrong buildR output # as source material for the crop generation? farmR_input2 &lt;- farmR_input %&gt;% filter(grepl(x = land_use, &quot;a_&quot;)) %&gt;% mutate(land_use = str_replace(.$land_use, &quot;_lum&quot;, &quot;_drn_lum&quot;)) farmR_input3 &lt;- farmR_input %&gt;% filter(!grepl(x = land_use, &quot;a_&quot;)) farmR_input4 &lt;- rbind(farmR_input2, farmR_input3) write_csv(farmR_input4, file =&quot;model_data/input/management/farmR_input2.csv&quot;) rm(farmR_input, farmR_input2, farmR_input3, farmR_input4) # Yes I am aware that code is bad and ugly. 10.2 Initializing FarmR If the FarmR has never been initialized, then use new_farmr() and read_management(), otherwise load_farmr(). EVAL FALSE as BuildR script is still broken project_path &lt;- &quot;model_data/cs10_setup&quot; SWATfarmR::new_farmr(project_name = &quot;cs10&quot;, project_path = project_path ) #load_farmr(...) cs10$read_management(file = &quot;farmR_input/farmR_input2.csv&quot;) 10.3 Calculating Antecedent Precipitation Index TODO loading API from section REF …. 10.4 Scheduling Operations EVAL FALSE as BuildR script is still broken cs10$schedule_operations(start_year = 2010, end_year = 2020, n_schedule = 2) 10.4.1 Write Operations EVAL FALSE as BuildR script is still broken We cannot write from 1988 as it is limited to our climate data, which currently only spans back to 2010 cs10$write_operations(start_year = 2010, end_year = 2020) "],["model-verification.html", "Chapter 11 Model Verification 11.1 Preparation 11.2 Analysis of simulated climate variables 11.3 Simulation of management operations", " Chapter 11 Model Verification 11.1 Preparation Loading required packages, defining paths, and loading objects. # Install/Update SWATdoctR if needed: # remotes::install_git(&#39;https://git.ufz.de/schuerz/swatdoctr&#39;, force = FALSE) require(SWATdoctR) 11.1.1 Generate Model Run veri_no_stress &lt;- SWATdoctR::run_swat_verification( project_path = ppath, nostress = 0, keep_folder = F, outputs = &quot;wb&quot; ) saveRDS(veri_no_stress, &quot;model_data/swat_doctR/verification_runs/veri_no_stress.rds&quot;) 11.1.2 Load Model Run veri_no_stress &lt;- readRDS(&quot;model_data/swat_doctR/verification_runs/veri_no_stress.rds&quot;) 11.2 Analysis of simulated climate variables Modified from the Protocol: The climate variables daily precipitation and daily minimum/maximum temperatures are required inputs of a SWAT+ model setup (more info: protocol section 2.4). We have these present. Further climate inputs such as solar radiation, relative humidity and wind speed are optional input variables and can be essential for the calculation of the potential evapotranspiration (PET). We have these, see issue #42 (link) Climate inputs are grouped to weather stations in a model setup and are assigned to spatial objects (HRUs, channels, reservoirs, etc.) with the nearest neighbor method. We only have 1 weather station so this is not of great relevance. The input of weather data and the assignment of climate variables to spatial objects can be sources for several issues which must be analysed: Data structure of the climate input tables, units of the climate variable, no data flag, etc. was wrong and can result in unrealistically small or large values of the climate variables in the simulation. Does not seem to be the case in our data The nearest neighbor assignment allocates weather stations to spatial objects where the weather records do not represent the actual weather conditions in a spatial object well. This can for example be an issue in complex terrain. Should not be a concern for us (for now) since we only have a single met station The selected method for the calculation of PET results in an under/overestimation of PET when compared to estimates of PET for the region. In such cases other methods for the simulation of PET which are included in SWAT+ should be tested if they better fit the regional conditions and available weather inputs (see more Additional settings). See issue #39 Large implausibilities in the weather inputs can be identified in analyses of annual basin averages of the simulated climate variables. Simulated annual and average values of climate variables must be comparable to observation data and/or region specific literature values. Any larger deviations of precipitation can indicate errors in the input file or an inappropriate assignment of weather stations to spatial units. Have not checked this yet (!!!) If the lapse rate option is active (Read more in Additional settings), it may be another potential reason for deviations from observations. We are tracking this topic in issue #39 (link). 11.2.1 Lapse rate on: # TODO, fix #43 first 11.2.2 Lapse rate off: # TODO, fix #43 first Over or underestimated PET can indicate errors in the temperature input files (and if provided in the solar radiation, relative humidity and wind speed inputs). We are tracking this topic in issue #39 (link). SWATdoctR provides the function plot_climate_annual() to analyse the annual simulated basin averages of climate variables. fig1 &lt;- SWATdoctR::plot_climate_annual(veri_no_stress) plot(fig1) The first panel shows ET fractions. Current version of SWAT+ often has implausible ET fractions! Are these values plausible? ANSWER: Are they Csilla? The second panel shows the precipitation fractions rainfall (rainfall) and snowfall (snofall). Are these values plausible? ANSWER: resolved in issue #43 (link) – but are they plausible Csilla? The third panel shows the annual temperature values. Are these values plausible? ANSWER: YES Csilla do you agree? The fourth panel shows the relative humidity values. Are these values plausible? ANSWER: resolved in issue #42 (link), but are they plausible? Csilla The fifth panel shows wind speed. Are these values plausible? ANSWER: resolved in #42, Csilla plausible? The sixth panel shows the annual sums of solar radiation. A comparison to literature values of annual solar radiation sums for the region can indicate issues in this input. Are these values plausible? ANSWER: Yes, values for the Oslo area seem to be around 4000 MJ – Csilla do you agree? The analysis of mean monthly precipitation (output variable precip), snowfall (output variable snofall) and snow melt (output variable snomlt) sums and their comparison with region specific information (or in the best case observations) provides insight in seasonal dynamics of the precipitation input. Particularly in snow impacted catchments a first verification of snowfall is valuable to see whether precipitation in solid form is simulated, a snow storage can build up and cause increased spring runoff through snow melt. The hydrological cycle of some catchments may be dominated by spring flood events which must be reflected by the simulated processes. Any observed implausibility in such analysis can indicate issues in the weather inputs or require to pay attention in the calibration of model parameters which control the simulation of snow processes (snofall_tmp, snomelt_tmp, snomelt_lag). CS10, a Boreal catchment, is impacted by snow melt – this is relevant to the verification SWATdoctR::plot_monthly_snow(sim_verify = veri_no_stress) This has been resolved in issue #43. (link). Are these values plausible csilla? In situations where not all required climate inputs are available which are necessary to estimate PET with PM method the estimates will be more uncertain and annual PET sums may differ to regional values. Then the use of a simpler method for the calculation of PET can be a valid solution. We do not need a simpler method since we have the required data STATUS: Waiting on climate verification before continuing to the next step: 11.3 Simulation of management operations waiting on step 1… "]]
