[["index.html", "OPTAIN CS-10 Introduction", " OPTAIN CS-10 Moritz Shore, Csilla Farkas 2023-05-31 Introduction The aim of page is to: Document the process of setting up the SWAT+ and SWAP models for Kraakstad, Norway (Case study 10 / CS10) Link and execute all workflow steps in parallel with the documentation using Rmarkdown Make the model setup process reproducible Be a reference to the CS10 modelers, as well as other OPTAIN modelers. If all goes well, the compiling of this Gitbook will also compile the OPTAIN model setup in tandem, from source input files to final model setup (..and beyond!) The model setup follows the OPTAIN modelling protocols for SWAP (Farkas et al. 2023) and SWAT+ (Schürz et al. 2022). We begin with SWAT+ and the model setup with SWATBuildR, in Section 1. References "],["model-setup-with-swatbuildr.html", "Section 1 Model setup with SWATBuildR 1.1 BuildR Setup 1.2 Processing input data 1.3 Calculating Contiguous Object Connectivity 1.4 Generate SWAT+ input", " Section 1 Model setup with SWATBuildR print(paste(&quot;Code executed on&quot;, Sys.time())) ## [1] &quot;Code executed on 2023-05-31 11:25:00.399744&quot; The OPTAIN project is using the COCOA approach (Schürz et al. 2022) and as such, needs to use the SWATBuildR package to build the SWAT+ model setup and calculate the connectivity between HRUs in the catchment. This chapter covers this process. 1.1 BuildR Setup We will need the following packages for this chapter: # not loading most packages because they interfere with the `install_load` # functions of BuildR. this means I need to prefix everything with :: until # Christoph turns BuildR into a proper package with proper scoping. # require(dplyr) # require(ggplot2) # require(mapview) # require(sf) # require(raster) # require(terra) require(magrittr) require(raster) # Common ggplot theme for simple features sf_theme &lt;- ggplot2::theme(axis.text.x=ggplot2::element_blank(), axis.ticks.x=ggplot2::element_blank(), axis.text.y=ggplot2::element_blank(), axis.ticks.y=ggplot2::element_blank() ) We will define the location of our project here, and give it a name: project_path &lt;- &#39;model_data/cs10_setup&#39; project_name &lt;- &#39;optain-cs10&#39; # run the buildR? run_buildr = FALSE # load the BuildR source(&#39;model_data/swat_buildR/init.R&#39;) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:raster&#39;: ## ## intersect, select, union ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:crayon&#39;: ## ## %+% ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:raster&#39;: ## ## intersect, union ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union ## ## Attaching package: &#39;purrr&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## set_names ## ## Attaching package: &#39;tidyr&#39; ## The following object is masked from &#39;package:raster&#39;: ## ## extract ## The following object is masked from &#39;package:magrittr&#39;: ## ## extract ## ## Attaching package: &#39;vroom&#39; ## The following objects are masked from &#39;package:readr&#39;: ## ## as.col_spec, col_character, col_date, col_datetime, col_double, ## col_factor, col_guess, col_integer, col_logical, col_number, ## col_skip, col_time, cols, cols_condense, cols_only, date_names, ## date_names_lang, date_names_langs, default_locale, fwf_cols, ## fwf_empty, fwf_positions, fwf_widths, locale, output_column, ## problems, spec ## Linking to liblwgeom 3.0.0beta1 r16016, GEOS 3.11.2, PROJ 9.2.0 ## Linking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE ## terra 1.7.29 ## ## Attaching package: &#39;terra&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## The following objects are masked from &#39;package:magrittr&#39;: ## ## extract, inset ## udunits database from C:/Users/NIBIO/AppData/Local/R/win-library/4.3/units/share/udunits/udunits2.xml # Note the many &quot;maskings&quot; occuring here, dangerous! 1.2 Processing input data SWATBuildR reads input data, performs some checks on it, and saves it in a compatible format for subsequent calculations. Little documentation exists on the creation of these data sets. IF you have any information to add, feel free to do so! 1.2.1 High-Resolution Digital Elevation Model (DEM) The high-resolution DEM is the basis for calculation of water connectivity, among other things. Most of the documentation of the creation of the DEM for CS10 has been lost to the sands of time. All we know is that it is located here: dem_path &lt;- &quot;model_data/input/elevation/dtm3_ns_v5.tif&quot; dem_rast &lt;- raster::raster(dem_path) plot(dem_rast) (#fig:buildr_plotdemraster)CS10 Digital elevation model (DEM). To our knowledge, it has a 10 meter resolution. 1 meter resolution was available but there seems to have been issues with using it. It is definitely preferable to use the 1m DEM as certain important information can be lost with (max allowed resolution) of 10m. An example of a hydrologically effective landscape features being lost due to coarse DEM resolution. From (Schürz et al. 2022) 1.2.2 Processing the Basin Boundary The basin boundary has presumably been created using the defined outlet point of the catchment and the DEM. No more is currently known about this file other than that it is located here: bound_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; bound_sf &lt;- sf::read_sf(bound_path) basin &lt;- ggplot2::ggplot(bound_sf) + ggplot2::geom_sf() + sf_theme print(basin) (#fig:buildr_watershedplot)CS10 Basin, caclulated from the DEM. The following code and commentary is from version 1.5.14, written by Christoph Schuerz. BuildR recommends all layers to be in the same CRS, if we set project_layer to FALSE, it will throw an error when this is not the case: The input layers might be in different coordinate reference systems (CRS). It is recommended to project all layers to the same CRS and check them before using them as model inputs. The model setup process checks if the layer CRSs differ from the one of the basin boundary. By setting ‘proj_layer &lt;- TRUE’ the layer is projected if the CRS is different. If FALSE different CRS trigger an error. project_layer &lt;- TRUE We read in and check the basin boundary and run some checks bound &lt;- read_sf(bound_path) %&gt;% select() set_proj_crs(bound, data_path) check_polygon_topology(layer = bound, data_path = data_path, label = &#39;basin&#39;, n_feat = 1, checks = c(F,T,T,T,F,F,F,F)) 1.2.3 Processing the Land layer Our land layer is located here: land_path &lt;- &quot;model_data/input/land/CS10_LU.shp&quot; Documentation on its creation does not exist. lu_shp &lt;- sf::read_sf(land_path) lu_map &lt;- ggplot2::ggplot(lu_shp) + ggplot2::geom_sf(ggplot2::aes(fill = type)) + sf_theme + ggplot2::theme(legend.position = &quot;none&quot;) print(lu_map) (#fig:buildr_maplanduse)Land use map of CS10 by Farm ID We had an issue with the classification of the land uses. For the OPTAIN project, all agricultural fields must have a unique ID, and our land uses only had the ID of the given farm KGB (which had many different fields). To remedy this, new IDs were generated with the format a_###f_# where a_ represents the farm, and f_ represents the respective field of that farm. The farm names needed to be shortened because the SWAT+ model often cannot handle long ID names (longer than 16 characters) Note, the potential measures polygons were not counted as individual fields, which is why SP_ID does not match up with field_ID – This is by design. readr::read_csv(&quot;model_data/farm_id/a_f_id.csv&quot;, show_col_types = F) %&gt;% head() ## # A tibble: 6 × 4 ## KGB type_fr sp_id field_ID ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 213/8/1 a_084 1 a_084f_1 ## 2 213/7/6 a_083 2 a_083f_1 ## 3 213/7/6 a_083 3 a_083f_2 ## 4 213/7/4 a_082 4 a_082f_1 ## 5 213/65/1 a_081 5 a_081f_1 ## 6 213/65/1 a_081 6 a_081f_2 This was done in a simple QGIS workflow of dissolving by farm, splitting from single part to multipart, and then adding an iterating ID per farm field. This workflow could be replicated in R, and then shown here. It is under consideration… BuildR will now run some checks on our land layer. land &lt;- read_sf(land_path) %&gt;% check_layer_attributes(., type_to_lower = FALSE) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;land&#39;, type = &#39;vector&#39;) check_polygon_topology(layer = land, data_path = data_path, label = &#39;land&#39;, area_fct = 0.00, cvrg_frc = 99.9, checks = c(T,F,T,T,T,T,T,T)) BuildR splits the land layer into HRU (land) and reservoir (water) objects split_land_layer(data_path) 1.2.4 Processing the Channels No documentation exists on the creation of the channels layer, all we know is that it is located here: channel_path &lt;- &#39;model_data/input/line/cs10_channels.shp&#39; channels &lt;- read_sf(channel_path) %&gt;% dplyr::select(&quot;type&quot;) channel_map &lt;- ggplot2::ggplot() + ggplot2::geom_sf(data = bound_sf) + ggplot2::geom_sf(data = channels, mapping = ggplot2::aes(color = type)) + sf_theme channel_map (#fig:buildr_channel_plot_x)CS10 Channels including surface and subsurface channels BuildR runs some checks: channel &lt;- read_sf(channel_path) %&gt;% check_layer_attributes(., type_to_lower = TRUE) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;channel&#39;, type = &#39;vector&#39;) check_line_topology(layer = channel, data_path = data_path, label = &#39;channel&#39;, length_fct = 0, can_cross = FALSE) BuildR then checks the connectivity between the channels and reservoirs. For this we need to define our id_cha_out and id_res_out. “Variable id_cha_out sets the outlet point of the catchment. Either define a channel OR a reservoir as the final outlet. If channel then assign id_cha_out with the respective id from the channel layer. If reservoir then assign the respective id from the land layer to id_res_out, otherwise leave as NULL” id_cha_out &lt;- 37 id_res_out &lt;- NULL Running connectivity checks between channels and reservoirs: check_cha_res_connectivity(data_path, id_cha_out, id_res_out) Checking if any defined channel ids for drainage from land objects do not exist check_land_drain_ids(data_path) 1.2.5 Processing the DEM BuildR loads and checks the DEM, and saves it. dem &lt;- rast(dem_path) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;dem&#39;, type = &#39;raster&#39;) check_raster_coverage(rst = dem, vct_layer = &#39;land&#39;, data_path = data_path, label = &#39;dem&#39;, cov_frc = 0.95) save_dem_slope_raster(dem = dem, data_path = data_path) Here is the result: map &lt;- raster::raster(&quot;model_data/cs10_setup/optain-cs10/data/raster/dem.tif&quot;) plot(map) (#fig:buildr_demcropplot)CS10 DEM cropped to basin 1.2.6 Processing soil data Our soil map is located here: soil_layer_path &lt;- &#39;model_data/input/soil/soil_layer.tif&#39; No documentation exists on its creation. plot(raster::raster(soil_layer_path)) (#fig:buildr_soilrastermapplot)CS10 Soil map The soil data and look-up path are located here: soil_lookup_path &lt;- &#39;model_data/input/soil/soil_lookup.csv&#39; soil_data_path &lt;- &#39;model_data/input/soil/UserSoil_Krakstad.csv&#39; Not much documentation exists here either. Some can be found in the excel sheet: ~/model_data/input/soil/swatsoil2.xlsx BuildR reads in the soil data, performs checks, processes, and saves. soil &lt;- rast(soil_layer_path) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;soil&#39;, type = &#39;raster&#39;) check_raster_coverage(rst = soil, vct_layer = &#39;hru&#39;, data_path = data_path, label = &#39;soil&#39;, cov_frc = 0.75) save_soil_raster(soil, data_path) BuildR then generates a table with aggregated elevation, slope, soil for HRU units. aggregate_hru_dem_soil(data_path) Read and prepare the soil input tables and a soil/hru id table and write them into data_path/tables.sqlite build_soil_data(soil_lookup_path, soil_data_path, data_path) 1.3 Calculating Contiguous Object Connectivity SWATBuildR follows COCOA. This section contains the calculations. You can read more about it in the protocol. 1.3.1 Calculating land unit connectivity Preparing raster layers based on the DEM and the surface channel objects that will be used in the calculation of the land object connectivity. prepare_terrain_land(data_path) The connection of each land object to neighboring land and water objects is calculated based on the flow accumulation and the D8 flow pointer along the object edge calculate_land_connectivity(data_path) 1.3.1.1 Eliminate land object connections with small flow fractions: For each land object the flow fractions are compared to connection with the largest flow fraction of that land object. Connections are removed if their fraction is smaller than frc_thres relative to the largest one. This is necessary to: Simplify the connectivity network To reduce the risk of circuit routing between land objects. Circuit routing will be checked. If an error due to circuit routing is triggered, then ‘frc_thres’ must be increased to remove connectivities that may cause this issue. frc_thres &lt;- 0.3 The remaining land object connections are analyzed for infinite loop routing. For each land unit the connections are propagated and checked if the end up again in the same unit. # reduce_land_connections(data_path, frc_thres) %&gt;% # check_infinite_loops(., data_path, &#39;Land&#39;) # Analyzing land objects for infinite loop routing: Completed 6185 Land objects # in 10M 44S # # ✘ 150 Land objects identified where water is routed in loops. # # You can resolve this issue in the following ways: # # - Use the layer &#39;land_infinite_loops.gpkg&#39; that was written to # model_data/cs10_setup/optain-cs10/data/vector to identify land polygons that # cause the issue and split them to break the loops This would require to # restart the entire model setup procedure! # # - Increase the value of &#39;frc_thres&#39;. # This reduces the number of connections of each land unit (maybe undesired!) # and can remove the connections that route the water in loops. # # - Continue with the model setup (only recommended for small number of identified units!). # The function &#39;resolve_loop_issues()&#39; will then eliminate a certain number of # connections. What do we do here ? – I think in the past we just removed all the loops, but it says here it is not recommended when there are a lot of them. Should we increase the threshold? Should we try to break up the loops? You can see the problem polygons on the map below: land_inifnite_loops_shp &lt;- sf::read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land_infinite_loops.gpkg&quot;) mapview::mapview(land_inifnite_loops_shp) To me it seems to complicated to try to break up the loops, maybe increase the threshold a bit and see what happens? and delete the rest? Increasing threshold and re-running leads to this: frc_thres &lt;- 0.1 # loops ( mins) frc_thres &lt;- 0.2 # 238 loops (30 mins) frc_thres &lt;- 0.3 # 150 loops (12 mins) frc_thres &lt;- 0.4 # 115 loops ( mins) frc_thres &lt;- 0.5 # 94 loops ( mins) frc_thres &lt;- 0.6 # 57 loops ( mins) We have decided that at a threshold of 0.3 and 150, we are within the “not that many” loops territory, considering out of 6000+ HRUs, it is only around 2%. Therefore we will let BuildR continue and remove them. frc_thres &lt;- 0.3 # 150 loops reduce_land_connections(data_path, frc_thres) %&gt;% check_infinite_loops(., data_path, &#39;Land&#39;) 1.3.1.2 Resolve infinite loops If infinite loops were identified this routine tries to resolve the issues by selectively removing connections between land units in order to get rid of all infinite loops. resolve_loop_issues(data_path) 1.3.2 Calculating channel/reservoir connectivity 1.3.2.1 Calculating the water object connectivity the function returns the cha and res con_out tables in SWAT+ database format and writes them into data_path/tables.sqlite build_water_object_connectivity(data_path) 1.3.3 Checking the water objects for infinite loops From the cha_res_con_out tables id_from/id_to links are generated and checked for infinite loop routing. prepare_water_links(data_path) %&gt;% check_infinite_loops(., data_path, &#39;Water&#39;, Inf) 1.3.4 Terrain properties Calculate terrain properties such as elevation, slope, catchment area, channel width/depth for channel and reservoir objects and write them into data_path/tables.sqlite prepare_terrain_water(data_path) 1.4 Generate SWAT+ input 1.4.1 Generate land object SWAT+ input tables Build the landuse.lum and a landuse/hru id table and write them into data_path/tables.sqlite build_landuse(data_path) Build the HRU SWAT+ input files and write them into data_path/tables.sqlite build_hru_input(data_path) Add wetlands to the HRUs and build the wetland input files and write them into data_path/tables.sqlite We only use the wetf land use . wetland_landuse &lt;- c(&#39;wehb&#39;, &#39;wetf&#39;, &#39;wetl&#39;, &#39;wetn&#39;) add_wetlands(data_path, wetland_landuse) 1.4.2 Generate water object SWAT+ input tables Build the SWAT+ cha input files and write them into data_path/tables.sqlite build_cha_input(data_path) Build the SWAT+ res input files and write them into data_path/tables.sqlite build_res_input(data_path) Build SWAT+ routing unit con_out based on ‘land_connect_fraction’. build_rout_con_out(data_path) Build the SWAT+ rout_unit input files and write them into data_path/tables.sqlite build_rout_input(data_path) Build the SWAT+ LSU input files and write them into data_path/tables.sqlite build_ls_unit_input(data_path) 1.4.3 Build aquifer input Build the SWAT+ aquifer input files for a single aquifer for the entire catchment. The connectivity to the channels with geomorphic flow must be added after writing the txt input files. This is not implemented in the script yet. build_single_aquifer_files(data_path) 1.4.4 Add point source inputs The point source locations are provided with a point vector layer in the path ‘point_path’. We have yet to complete this step. It is being tracked in Issue # 21 point_path &lt;- &#39;model_data/input/point/cs10_pointsource.shp&#39; point_sf &lt;- sf::read_sf(point_path) point_map &lt;- mapview::mapview(point_sf, zcol = &quot;GRAD_P&quot;, cex = &quot;GRAD_N&quot;) mapview::mapview(bound_sf, alpha.regions = 0.2)+point_map Map of point sources, colored by (assumed) phosphorous and size by (assumed) Nitrogen Maximum distance of a point source to a channel or a reservoir to be included as a point source object (recall) in the model setup: max_point_dist &lt;- 500 # meters Point source records can automatically be added from files in the same folder as the point source location layer. To be identified as point source data the files must be named as &lt;name&gt;_&lt;interval&gt;.csv, where &lt;name&gt; must be the name of a point int the vector layer and &lt;interval&gt; must be one of const, yr, mon, or day depending on the time intervals in the input data. add_point_sources(point_path, data_path, max_point_dist) 1.4.5 Create SWAT+ sqlite database 1.4.5.1 Write the SWAT+Editor project database The database will be located the ‘project_path’. create_swatplus_database(project_path, project_name) References "],["climate-inputs-and-weather-generator.html", "Section 2 Climate inputs and weather generator 2.1 Preparation and loading data 2.2 Creating the weather generator 2.3 Adding weather data to SWAT+ project 2.4 Adding Atmospheric deposition", " Section 2 Climate inputs and weather generator 2.1 Preparation and loading data To add weather and climate data to our SWAT project, we will use svatools (Svajunas, Rets, and Strauch 2023) 2.1.1 Required packages require(euptf2) # devtools::install_github(&quot;tkdweber/euptf2&quot;) require(svatools) # devtools::install_github(&quot;biopsichas/svatools&quot;) require(readr) require(readxl) require(sf) require(mapview) require(ggpot2) ## Warning in library(package, lib.loc = lib.loc, character.only = TRUE, ## logical.return = TRUE, : there is no package called &#39;ggpot2&#39; require(tidyr) 2.1.2 Load in the file(s) and load svatools template First we load in the template and fill it in with our values and rename it to “cs10_weather_data.xlsx”. This is not done within R. No detailed documentation exists on the source of this data (yet). #temp_path &lt;- system.file(&quot;extdata&quot;, &quot;weather_data.xlsx&quot;, package = &quot;svatools&quot;) # /// fill out this template and save &quot;cs10_weather_data.xlsx&quot; /// We are using the following projection for this project: epgs_code = 25832 Now we can load it in with Svatools. met_lst &lt;- load_template(template_path = &quot;model_data/input/met/cs10_weather_data.xlsx&quot;, epgs_code) ## [1] &quot;Loading data from template.&quot; ## [1] &quot;Reading station ID1 data.&quot; ## [1] &quot;Loading of data is finished.&quot; 2.1.3 Proof the station plot_weather(met_lst, &quot;PCP&quot;, &quot;month&quot;, &quot;sum&quot;) Checking the location of the station: basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; basin &lt;- st_transform(st_read(basin_path), epgs_code) %&gt;% mutate(NAME = &quot;Basin&quot;) ## Reading layer `cs10_basin&#39; from data source ## `C:\\Users\\NIBIO\\Documents\\GitLab\\github\\optain-cs10\\model_data\\input\\shape\\cs10_basin.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1 feature and 1 field ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 603439.4 ymin: 6607792 xmax: 610984.4 ymax: 6622017 ## Projected CRS: ETRS89 / UTM zone 32N stations &lt;- st_transform(met_lst$stations, epgs_code) mapview(stations) + mapview(basin) 2.2 Creating the weather generator We only have one weather station, which means only one weather generator. wgn &lt;- prepare_wgn(met_lst, TMP_MAX = met_lst$data$ID1$TMP_MAX, TMP_MIN = met_lst$data$ID1$TMP_MIN, PCP = met_lst$data$ID1$PCP, RELHUM = met_lst$data$ID1$RELHUM, WNDSPD = met_lst$data$ID1$WNDSPD, MAXHHR = met_lst$data$ID1$MAXHHR, SLR = met_lst$data$ID1$SLR) ## [1] &quot;Coordinate system checked and transformed to EPSG:4326.&quot; ## [1] &quot;Working on station ID1:WS_AAS&quot; write.csv(wgn$wgn_st, &quot;model_data/input/met/as_wgn_st.csv&quot;, row.names = FALSE, quote = FALSE) write.csv(wgn$wgn_data, &quot;model_data/input/met/as_wgn_data.csv&quot;, row.names = FALSE, quote = FALSE) 2.3 Adding weather data to SWAT+ project This adds the weather generator and weather stations to our SWAT+ project db_path &lt;- &quot;model_data/cs10_setup/optain-cs10/optain-cs10.sqlite&quot; add_weather(db_path, met_lst, wgn) 2.4 Adding Atmospheric deposition Instructions and documentation can be found here Getting the data: basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; df &lt;- get_atmo_dep( basin_path, start_year = 2010, end_year = 2020, t_ext = &quot;year&quot; ) readr::write_csv(df, file = &quot;model_data/input/met/atmodep.csv&quot;) A plot of the results: df &lt;- readr::read_csv(&quot;model_data/input/met/atmodep.csv&quot;, show_col_types = F) ggplot(pivot_longer(df, !DATE, names_to = &quot;par&quot;, values_to = &quot;values&quot;), aes(x = DATE, y = values))+ geom_line()+ facet_wrap(~par, scales = &quot;free_y&quot;)+ theme_bw() (#fig:svatools_plotatmodep2)Atmospheric Deposition data grabbed by svatools Adding the data to the SQLITE db_path &lt;- &quot;model_data/cs10_setup/optain-cs10/optain-cs10.sqlite&quot; add_atmo_dep(df, db_path) Big thank you to Svajunas for his svatools package, making our lives much easier! References "],["writing-swat-input-files.html", "Section 3 Writing SWAT+ Input Files 3.1 Loading the project in SWAT+ Editor 3.2 SWAT+ Test Run 3.3 Link aquifers and channels with geomorphic flow", " Section 3 Writing SWAT+ Input Files The SWATBuildR cannot write the actual text files for our model setup, we have to use the SWAT+ Editor for this. Once the input files have been written, we will try to ONLY modify these parameters with R and never edit parameters in the Editor. The reason for this is because changing the values of the text files will not change the values of the sqlite database. If the input files are re-written by the editor, our changes to the text files will be overwritten. 3.1 Loading the project in SWAT+ Editor Open a project Path to the project sqlite Project Information. The weather generator has been complete by svatools in Section 2.3. After clicking on the Run Model / Save Scenario button, you arrive at the “Confirm Simulation Settings” page. Here you need to choose where to write your input files. We have chosen cs10_setup/swat_input. We have also changed our simulation time period, but this is not required. Make sure to un-check “Run SWAT” and “Analyze output for visualization” “Save settings &amp; Run Selected” Confirm Simulation Settings. Make sure to follow the instructions carefully here. Your SWAT+ input files will be generated, and you will be prompted to Save Scenario. This is not recommended, because in our testing, the scenario saving would recursively generate new scenarios within the same folder, until all file space had been exhausted on the drive. This extremely deep folder brings windows to its knees, even when trying to delete it using explorer (Use Powershell instead). Option to save scenario. Not Recommended. 3.2 SWAT+ Test Run We will copy in our SWAT+ executable, our input files, and our weather files into a folder and run SWAT+ as a test. For some reason, when writing the SWAT+ input files, it does not write the weather station data. I am not sure if this is intended or not. the SWAT+ model still runs fine, however maybe it is using the weather generator? The meteo input fles are located in the same directory as the sqlite, probably because svatools put them there? #TODO Our current SWAT+ version is rev60.5.4_64rel.exe but this is subject to change Make a run directory and enable code running. # if set to true, following code will be evalutated. run_this_chapter = FALSE dir.create(&quot;model_data/cs10_setup/run_swat&quot;, showWarnings = F) Copy all required files into this directory: sta_files &lt;- list.files(&quot;model_data/cs10_setup/optain-cs10/&quot;, pattern = &quot;sta_&quot;, full.names = T) cli_files &lt;- list.files(&quot;model_data/cs10_setup/optain-cs10/&quot;, pattern = &quot;.cli&quot;, full.names = T) input_files &lt;- list.files(&quot;model_data/cs10_setup/swat_input/&quot;, full.names = T) path_to_swat &lt;- &quot;model_data/cs10_setup/rev60.5.4_64rel.exe&quot; source_files &lt;- c(sta_files, cli_files, input_files, path_to_swat) status &lt;- file.copy(from = source_files, to = &quot;model_data/cs10_setup/run_swat/&quot;, overwrite = T) Lets run some checks on these files if(any(status == FALSE)){ warning(&quot;Some files were not copied:&quot;) print(source_files[which(status==FALSE)]) } print( paste( length(which(status)), &quot;of&quot;, length(source_files), &quot;files were copied into the run folder, amounting to a size of &quot;, round(sum(file.info(source_files)$size * 1e-6), 2), &quot;megabytes&quot; ) ) Now lets run the model to make sure it works # update time sim time_sim &lt;- readLines(&quot;model_data/cs10_setup/run_swat/time.sim&quot;) time_sim[3] &lt;- &quot; 0 2010 0 2011 0 &quot; writeLines(text = time_sim, con = &quot;model_data/cs10_setup/run_swat/time.sim&quot;) msg &lt;- processx::run(command = &quot;rev60.5.4_64rel.exe&quot;, wd = &quot;model_data/cs10_setup/run_swat/&quot;) simout &lt;- readLines(&quot;model_data/cs10_setup/run_swat/simulation.out&quot;) cat(tail(simout), sep = &quot;\\n&quot;) It will be useful to test SWAT+ along our journey, so we will reuse this code in the function test_swat() source(&quot;model_data/code/test_swat.R&quot;) We can now continue with our final buildR step 3.3 Link aquifers and channels with geomorphic flow A SWATBuildR model setup only has one single aquifer (in its current version). This aquifer is linked with all channels through a channel-aquifer-link file (aqu_cha.lin) in order to maintain recharge from the aquifer into the channels using the geomorphic flow option of SWAT+. The required input file cannot be written with the SWAT+Editor. Therefore it has to be generated in a step after writing the model text input files with the SWAT+Editor. Path of the TxtInOut folder (project folder where the SWAT+ text files are written with the SWAT+Editor) txt_path &lt;- &#39;model_data/cs10_setup/swat_input/&#39; Linking the aquifer to the channels project_path &lt;- &#39;model_data/cs10_setup&#39; project_name &lt;- &#39;optain-cs10&#39; source(&#39;model_data/swat_buildR/init.R&#39;) link_aquifer_channels(txt_path) This created the file aqu_cha.lin and changed file.cio to point to it (row 16, column 3) Let us test to see if SWAT+ still runs simout &lt;- test_swat() cat(tail(simout), sep = &quot;\\n&quot;) Success. # remove the swat+ run unlink(&quot;model_data/cs10_setup/run_swat/&quot;, recursive = T) "],["crop-map-generation.html", "Section 4 Crop Map Generation 4.1 Calculating Field Area 4.2 Generating the Crop Rotation 4.3 Merging crop rotation with BuildR output", " Section 4 Crop Map Generation As OPTAIN takes into account the individual field, we need to know what is growing on which field and when. Unfortunately our data foundation for this task is quite lackluster, but we will try out best to do so. in the following chapter. require(sf) require(dplyr) require(readr) require(stringr) require(readxl) require(reshape2) require(tibble) require(DT) require(ggplot2) require(gifski) sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) 4.1 Calculating Field Area We have data based on what area certain crops have per farm. In order to relate this to our spatial land use map, we require the area of both the farms, and the fields within these farms. The basis for these calculations comes from the BuildR output: (REF header here) lu_sf &lt;- read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land.shp&quot;) lu_map &lt;- ggplot() + geom_sf(lu_sf, mapping = aes(fill = type)) + theme(legend.position = &quot;left&quot;) + sf_theme + theme(legend.position = &quot;none&quot;) print(lu_map) (#fig:crop_map_lumap)Land use map for CS10, colored by type. 4.1.1 Calculate Field Area and Assign IDs This stage has been completed in QGIS. An R-implementation is being considered. farm_area_sf &lt;- read_sf(&quot;model_data/input/crop/buildr_output_dissolved_into_farms.shp&quot;) field_lu_plot &lt;- ggplot() + geom_sf(farm_area_sf, mapping = aes(fill = farm_id)) + theme(legend.position = &quot;none&quot;) +sf_theme print(field_lu_plot) (#fig:crop_map_farmidfig)Farms in the CS10 catchment, colored by ID 4.1.2 Calculate Field area and assign IDs This stage has been completed in QGIS. An R-implementation is being considered. 4.2 Generating the Crop Rotation The following algorithm takes information from reporting farm, and their respective area in the SWAT+ setup, and combines these data sets to generate a plausible crop rotation. Note, pasture refers to meadow (TODO: fix!) crop_data &lt;- read_excel(&quot;model_data/input/crop/crop_rotation_source_data.xlsx&quot;) datatable(crop_data) 4.2.1 Tidy up the source data: We are converting the source data to tidy format in the following code snippet crop_names &lt;- melt( crop_data, id.vars = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;, &quot;pastrure_area_daa&quot;), measure.vars = c(&quot;crop1&quot;, &quot;crop2&quot;, &quot;crop3&quot;), variable.name = &quot;crop_status&quot;, value.name = &quot;crop&quot; ) %&gt;% tibble() crop_area &lt;- melt( crop_data, id.vars = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;, &quot;pastrure_area_daa&quot;), measure.vars = c(&quot;percent_crop1&quot;, &quot;percent_crop2&quot;, &quot;percent_crop3&quot;), variable.name = &quot;crop_status&quot;, value.name = &quot;area&quot; ) %&gt;% tibble() crop_area$crop_status &lt;- crop_area$crop_status %&gt;% str_remove(&quot;percent_&quot;) # unstable, TODO fix! colnames(crop_area)[6] &lt;- &quot;crop_area_percent&quot; crop_tidy &lt;- left_join(crop_names, crop_area, by = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;,&quot;pastrure_area_daa&quot;, &quot;crop_status&quot;)) %&gt;% dplyr::select(-crop_status) # unstable, TODO fix! colnames(crop_tidy)[3] &lt;- &quot;farm_area_daa&quot; crop_tidy$crop_area_percent &lt;- (crop_tidy$crop_area_percent/100) %&gt;% round(2) crop_tidy &lt;- crop_tidy %&gt;% filter(crop %&gt;% is.na() == FALSE) datatable(crop_tidy) 4.2.2 Tidy up the BuildR output The maps shown in secttion (ref) were exported to CSV in QGIS. An R-implementation might be written in here sometime (#TODO). This CSV data needs to be re-formatted to be tidy as well. fields &lt;- read_csv( &quot;model_data/input/crop/buildr_landuse_dissolved_into_fields.csv&quot;, show_col_types = F ) %&gt;% dplyr::select(type, farm_id, field_area_daa) farms &lt;- read_csv( &quot;model_data/input/crop/buildr_landuse_dissolved_into_farms.csv&quot;, show_col_types = F ) %&gt;% dplyr::select(type, farm_id , farm_area_) # fix farm_area_ TODO # unstable, fix! TODO colnames(farms)[3] &lt;- &quot;farm_area_daa&quot; farms$farm_area_daa &lt;- farms$farm_area_daa %&gt;% round(2) fields$field_area_daa &lt;- fields$field_area_daa %&gt;% round(2) reported_data &lt;- left_join(fields, farms, by = &quot;farm_id&quot;) # unstable, fix! TODO colnames(reported_data)[1] &lt;- &quot;field_id&quot; reported_data &lt;- reported_data %&gt;% dplyr::select(field_id, farm_id, field_area_daa, farm_area_daa) datatable(reported_data) 4.2.3 Field classification We are now ready to perform our classification. We will store our results in this predefined dataframe: classed_fields &lt;- tibble( farm_id = NA, field_id = NA, farm_area_daa = NA, field_area_daa = NA, crop = NA, year = NA ) The following reporting farms will be used for the classification: farms &lt;- reported_data$farm_id %&gt;% unique() For the following years (AKA we have data for these years): years &lt;- c(2016, 2017, 2018, 2019) This for loop runs through all the fields and classifies them. The output will not be printed in this file, but will be saved in a log file, that you can dig out of the repository, located here: # TODO add the log file generation The following code is not executed: # For every year for(c_year in years){ # For every farm, for (farm in farms) { # Do the following: # Filter the source and buildR data to the given year and farm crop_filter &lt;- crop_tidy %&gt;% filter(year == c_year) %&gt;% filter(farm_id == farm) hru_filter &lt;- reported_data %&gt;% filter(farm_id == farm) # Behavior if the farm has no reporting data: if(crop_filter$farm_id %&gt;% length() == 0){ # Set all fields to winter wheat. farm_fields &lt;- hru_filter %&gt;% dplyr::select(farm_id, field_id, farm_area_daa, field_area_daa) %&gt;% arrange(desc(field_area_daa)) farm_fields$crop = &quot;wwht&quot; farm_fields$year = c_year # Add them to the results dataframe classed_fields &lt;- rbind(classed_fields, farm_fields) # and skipping to next farm next() } # Behavior, if there is reporting data on the farm&quot; # Find the area of the farm (all elements in vector are the same, so ok # to just grab the first) farm_area_hru &lt;- hru_filter$farm_area_daa[1] # Find the discrepency between &quot;SHOULD BE&quot; crop area and &quot;CURRENTLY IS&quot; # crop area. We want this as an absolute value. area_dis &lt;- (hru_filter$farm_area_daa[1]-crop_filter$farm_area_daa[1]) %&gt;% round(2) %&gt;% abs() # Some farms have no reported area. in this case we set it to some random # negative number. if(area_dis %&gt;% is.na()){area_dis = -999} # Extracting the area of the pasture and farm (all vectors same value, ok # to just take the first) past_area &lt;- crop_filter$pastrure_area_daa[1] farm_area &lt;- crop_filter$farm_area_daa[1] # Extract relevant fields farm_fields &lt;- hru_filter %&gt;% dplyr::select(farm_id, field_id, farm_area_daa, field_area_daa) %&gt;% # and sort by area arrange(desc(field_area_daa)) # pre-define crops column to be NA farm_fields$crop = NA # Bollean check to see if the field has been meadow in a previous year. If # it has, then we want to continue planting meadow on it (This was a # decision we made.) has_meadow &lt;- (classed_fields %&gt;% filter(field_id %in% farm_fields$field_id) %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_id) %&gt;% pull() %&gt;% length() &gt; 0) ### Determining how much land the crops should cover. # This if statement checks if is currently a pasture portion for the reporting # farm, or if there has been in the past. if((past_area %&gt;% is.na() == FALSE) | has_meadow) { # If it does, or did, then this special routine is enacted: # Determines the pasture percentage of the total farm past_percent &lt;- (past_area/farm_area) # If it cannot be calculated, then it is set to 0 if(past_percent %&gt;% is.na()){past_percent = 0} # The crop fractions provided to us in the source data did not account for # the pasture fraction. therefore we need to reduce the other crop # fractions by this amount. adjuster &lt;- past_percent / length(crop_filter$crop) crop_filter$crop_area_percent &lt;- crop_filter$crop_area_percent-adjuster # creating a new entry for pasture and adding it to the now updated crop\\ # fractions list past_row &lt;- tibble( farm_id = farm, year = c_year, farm_area_daa = crop_filter$farm_area_daa[1], pastrure_area_daa = past_area, crop = &quot;meadow&quot;, crop_area_percent = past_percent ) crop_filter &lt;- rbind(crop_filter, past_row) # Creating an updated &quot;SHOULD BE&quot; and &quot;CURRENTLY IS&quot; crop coverage # datafram crop_coverage &lt;- tibble( crop = crop_filter$crop, to_cover = (farm_area_hru*crop_filter$crop_area_percent)) # Setting the initial actual coverage to 0 (&quot;CURRENTLY IS&quot;) crop_coverage$actual &lt;- 0 # This extracts the fields that were previously pasture fields. previously_pasture &lt;- classed_fields %&gt;% filter(field_id %in% farm_fields$field_id) %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_id) %&gt;% pull() if (length(previously_pasture) &gt; 0) { # if some of the fields were previously pasture, they are set to pasture # again: farm_fields$crop[ which(farm_fields$field_id %in% previously_pasture)] = &quot;meadow&quot; # We save the amount of area the meadow crops cover, so that we can # adjust the other &quot;SHOULD BE&quot; fractions correctly. custom_actual &lt;- farm_fields %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_area_daa) %&gt;% pull() %&gt;% sum(na.rm = T) crop_coverage$actual[which(crop_coverage$crop==&quot;meadow&quot;)] = custom_actual } # Now the algorithm continues as normal. # This is the routine that is run, when no meadow is detected: }else{ # creating a tibble which tracks how much land the crops SHOULD cover crop_coverage &lt;- tibble( crop = crop_filter$crop, to_cover = (farm_area_hru * crop_filter$crop_area_percent) ) # pre define crop coverage to 0 (CURRENT) crop_coverage$actual &lt;- 0 } ### Crop classification # Now we know how much land the crops SHOULD cover, it is time to assign # the fields accordingly. We do this with a WHILE loop, which keeps going # until we have classified every field. while(any(farm_fields$crop %&gt;% is.na())){ # run through all the crops in the crop coverage list and determine or # update their current coverage for (c_crop in crop_coverage$crop) { # figure out which index we are in the list crop_index &lt;- which(c_crop == crop_coverage$crop) # Determine/UPDATE the current actual coverage of the crop (sum) crop_coverage$actual[crop_index] &lt;- farm_fields %&gt;% filter(crop == c_crop) %&gt;% dplyr::select(field_area_daa) %&gt;% sum() } # Calculate the difference between crop SHOULD BE coverage and ACTUAL # coverage crop_coverage$diff &lt;- crop_coverage$actual-crop_coverage$to_cover # determine the maximum difference. max_diff &lt;- crop_coverage$diff %&gt;% min(na.rm = T) # determine which crop has the maximum difference max_diff_crop &lt;- crop_coverage$crop[which(crop_coverage$diff == max_diff) %&gt;% min(na.rm = T)] # determine the biggest field left un-classified. biggest_na_field &lt;- farm_fields %&gt;% filter(crop %&gt;% is.na()) %&gt;% arrange(desc(field_area_daa)) %&gt;% nth(1) # Set the field which is still NA, and also the biggest to the crop with # the maximum difference (This code seems weird, and could be # improved?) farm_fields$crop[which((farm_fields$crop %&gt;% is.na() == TRUE) &amp; farm_fields$field_area_daa == biggest_na_field$field_area_daa )] &lt;- max_diff_crop } # After having classified the field, we update the actual coverage value # the respective crop that has been classified. for (c_crop in crop_coverage$crop) { crop_index &lt;- which(c_crop == crop_coverage$crop) crop_coverage$actual[crop_index] &lt;- farm_fields %&gt;% filter(crop == c_crop) %&gt;% dplyr::select(field_area_daa) %&gt;% sum() } # save the year of this crop assignment in the dataframe farm_fields$year = c_year # add the classification to the result dataframe classed_fields &lt;- rbind(classed_fields, farm_fields) # and move onto the next farm } # for every year } # Remove the first NA line classed_fields &lt;- classed_fields[-1, ] Now we can have a look at our results (not evaluated) datatable(classed_fields) We can now extract the crop rotation from this datatable. (not evaluated) # the format of our final dataframe final_df &lt;- tibble(field = NA, y_2016 = NA, y_2017 = NA, y_2018 = NA, y_2019 = NA) # fields we need to extract field_ids &lt;- classed_fields$field_id %&gt;% unique() We will do this with this for loop (note this is bad R-practice. Should be done with something like pivot_longer (TODO)) for(c_field_id in field_ids) { crop_rotation &lt;- tibble(field = c_field_id) crops &lt;- classed_fields %&gt;% filter(field_id == c_field_id) %&gt;% dplyr::select(crop) %&gt;% t() %&gt;% as_tibble(.name_repair = &quot;minimal&quot;) colnames(crops) &lt;- c(&quot;y_2016&quot;, &quot;y_2017&quot;, &quot;y_2018&quot;, &quot;y_2019&quot;) crop_rotation &lt;- cbind(crop_rotation, crops) %&gt;% as_tibble() final_df &lt;- rbind(final_df, crop_rotation) } # Remove the first NA line crop_rotation &lt;- final_df[-1,] # Save the crop rotation in CSV format write_csv(x = crop_rotation, file = &quot;model_data/input/crop/cs10_crop_rotation.csv&quot;) Here is a look at the crop rotation: lu_shp &lt;- &#39;model_data/input/crop/land_with_cr.shp&#39; lu_sf &lt;- read_sf(lu_shp) years &lt;- paste0(&quot;y_&quot;, 2016:2019) for (year in years) { index &lt;- which(colnames(lu_sf) == year) lu_sf_filt &lt;- lu_sf[c(index,length(lu_sf))] plot &lt;- ggplot(data = lu_sf_filt) + geom_sf(color = &quot;black&quot;, aes(fill = get(year)))+ guides(fill=guide_legend(title=year))+ ggtitle(&quot;CS10 Catchment&quot;, &quot;crop rotation&quot;)+ theme(legend.position=&quot;right&quot;)+ theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) print(plot) } (#fig:crop_map_croprotgif)Generated crop map for years 2016-2019 4.2.4 Extrapolating the Crop Rotation For OPTAIN, the crop rotation needs to span from 1988 to 2020. We currently have 2016 to 2019. crop_rotation &lt;- read_csv(&quot;model_data/input/crop/cs10_crop_rotation.csv&quot;, show_col_types = F) rotation &lt;- crop_rotation %&gt;% dplyr::select(-field) crop_rotation_extrapolated &lt;- cbind(crop_rotation, rotation, rotation, rotation, rotation, rotation, rotation, rotation, rotation$y_2016) # set the column names to be in the correct format colnames(crop_rotation_extrapolated) &lt;- c(&quot;field&quot;, paste0(&quot;y_&quot;, 1988:2020)) 4.3 Merging crop rotation with BuildR output The output of SWATbuildR “land.shp” needs to be connected to the newly generated crop map. lu &lt;- read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land.shp&quot;) # temporary rename to field, for the left join # unstable, fix! TODO colnames(lu)[2] = &quot;field&quot; # join the crop rotation and land use layer by their field ID lu_cr &lt;- left_join(lu, crop_rotation_extrapolated, by = &quot;field&quot; ) # reset column name. # unstable, fix! TODO colnames(lu_cr)[2] = &quot;lu&quot; # Write the new shape file write_sf(lu_cr, &quot;model_data/input/crop/land_with_cr.shp&quot;) "],["landuse.lum-update.html", "Section 5 Landuse.lum update 5.1 Data preperation 5.2 Assigning landuse values 5.3 Writing Changes", " Section 5 Landuse.lum update This section covers the modifications made to the landuse file. For some reason it was written in a tutorial fashion, as if the reader were new to R. require(tidyverse) require(reshape2) require(sf) require(DT) require(dplyr) # require dplyr last to overwrite plyr count() require(ggplot2) ggplot theme(s): sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) 5.1 Data preperation We read in the land use file. We want to set skip = 1 to ignore the SWAT+editor text, and set header = T. We then convert it to a tibble format for better printing to console landuse_lum &lt;- read.table(&quot;model_data/cs10_setup/swat_input/landuse.lum&quot;, skip = 1, header = T) %&gt;% tibble::as_tibble() (#tab:lu_lum_tab)The landuse.lum file, post BuildR name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp a_001f_1_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_2_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_3_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_4_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_5_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_6_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_001f_7_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_002f_1_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_002f_2_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null a_002f_3_drn_lum null nopl_comm null null null null null null mw24_1000 null null null null Our field_id for our cropland does not match name of landuse_lum so we need to parse it out, we can do this many ways, but a safe way is to split it by “_” and combine the first 3 splits with that same underscore: splitted &lt;- landuse_lum$name %&gt;% str_split(&quot;_&quot;) landuse_lum$field_id &lt;- paste(splitted %&gt;% map(1), splitted %&gt;% map(2), splitted %&gt;% map(3), sep = &quot;_&quot;) head(landuse_lum$field_id) ## [1] &quot;a_001f_1&quot; &quot;a_001f_2&quot; &quot;a_001f_3&quot; &quot;a_001f_4&quot; &quot;a_001f_5&quot; &quot;a_001f_6&quot; But wait, this does not work for our “non-fields”. So lets find out which ones they are, and set them to NA not_fields &lt;- which(!grepl(x=landuse_lum$field_id, &quot;a_&quot;)) landuse_lum$field_id[not_fields] &lt;- NA So, what non-fields do we have? landuse_lum$name[not_fields] ## [1] &quot;frst_lum&quot; &quot;past_lum&quot; &quot;rngb_lum&quot; &quot;urml_lum&quot; &quot;utrn_lum&quot; &quot;wetf_lum&quot; Good to know. We’ll keep that in mind. 5.2 Assigning landuse values 5.2.1 Setting cal_group There is no info on this column, so we are going to leave it as null. 5.2.2 Setting plnt_com This step will be done by SWATFarmR in Section 10. 5.2.3 Setting mgt This step will be done by SWATFarmR in Section 10. 5.2.4 Setting urb_ro We want to set the urb_ro column for all urban land uses (in our case this would be urml and utrn) to usgs_reg. We can do this like so: landuse_lum$urb_ro[which(landuse_lum$name %in% c(&quot;urml_lum&quot;, &quot;utrn_lum&quot;))] &lt;- &quot;usgs_reg&quot; (#tab:lu_urbro_tab)Changing urb_ro in the landuse file name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp field_id urml_lum null null null null null null usgs_reg null null null null null null NA utrn_lum null null null null null null usgs_reg null null null null null null NA Very good. In our case this was only two – could be done by hand, but that will not be the case for all of our land uses. 5.2.5 Setting urban This one is easy, we set our urban column to an urban parameter set of the same name. The rest we leave as null landuse_lum$urban[which(landuse_lum$name ==&quot;utrn_lum&quot;)] &lt;- &quot;utrn&quot; landuse_lum$urban[which(landuse_lum$name ==&quot;urml_lum&quot;)] &lt;- &quot;urml&quot; (#tab:lu_urbro_tab2)Changing urban in the landuse file name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp field_id urml_lum null null null null null urml usgs_reg null null null null null null NA Lets make sure other land uses still have null: (#tab:lu_lufrst)Landuse frst in the landuse file name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile sep vfs grww bmp field_id frst_lum null nopl_comm null null null null null null null null null null null NA Looks good. 5.2.6 Setting Manning’s n (ovn) Lets get the easy ones out of the way landuse_lum$ov_mann[which(landuse_lum$name ==&quot;past_lum&quot;)] &lt;- &quot;densegrass&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;rngb_lum&quot;)] &lt;- &quot;rangeland_20cover&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;urml_lum&quot;)] &lt;- &quot;urban_rubble&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;urtn_lum&quot;)] &lt;- &quot;urban_asphalt&quot; Don’t fall asleep yet! For wetf we want forest_heavy but with a higher value. this means we need to add a new entry. And since we are doing this fancy Rmarkdown stuff, we’re going to do it with code! Lets jump in and get at this file. (REMEBER TO REMOVE THE TEMP) ovn_table_path &lt;- &quot;model_data/cs10_setup/swat_input/ovn_table.lum&quot; ovn_table_path_new &lt;- &quot;model_data/cs10_setup/swat_input_mod/ovn_table.lum&quot; ovn_table &lt;- readLines(ovn_table_path) Good, now where is this forest heavy entry? and what does the format look like? index &lt;- grepl(x=ovn_table, &quot;forest_heavy&quot;) %&gt;% which %&gt;% min() ovn_table[c(2, index)] %&gt;% print() ## [1] &quot;name ovn_mean ovn_min ovn_max description&quot; ## [2] &quot;forest_heavy 0.80000 0.70000 0.90000 Forest_heavy&quot; Now, lets make our own and add it in. But only if it doesn’t exist it (Like if the script has been run before…) if(grepl(x = ovn_table, &quot;forest_heavy_cs10&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;forest_heavy_cs10 0.90000 0.80000 0.95000 Forest_heavy_mod&quot; ovn_table &lt;- c(ovn_table, entry) } Did it work? Yes: ovn_table %&gt;% tail() ## [1] &quot;forest_med 0.60000 0.50000 0.70000 Forest_medimum_good&quot; ## [2] &quot;forest_heavy 0.80000 0.70000 0.90000 Forest_heavy&quot; ## [3] &quot;urban_asphalt 0.01100 0.01100 0.01100 Urban_asphalt&quot; ## [4] &quot;urban_concrete 0.01200 0.01200 0.01200 Urban_concrete&quot; ## [5] &quot;urban_rubble 0.02400 0.02400 0.02400 Urban_rubble&quot; ## [6] &quot;forest_heavy_cs10 0.90000 0.80000 0.95000 Forest_heavy_mod&quot; Now lets write this new table writeLines(ovn_table, con = ovn_table_path_new) And now we can enter our wetland class: landuse_lum$ov_mann[which(landuse_lum$name ==&quot;wetf_lum&quot;)] &lt;- &quot;forest_heavy_cs10&quot; For the fields we are going to need the crop rotation info which we created in section 4. crop_rotation &lt;- read_csv(&quot;model_data/input/crop/cs10_crop_rotation.csv&quot;, show_col_types = F) head(crop_rotation) ## # A tibble: 6 × 5 ## field y_2016 y_2017 y_2018 y_2019 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a_025f_1 wwht wwht wwht wwht ## 2 a_025f_3 wwht wwht wwht wwht ## 3 a_025f_4 wwht wwht wwht wwht ## 4 a_025f_2 wwht wwht wwht wwht ## 5 a_105f_1 wwht wwht wwht wwht ## 6 a_105f_4 wwht wwht wwht wwht We have decided to classify ovn based on the degree to which a crop rotation contains meadow, conventional crops (wwht, pota), and conservation crops (all others). To do this we need to analyze the crop rotation. Now it gets a little complicated, but trust me its not actually as bad as it looks. We need to count how many times certain crops show up in the crop rotation of a certain field. Lets do it for conventional crops first. Now, lets get into it conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() %&gt;% dplyr::rename(conv = n) head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field conv ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 What happened here? well we took our crop_rotation, and melted it by field using melt. This function converts the data into tidy format (Wickham 2014). This format makes it easier to apply the following operations on a field basis. What does this look like? crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% arrange(field) %&gt;% head() ## field variable value ## 1 a_001f_1 y_2016 wwht ## 2 a_001f_1 y_2017 wwht ## 3 a_001f_1 y_2018 wwht ## 4 a_001f_1 y_2019 wwht ## 5 a_001f_2 y_2016 wwht ## 6 a_001f_2 y_2017 wwht melt is a function from reshape2 which is why we required it. And what does that “.” mean in there, to the left of \"field\"?? That is a code word for the object to the left of the “pipe” (in this case crop_rotation). the pipe is “%&gt;%” and passes the object to the left of it, into the function to the right of it. look it up! When arranged by field, we can see that every field gets one entry per crop per year. This is a good format to count how many times we have a certain type of crop on a field. Next we group_by the field – this means all the following operations will be done on a field basis. Then we filter our value (which is the crop name). For conventional we needed to filter in any fields with the crop \"wwht\" or \"pota\". What does this look like? crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) ## # A tibble: 1,519 × 3 ## # Groups: field [415] ## field variable value ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 a_025f_1 y_2016 wwht ## 2 a_025f_3 y_2016 wwht ## 3 a_025f_4 y_2016 wwht ## 4 a_025f_2 y_2016 wwht ## 5 a_105f_1 y_2016 wwht ## 6 a_105f_4 y_2016 wwht ## 7 a_105f_2 y_2016 wwht ## 8 a_105f_3 y_2016 wwht ## 9 a_187f_1 y_2016 wwht ## 10 a_206f_1 y_2016 wwht ## # ℹ 1,509 more rows Looks good. We only have crops with winter wheat and potatoes, for every year. Exactly what we need, now we just need to count() them. conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field n ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 And we are back where we started! See, not that complicated. One last thing we need to do is rename n to conv, we do that like so: conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() %&gt;% dplyr::rename(conv = n) head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field conv ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 Now lets go ahead and do the same thing for the two other categories: cons andmeadow. meadow_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value == &quot;meadow&quot; ) %&gt;% dplyr::count() %&gt;% dplyr::rename(meadow = n) cons_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(!(value %in% c(&quot;wwht&quot;, &quot;pota&quot;, &quot;meadow&quot;))) %&gt;% dplyr::count() %&gt;% dplyr::rename(cons = n) cons_crop_count %&gt;% head() ## # A tibble: 6 × 2 ## # Groups: field [6] ## field cons ## &lt;chr&gt; &lt;int&gt; ## 1 a_002f_1 2 ## 2 a_002f_2 2 ## 3 a_002f_3 4 ## 4 a_002f_4 4 ## 5 a_002f_5 2 ## 6 a_002f_6 4 meadow_crop_count %&gt;% head() ## # A tibble: 6 × 2 ## # Groups: field [6] ## field meadow ## &lt;chr&gt; &lt;int&gt; ## 1 a_012f_3 4 ## 2 a_012f_6 4 ## 3 a_016f_3 4 ## 4 a_017f_2 4 ## 5 a_017f_3 4 ## 6 a_046f_1 4 meadow was a simple filter, all we have to do was grab crops with the meadow name. For cons crops, we just grabbed the remaining crops that were not conv or meadow. Great. We have these 3 separate, we need to combine them. we can do that with left_join and join by the field column which contains our IDs # create a base dataframe to join to crop_fractions &lt;- crop_rotation %&gt;% dplyr::select(field) %&gt;% distinct() # join our 3 data frames crop_fractions &lt;- left_join(crop_fractions, conv_crop_count, by = &quot;field&quot;) crop_fractions &lt;- left_join(crop_fractions, cons_crop_count, by = &quot;field&quot;) crop_fractions &lt;- left_join(crop_fractions, meadow_crop_count, by = &quot;field&quot;) # lets have a look crop_fractions %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 a_025f_1 4 NA NA ## 2 a_025f_3 4 NA NA ## 3 a_025f_4 4 NA NA ## 4 a_025f_2 4 NA NA ## 5 a_105f_1 4 NA NA ## 6 a_105f_4 4 NA NA That does not look good! when it seems like when the count is 0, it is returned as NA. Lets fix that… crop_fractions &lt;- crop_fractions %&gt;% mutate(cons = ifelse(is.na(cons), 0, cons)) crop_fractions &lt;- crop_fractions %&gt;% mutate(conv = ifelse(is.na(conv), 0, conv)) crop_fractions &lt;- crop_fractions %&gt;% mutate(meadow = ifelse(is.na(meadow), 0, meadow)) What are we doing here? we are mutating the the 3 columns using an ifelse statement. The statement is simple. If the value is.na then we set it to 0. Otherwise, we set it to the same value it had before. did it work? crop_fractions %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a_025f_1 4 0 0 ## 2 a_025f_3 4 0 0 ## 3 a_025f_4 4 0 0 ## 4 a_025f_2 4 0 0 ## 5 a_105f_1 4 0 0 ## 6 a_105f_4 4 0 0 Very good. Now we need to decide what to classify our fields at. lets Define some functions to do that. is_meadow &lt;- function(conv, cons, meadow) { ((meadow &gt; cons) &amp; (meadow &gt; conv)) %&gt;% return() } is_cons &lt;- function(conv, cons, meadow) { ((cons &gt;= meadow) &amp; (cons &gt; conv)) %&gt;% return() } is_conv &lt;- function(conv, cons, meadow) { ((conv &gt;= meadow) &amp; (conv &gt;= cons)) %&gt;% return() } The &amp; sign means that both conditions need to be met. and &gt;= you should know already. Lets use those functions in action. lets do the meadow first. We will create a new dataframe from crop fractions, named field_class. field_class &lt;- crop_fractions %&gt;% mutate(meadow = is_meadow(conv,cons,meadow)) head(field_class) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 a_025f_1 4 0 FALSE ## 2 a_025f_3 4 0 FALSE ## 3 a_025f_4 4 0 FALSE ## 4 a_025f_2 4 0 FALSE ## 5 a_105f_1 4 0 FALSE ## 6 a_105f_4 4 0 FALSE Ok, so none of those first 6 fields are meadow dominated. What about cons? (lets stick with our field_class dataframe and just keep adding on) field_class &lt;- field_class %&gt;% mutate(cons = is_cons(conv,cons,meadow)) head(field_class) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 a_025f_1 4 FALSE FALSE ## 2 a_025f_3 4 FALSE FALSE ## 3 a_025f_4 4 FALSE FALSE ## 4 a_025f_2 4 FALSE FALSE ## 5 a_105f_1 4 FALSE FALSE ## 6 a_105f_4 4 FALSE FALSE Nope, not cons either. Then it must be conv dominant. field_class &lt;- field_class %&gt;% mutate(conv = is_conv(conv,cons,meadow)) field_class %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 a_025f_1 TRUE FALSE FALSE ## 2 a_025f_3 TRUE FALSE FALSE ## 3 a_025f_4 TRUE FALSE FALSE ## 4 a_025f_2 TRUE FALSE FALSE ## 5 a_105f_1 TRUE FALSE FALSE ## 6 a_105f_4 TRUE FALSE FALSE Correct! now lets just double check that we didnt have any cases where all are TRUE or where all are FALSE field_class %&gt;% dplyr::select(conv, cons, meadow) %&gt;% isTRUE %&gt;% all() ## [1] FALSE field_class %&gt;% dplyr::select(conv, cons, meadow) %&gt;% isFALSE() %&gt;% all() ## [1] FALSE Looks good to me! carry on: We have our field classifications now, they will be very useful to us later. Lets pull them out of our dataframe to store them as a nice list. cons_fields &lt;- field_class %&gt;% filter(cons) %&gt;% dplyr::select(field) %&gt;% pull() conv_fields &lt;- field_class %&gt;% filter(conv) %&gt;% dplyr::select(field) %&gt;% pull() meadow_fields &lt;- field_class %&gt;% filter(meadow) %&gt;% dplyr::select(field) %&gt;% pull() cons_fields %&gt;% head() ## [1] &quot;a_182f_1&quot; &quot;a_182f_5&quot; &quot;a_182f_3&quot; &quot;a_182f_4&quot; &quot;a_182f_2&quot; &quot;a_182f_7&quot; conv_fields %&gt;% head() ## [1] &quot;a_025f_1&quot; &quot;a_025f_3&quot; &quot;a_025f_4&quot; &quot;a_025f_2&quot; &quot;a_105f_1&quot; &quot;a_105f_4&quot; meadow_fields %&gt;% head() ## [1] &quot;a_182f_6&quot; &quot;a_182f_8&quot; &quot;a_017f_3&quot; &quot;a_017f_2&quot; &quot;a_012f_3&quot; &quot;a_012f_6&quot; Fantastic! Now that was a big task, but it makes the next bit very easy. Lets assign the correct ovn to the types of fields we have classified: 5.2.6.1 Agricultural Fields: Meadow landuse_lum$ov_mann[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;shortgrass&quot; 5.2.6.2 Agricultural Fields: Conventional landuse_lum$ov_mann[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;convtill_nores&quot; 5.2.6.3 Agricultural Fields: Conservational landuse_lum$ov_mann[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;falldisk_res&quot; 5.2.6.4 Forest Initially we wanted certain classes for the forest land use based on how good the soil quality was. But to do this, we would need multiple land uses, which we do not have. We will just use forest_medium for all frst. We can only change this if we go back to buildR and define more generic forest classes. landuse_lum$ov_mann[which(landuse_lum$name ==&quot;frst_lum&quot;)] &lt;- &quot;forest_med&quot; 5.2.6.5 Summary With all that data processing out of the way, lets have a quick look at what we’ve done: landuse_lum$ov_mann[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;densegrass&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;rangeland_20cover&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urban_rubble&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;urtn_lum&quot;)] &lt;- &quot;urban_asphalt&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;forest_med&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;shortgrass&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;convtill_nores&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;falldisk_res&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;forest_heavy_cs10&quot; Sweet. Next column.. 5.2.7 Setting cn2: You know the drill – I am sure you know how to read this code by now. # Brush-brush-weed-grass_mixture_with_brush_the_major_element (poor) landuse_lum$cn2[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;brush_p&quot; # Woods (poor) landuse_lum$cn2[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;wood_p&quot; # Paved_streets_and_roads;_open_ditches_(incl._right-of-way) landuse_lum$cn2[which(landuse_lum$name == &quot;utrn_lum&quot;)] &lt;- &quot;paveroad&quot; # Paved_parking_lots_roofs_driveways_etc_(excl_right-of-way) landuse_lum$cn2[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urban&quot; # Woods (fair) landuse_lum$cn2[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;wood_f&quot; # Pasture_grassland_or_range-continuous_forage_for_grazing landuse_lum$cn2[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;pastg_g&quot; # Meadow-continuous_grass_protected_from_grazing_mowed_for_hay landuse_lum$cn2[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;pasth&quot; # Row_crops landuse_lum$cn2[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;rc_conterres_g&quot; # Row_crops landuse_lum$cn2[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;rc_strow_p&quot; 5.2.8 Setting cons_prac For this, we need to add some custom entries to the database: cons_prac_path &lt;- &quot;model_data/cs10_setup/swat_input/cons_practice.lum&quot; cons_prac_path_new &lt;- &quot;model_data/cs10_setup/swat_input_mod/cons_practice.lum&quot; cons_prac &lt;- readLines(cons_prac_path) cons_prac[1:3] %&gt;% print() ## [1] &quot;cons_practice.lum: written by SWAT+ editor v2.1.0 on 2023-05-30 10:39 for SWAT+ rev.60.5.4&quot; ## [2] &quot;name usle_p slp_len_max description&quot; ## [3] &quot;up_down_slope 1.00000 121.00000 Up_and_down_slope&quot; Let us do it in a way so that it is only added if it doesn’t exist yet: if(grepl(x = cons_prac, &quot;agri_conv&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_conv 1.00000 60.00000 no_convervation&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_part_conv&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_part_conv 0.85000 60.00000 75_percent_convential&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_half&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_half 0.70000 50.00000 50_percent_convential&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_part_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_part_cons 0.50000 30.00000 75_percent_consveration&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_cons 0.30000 30.00000 100_percent_consveration&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;past_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;past_cons 0.1 60 pasture&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;rngb_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;rngb_cons 0.2 60 rangeland&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;frst_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;frst_cons 0.1 60 forest&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;urml_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;urml_cons 1 60 cs10urban&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;utrn_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;utrn_cons 1 60 road&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_meadow&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_meadow 0.2 60 cs10_meadow&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;wetf_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;wetf_cons 0.05 30 wetlands&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_sed_pond&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_sed_pond 0.1 30 cs10_sed_pond&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_cons_wetl&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_cons_wetl 0.1 30 cs10_cons_wetl&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_buff_grass&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_buff_grass 0.25 10 cs10_buff_grass&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs_10_buff_wood&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs_10_buff_wood 0.2 10 cs_10_buff_wood&quot; cons_prac &lt;- c(cons_prac, entry) } And write the updated file: writeLines(cons_prac, con = cons_prac_path_new) Now we need to define the 0, 25, 50, 75, 100 percent conventional crops. We can use our old “crop_fractions” dataframe: head(crop_fractions) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a_025f_1 4 0 0 ## 2 a_025f_3 4 0 0 ## 3 a_025f_4 4 0 0 ## 4 a_025f_2 4 0 0 ## 5 a_105f_1 4 0 0 ## 6 a_105f_4 4 0 0 Lets derive which fields get classified as what p100_conv &lt;- crop_fractions %&gt;% filter(conv==4) %&gt;% dplyr::select(field) %&gt;% pull() p75_conv &lt;- crop_fractions %&gt;% filter(conv==3) %&gt;% dplyr::select(field) %&gt;% pull() p50_conv &lt;- crop_fractions %&gt;% filter(conv==2) %&gt;% dplyr::select(field) %&gt;% pull() p25_conv &lt;- crop_fractions %&gt;% filter(conv==1) %&gt;% dplyr::select(field) %&gt;% pull() p0_conv &lt;- crop_fractions %&gt;% filter(conv==0) %&gt;% dplyr::select(field) %&gt;% pull() Now we can assign the all the land uses # Agricultural landuse_lum$cons_prac[which(landuse_lum$field_id %in% p100_conv)] &lt;- &quot;agri_conv&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p75_conv)] &lt;- &quot;agri_part_conv&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p50_conv)] &lt;- &quot;agri_half&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p25_conv)] &lt;- &quot;agri_part_cons&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p0_conv)] &lt;- &quot;agri_cons&quot; # Meadow landuse_lum$cons_prac[which(field_class$meadow)] &lt;- &quot;cs10_meadow&quot; # Generic landuse_lum$cons_prac[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;past_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;rngb_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;frst_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urml_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;utrn_lum&quot;)] &lt;- &quot;utrn_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;wetf_cons&quot; # Measures landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_sed_pond&quot;)] &lt;- &quot;cs10_sed_pond&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_cons_wetl&quot;)] &lt;- &quot;cs10_cons_wetl&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_buff_grass&quot;)] &lt;- &quot;cs10_buff_grass&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs_10_buff_wood&quot;)] &lt;- &quot;cs_10_buff_wood&quot; 5.2.9 Setting tile This column has already been completed by SWATbuildR in 1.4 5.2.10 Setting sep This column has been left as null 5.2.11 Setting vfs This column has been left as null 5.2.12 Setting grww This column has been left as null 5.2.13 Setting bmp This column has been left as null 5.3 Writing Changes We are done with our modifications however, we need to remove the field_id column landuse_lum &lt;- landuse_lum %&gt;% dplyr::select(-field_id) lets take a look at the final product: datatable(landuse_lum) Lets write out changes in a new directory where we will store any SWAT+ input files which we have modified from their source BuildR form. dir.create(&quot;model_data/cs10_setup/swat_input_mod&quot;, showWarnings = F) new_lum_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot; write.table(landuse_lum, file = new_lum_path, sep = &quot;\\t&quot;, quote = F, row.names = F) But wait – we need to keep that pesky header, otherwise the FarmR will be very angry with us. lum_lines &lt;- readLines(new_lum_path) header &lt;- &quot;header header HEADER, delete me and you will regret it FOREVER!&quot; lum_lines2&lt;- c(header,lum_lines) writeLines(text = lum_lines2, con = new_lum_path) Done! Now that We’ve changed another input file, we should test if our model still runs. We will do that with our custom test_swat_mod() function. This function is basically the same as the previous test_swat() function, only that it incorporates the modified SWAT input files source(&quot;model_data/code/test_swat_mod.R&quot;) simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) References "],["field-site-loggers.html", "Section 6 Field Site Loggers 6.1 Introduction 6.2 Data Processing 6.3 Data quality check 6.4 Data Cleanup", " Section 6 Field Site Loggers 6.1 Introduction This chapter is concerned with preparing the measured data for the OPTAIN field scale models. This will involve loading the data, transforming it into a usable format, and performing a quality check. The end of this chapter will result in data fit for use in SWAP as well as a tangentially related antecedent precipitation index (API) model for use in the SWAT+ modelling work, related to scheduling management operations using SWATFarmR . 6.1.1 Prerequisites The following packages are required for this chapter: require(readr) # for reading in data require(dplyr) # for manipulating data require(stringr) # for manipulating text require(purrr) # for extracting from nested lists require(plotly) # for diagnostic plotting require(lubridate) # for date conversions require(reshape2) # for data conversions require(wesanderson) # plot colors require(DT) # for tables require(mapview) # for plotting require(sf) # for shapefiles show_rows &lt;- 500 # number of rows to show in the tables As well as the following custom functions: source(&quot;model_data/code/plot_logger.R&quot;) 6.1.2 Locations The loggers are described as such: Logger3(600409): deep rocky sand (forest?) Logger4(600410): Tormods farm (agri) Logger2(600411): John Ivar lower area (agri) Logger1(600413): John Ivar behind the barn (agri) And are located here: field_sites_shp &lt;- read_sf(&quot;model_data/field_sites/geospatial/cs10_field_sites.shp&quot;) basin_shp &lt;- read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;) basin_map &lt;- mapview(basin_shp, alpha.regions = .1, legend = FALSE) field_map &lt;- mapview(field_sites_shp, legend = FALSE) basin_map+field_map 6.2 Data Processing 6.2.1 Loading Data To start we need to load in our data from the GroPoint Profile data loggers. The logger is approx. 1m long and has 7 temperature sensors, and 6 soil moisture sensors located along its segments. “GroPoint Profile Multi Segment Soil Moisture &amp; Temperature Profiling Probe Operation Manual” (2023) (#fig:logger_gropointfig)GroPoint Segment Schematic (GroPoint Profile User Manual, Page 6) (#fig:logger_gropointfig2)GroPoint Temperature sensor depths. We use GPLP-4 with seven temperature sensors. (GroPoint Profile User Manual, Page 6) The logger data has been modified before import into R. The file names have been changed and column headers have been added. Missing values stored as “Error” have been removed. Columns headers have assigned with variable name, depth, and unit of depth separated by an underscore. path &lt;- &quot;model_data/field_sites/loggers/data/&quot; Loading the data and modifying it is performed with readr and dplyr. # list of files datasheets &lt;- list.files(path, full.names = TRUE) # loading in the data, with an ID column &#39;source&#39; data &lt;- read_csv(datasheets, show_col_types = F, id = &quot;source&quot;) # translating the source text to logger ID data$site &lt;- data$source %&gt;% str_remove(path) %&gt;% str_remove(&quot;.csv&quot;) %&gt;% str_split(&quot;_&quot;) %&gt;% map(1) %&gt;% unlist() # removing the source column data &lt;- data %&gt;% select(-source) # removing duplicates data &lt;- data %&gt;% distinct() Data preview We need to force the date time column into the correct format. We can do this with the following command. (Please note, this has been done for region: United States this code may need to be adjusted to fit your regional settings) data$datetime &lt;- data$datetime %&gt;% as_datetime(format = &quot;%m/%d/%Y %H:%M&quot;, tz = &quot;CET&quot;) 6.2.2 Data Re-structuring For ease of use in R, we will re-structure the data to be in “tidy” format (Read more: (Wickham 2014)). # grab the measured variable column headers mea_var &lt;- colnames(data)[3:15] # melt (tidy) the data by datetime and site. data_melt &lt;- data %&gt;% melt(id.vars = c(&quot;datetime&quot;, &quot;site&quot;), measure.vars = mea_var) # parse out the variable data_melt$var &lt;- data_melt$variable %&gt;% str_split(&quot;_&quot;) %&gt;% map(1) %&gt;% unlist() # parse out the depth of measurement data_melt$depth &lt;- data_melt$variable %&gt;% str_split(&quot;_&quot;) %&gt;% map(2) %&gt;% unlist() # remove duplicates data_melt &lt;- data_melt %&gt;% distinct() Data preview: 6.2.3 Summarize to Daily Means As SWAP is a daily time step model, we do not need the hourly resolution, and can therefore simplify our analysis. We first need to parse out a (daily) date column data_melt$date &lt;- data_melt$datetime %&gt;% as.Date() And then group the data by this date, followed by an averaging function. daily_data &lt;- data_melt %&gt;% group_by(date, var, depth, site) %&gt;% summarise(value = round(mean(value, na.rm = TRUE), 1), .groups = &quot;drop_last&quot;) %&gt;% ungroup() Additional notes: The values are rounded to one decimal place, as this is is the precision limit of the Logger. The averaging is performed per date, variable, depth, and site The functionality of .groups = \"drop_last\" is unknown to me, however it does not change anything other than quieting a warning message. The data is “ungrouped” at the end, to avoid problems with plotting the data (with plotly) later on. Data preview: 6.2.4 Including Air Temperature and Precipitation Data We are going to add the temp / precipitation data from out SWAT+ setup to give us insight on temperature and moisture dynamics for the data quality check. Also important to note, is that all this code is only necessary, because of the strange format SWAT+ uses for its weather input. If it would just use a standard format (like everything else) then all this would not be needed. Reading in the SWAT source precipitation data: swat_pcp &lt;- read.table(&quot;model_data/cs10_setup/optain-cs10/sta_id1.pcp&quot;, skip = 3, header = F, sep = &quot;\\t&quot;) %&gt;% as_tibble() swat_tmp &lt;- read.table(&quot;model_data/cs10_setup/optain-cs10/sta_id1.tmp&quot;, skip = 3, header = F, sep = &quot;\\t&quot;) %&gt;% as_tibble() Data preview: Parsing out the first and last day and year. FIRST_DOY &lt;- swat_pcp[1,][[2]]-1 FIRST_YEAR &lt;- swat_pcp[1,][[1]] LAST_DOY &lt;- swat_pcp[length(swat_pcp$V1),][[2]]-1 LAST_YEAR &lt;- swat_pcp[length(swat_pcp$V1),][[1]] Converting these to an R format. Note that R uses a 0-based index only for dates. (very confusing!). first_day &lt;- as.Date(FIRST_DOY, origin = paste0(FIRST_YEAR, &quot;-01-01&quot;)) last_day &lt;- as.Date(LAST_DOY, origin = paste0(LAST_YEAR, &quot;-01-01&quot;)) date_range &lt;- seq(from = first_day, to = last_day, by = &quot;day&quot;) Creating a dataframe from the source data pcp_data &lt;- tibble(date = date_range, pcp = swat_pcp$V3) And filtering it to only cover the same range as our logger data. pcp_data &lt;- pcp_data %&gt;% filter(date %in% daily_data$date) And we do the same for temperature FIRST_DOY &lt;- swat_tmp[1,][[2]]-1 FIRST_YEAR &lt;- swat_tmp[1,][[1]] LAST_DOY &lt;- swat_tmp[length(swat_tmp$V1),][[2]]-1 LAST_YEAR &lt;- swat_tmp[length(swat_tmp$V1),][[1]] first_day &lt;- as.Date(FIRST_DOY, origin = paste0(FIRST_YEAR, &quot;-01-01&quot;)) last_day &lt;- as.Date(LAST_DOY, origin = paste0(LAST_YEAR, &quot;-01-01&quot;)) date_range &lt;- seq(from = first_day, to = last_day, by = &quot;day&quot;) tmp_data &lt;- tibble(date = date_range, tmp_max = swat_tmp$V3, tmp_min = swat_tmp$V4) tmp_data &lt;- tmp_data %&gt;% filter(date %in% daily_data$date) (#fig:logger_plotclimbase)Temperature range and precipitation for CS10 Note: this does (yet) include data from 2023, even though the loggers do. 6.3 Data quality check In this section we are proofing the quality of the data. 6.3.1 Logger 600409 “Deep Rocky Sand” plot_logger(daily_data, &quot;600409&quot;, pcp_data, tmp_data) (#fig:logger_plot09)Logger 600409 6.3.1.1 Soil Temperature Remove first few days with unrealistic data 6.3.1.2 Soil Moisture Content First few days need to be removed 150mm depth seems usable before Sept2022 – However Attila (instrument installer) mentioned this sensor might be in the air (which would explain low moisture levels), so potentially useless 350mm depth seems usable, breaks around/after Sept2022 450mm depth seems useless, constantly getting wetter for an entire year, spiking post Sept2022 600mm depth seems useless, constantly getting wetter for an entire year, spiking post Sept2022 750mm depth seems useless, constantly getting wetter for an entire year. 900mm depth seems useless, constantly getting wetter for an entire year. 6.3.2 Logger 600410 “Tormods farm” plot_logger(daily_data, &quot;600410&quot;, pcp_data, tmp_data) 6.3.2.1 Soil Temperature Remove first few days with unrealistic data 6.3.2.2 Soil Moisture Content First few days need to be removed Depth 150 probably useless, as sensor is in free air. (According to Attila) Logger seems to have broken Sometime after Sept1 2022, for all depth levels. Data pre-Sept22 seems usable (Csilla ?) 6.3.3 Logger 600411 “John Ivar Lower Area” plot_logger(daily_data, &quot;600411&quot;, pcp_data, tmp_data) 6.3.3.1 Soil Temperature Need to remove the first few days Need to remove the malfunction in April-May 2022 6.3.3.2 Soil Moisture Content First few days need to be removed Need to remove the malfunction in April-May 2022 Depths 150 is out of soil, must be removed. Logger seems to have broken August 16th, 2022, data seems useful before that date (Csilla, agree?) 6.3.4 Logger 600413 “John Ivar behind the barn” plot_logger(daily_data, &quot;600413&quot;, pcp_data, tmp_data) 6.3.4.1 Soil Temperature Remove the first few days with unrealistic data 6.3.4.2 Soil Moisture Content Remove first few days Remove 150mm, seems out of earth Hard to tell when sensor broke, earliest June 24th, latest Sept.12 2022, what do you think Csilla ? 6.4 Data Cleanup 6.4.1 General cleanup Removing the first few days of the simulation start_date &lt;- &quot;2021-11-01&quot; data_clean &lt;- daily_data %&gt;% filter(date &gt; start_date) To be on the safe side, I am setting the logger cut-off date to XXXXXXXXX This could be tuned “per logger” later on if one feels confident. cut_off_date &lt;- &quot;2022-08-14&quot; data_clean &lt;- data_clean %&gt;% filter(date &lt; cut_off_date) And we will do the same for the climate data # TODO dont actually do this. pcp_data_clean &lt;- pcp_data %&gt;% filter(date &gt; start_date) pcp_data_clean &lt;- pcp_data_clean %&gt;% filter(date &lt; cut_off_date) tmp_data_clean &lt;- tmp_data %&gt;% filter(date &gt; start_date) tmp_data_clean &lt;- tmp_data_clean %&gt;% filter(date &lt; cut_off_date) We will remove the 150 mm depth SMC measurement, as it was not submerged properly in the soil, and is therefore of little use. The temperature reading might be useful, so we will keep it. data_clean &lt;- data_clean %&gt;% filter(depth != &quot;150&quot;) 6.4.2 Logger specific cleanup 6.4.2.1 Logger 600409 No logger specific cleanup required 6.4.2.2 Logger 600410 No logger specific cleanup required 6.4.2.3 Logger 600411 Logger 600411 needs temperature and SMC data removed from 2022-04-21 to 2022-05-10. It seems like the logger was removed from the soil. # define time range to remove remove_dates &lt;- seq(from = as.Date(&quot;2022-04-21&quot;), to = as.Date(&quot;2022-05-10&quot;), by = &quot;day&quot;) # set values in that site and date range to NA data_clean &lt;- data_clean %&gt;% mutate(value = case_when((site == &quot;600411&quot; &amp; date %in% remove_dates) ~ NA, .default = value)) 6.4.2.4 Logger 600413 No logger specific cleanup required 6.4.3 Saving the cleaned data We can now save our cleaned up data. We will do so in CSV form write_csv(data_clean, file = &quot;model_data/field_sites/loggers/clean/logger_data_daily_clean.csv&quot;) write_csv(pcp_data_clean, file = &quot;model_data/field_sites/loggers/clean/pcp_data_clean.csv&quot;) References "],["field-site-properties.html", "Section 7 Field Site Properties 7.1 Determining the HSG group of the logger sites", " Section 7 Field Site Properties For various use-cases across the modelling project, we will require certain information about the locations of the field sites. This section gathers that information and makes it available to other sections. 7.1 Determining the HSG group of the logger sites require(sf) require(readr) require(DT) require(dplyr) require(mapview) require(ggplot2) sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) We load the soil map of the area. soil_map &lt;- &quot;model_data/input/soil/soil_map_UTM32N.shp&quot; soil_map_shp &lt;- read_sf(soil_map) We will crop it by the watershed. Therefore we load the water shed cs10_basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; cs10_basin &lt;- read_sf(cs10_basin_path) And crop by by basin. (current not sure of the effect of constant geometry) st_agr(soil_map_shp) = &quot;constant&quot; st_agr(cs10_basin) = &quot;constant&quot; soil_map_shp &lt;- st_intersection(soil_map_shp, cs10_basin) ggplot()+geom_sf(soil_map_shp, mapping = aes(fill = SNAM))+sf_theme (#fig:fs_soilmapplot)Soil map of CS10, in shapefile format We need information on the soil itself. This is found in our user table. We only need the hydrologic soil group (HSG) HYDGRP, but we will keep the soil depth SOL_ZMX and soil texture TEXTURE just in case usersoil_path &lt;- &quot;model_data/input/soil/UserSoil_Krakstad.csv&quot; usersoil &lt;- read_csv(usersoil_path, show_col_types = F) usersoil &lt;- usersoil %&gt;% dplyr::select(OBJECTID, SNAM, SOL_ZMX, HYDGRP, TEXTURE) We can combine the two data sets with a left join by soil ID / Object ID usersoil$SOIL_ID &lt;- usersoil$OBJECTID usersoil &lt;- usersoil %&gt;% dplyr::select(-OBJECTID) soil_propery_map &lt;- dplyr::left_join(soil_map_shp, usersoil, by = &quot;SOIL_ID&quot;) ggplot()+geom_sf(soil_propery_map, mapping = aes(fill = HYDGRP))+sf_theme (#fig:fs_hsgmap)HSG of CS10 Now we need the locations of the field sites. These are stored as points in the following file: cs10_field_sites_path &lt;- &quot;model_data/field_sites/geospatial/cs10_field_sites.shp&quot; cs10_field_sites &lt;- read_sf(cs10_field_sites_path) We are going to join the attributes using a spatial join field_sites_attr &lt;- st_join(cs10_field_sites, soil_propery_map) Here is the result: We have two sites covering C and D, and one site covering B. This covers all existing HSGs in the catchment, which is good. We can save this information in a dataframe. field_site_attr_df &lt;- st_drop_geometry(field_sites_attr) datatable(field_site_attr_df) We will write this into our output folder write_csv(x = field_site_attr_df, file = &quot;model_data/field_sites/output/field_site_attr_df.csv&quot;) And save our soil property map, and field site points (with attributes) # TODO fix the weird ID column mess you made write_sf(field_sites_attr, &quot;model_data/field_sites/output/field_site_attr_map.shp&quot;) ## Warning in CPL_write_ogr(obj, dsn, layer, driver, ## as.character(dataset_options), : GDAL Message 6: Normalized/laundered field ## name: &#39;Id&#39; to &#39;Id_1&#39; write_sf(soil_propery_map, &quot;model_data/field_sites/output/soil_propery_map.shp&quot;) "],["antecedent-precipitation-index.html", "Section 8 Antecedent Precipitation Index 8.1 Introduction 8.2 Antecedent Precipitation Index 8.3 Conclusions", " Section 8 Antecedent Precipitation Index 8.1 Introduction SWATFarmR cannot account for operations based on values modeled by SWAT+, such as soil moisture. This is because FarmR does not run the model itself, and creates and pre-defines all the management schedules based on measured climate data. This means we need to relate our measured climate data to our soil moisture (Because soil moisture is an important factor in planning agricultural operations). One method to do this is called the Antecedent Precipitation Index. Which is what we will creating for CS10 in this part. As a heads up, this section requires Python! 8.1.1 Pre-requirements Required R packages require(readr) require(dplyr) require(hydroGOF) library(reticulate) # Required Python packages: &quot;pandas&quot;, &quot;numpy&quot;, &quot;scipy&quot; source(&quot;model_data/code/plot_api.R&quot;) source(&quot;model_data/code/calc_api.R&quot;) 8.1.2 Setting up Python for R/Rmarkdown A quick (non-evaluated) code block to show how a python environment can be created for use in Rmarkdown/Reticulate on your local machine. # install miniconda install_miniconda() # create conda environment for your project conda_create(&quot;cs10_env&quot;, # environment name packages = c(&quot;pandas&quot;, &quot;numpy&quot;, &quot;scipy&quot;), # required packages conda = &quot;auto&quot;) conda_python(&quot;cs10_env&quot;) # returns the .exe python path for your environment # you need to change your R environment path in this file usethis::edit_r_environ() # RETICULATE_PYTHON=&quot;YOUR ENVIRONMENT PATH/python.exe&quot; # restart your R session. py_config() # If everything is right, you should get something like: # python: YOUR ENVIRONMENT PATH/python.exe libpython: YOUR ENVIRONMENT # PATH/python39.dll pythonhome: YOUR ENVIRONMENT PATH version: 3.9.7 (default, # Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] Architecture: 64bit numpy: # YOUR ENVIRONMENT PATH\\lib\\site-packages\\numpy numpy_version: 1.21.2 # After this it should work :) 8.2 Antecedent Precipitation Index From (Patrignani 2020) The API is a well-known, parsimonious, recursive model for predicting soil moisture solely based on precipitation records. The API is commonly implemented using daily precipitation records, but it is possible to work at finer temporal scales (e.g. hourly) if both precipitation (model input) and soil moisture (for validation purposes) are available. The equation describing the simple version of the model is: \\[API_{t} = \\alpha API_{t-1} + P_t\\] \\(API_t\\): Soil water content at time \\(t\\) (today) \\(API_{t-1}\\): Soil water content at time \\(t-1\\) (yesterday) \\(\\alpha\\): Loss coefficient. Range between 0 and 1 \\(P_t\\): Precipitation at time \\(t\\) (today) 8.2.1 Loading data We will load out data from the previous steps. (ref) logger_data &lt;- read_csv(&quot;model_data/field_sites/loggers/clean/logger_data_daily_clean.csv&quot;, show_col_types=F) # we only need SMC for this part logger_data = logger_data %&gt;% filter(var == &quot;smc&quot;) site_attributes &lt;- read_csv(&quot;model_data/field_sites/output/field_site_attr_df.csv&quot;, show_col_types = F) pcp_data &lt;- read_csv(&quot;model_data/field_sites/loggers/clean/pcp_data_clean.csv&quot;, show_col_types = F) head(site_attributes) ## # A tibble: 5 × 8 ## id SOIL_ID SNAM.x Id SNAM.y SOL_ZMX HYDGRP TEXTURE ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 600413 23 ST 0 Sea_dep_thick 800 C sandy loam ## 2 600411 1 AB 0 ARdy 1000 C loamy_sand ## 3 600410 20 RGah 0 GLum-ar 1000 B sandy_loam ## 4 600409 13 Humic 0 GLch-hu-sl 1000 D silty_clay_loam ## 5 600408 13 Humic 0 GLch-hu-sl 1000 D silty_clay_loam We will calculate the model per Hydrologic soil group (HSG), and start with B, which was measured by logger 600410. b_data &lt;- logger_data %&gt;% filter(site == &quot;600410&quot;) 8.2.2 Calculating the total water content We need to calculate the total water content of the profile from the depth measurements from the following depths: unique(b_data$depth) ## [1] &quot;300&quot; &quot;450&quot; &quot;600&quot; &quot;750&quot; &quot;900&quot; Following code is modified from Patrignani (2020) (read more) (This calculation could be improved by considering the soil horizons) a &lt;- b_data %&gt;% filter(depth == &quot;300&quot;) %&gt;% select(value) b &lt;- b_data %&gt;% filter(depth == &quot;450&quot;) %&gt;% select(value) c &lt;- b_data %&gt;% filter(depth == &quot;600&quot;) %&gt;% select(value) d &lt;- b_data %&gt;% filter(depth == &quot;750&quot;) %&gt;% select(value) e &lt;- b_data %&gt;% filter(depth == &quot;900&quot;) %&gt;% select(value) total_water_content &lt;- a * .35 + # from 0 to 35 b * .15 + # from 35 to 50 c * .15 + # from 55 to 65 d * .15 + # from 65 to 80 e * .20 # from 80 to 100 b_col &lt;- tibble(b_data %&gt;% filter(depth == &quot;300&quot;) %&gt;% select(date), total_water_content) And we can add the precipitation data from 6.2.4 b_col &lt;- left_join(b_col, pcp_data, by = &quot;date&quot;) plot_api(obs = b_col, title = &quot;HSG B&quot;) (#fig:hsg_b_twc_plot)Total water content for logger 600410 (HSG B) Saving the data write_csv(b_col, &quot;model_data/field_sites/output/twc.csv&quot;) 8.2.3 Defining API model The value is capped at the maximum value of the source data (50). This could be improved by capping site specific. def api_model(P,alpha=0.97,ini=36.8): api = [ini] for t in range(1,len(P)): append_val = api[t-1]*alpha + P[t] if append_val &gt; 50: append_val = 50 api.append(append_val) return api Load in our data: import pandas as pd df = pd.read_csv(&#39;model_data/field_sites/output/twc.csv&#39;) df = df[[&#39;date&#39;,&#39;value&#39;,&#39;pcp&#39;]] df.head() ## date value pcp ## 0 2021-11-02 36.880 0.3 ## 1 2021-11-03 36.930 1.1 ## 2 2021-11-04 36.995 0.0 ## 3 2021-11-05 37.025 0.0 ## 4 2021-11-06 37.030 0.7 As a test, we will calculate the predictions with an alpha of 0.95: import pandas as pd from scipy.optimize import curve_fit # guess storage_guessed = api_model(df[&#39;pcp&#39;], alpha=0.95, ini=36.8) # write to text dates = df[[&#39;date&#39;]] guess = pd.DataFrame(dates.values.tolist(),storage_guessed) guess.to_csv(&quot;model_data/field_sites/output/first_guess.csv&quot;) Now we can have a look at it in R guess &lt;- read_csv(&quot;model_data/field_sites/output/first_guess.csv&quot;, show_col_types = F, skip = 1, col_names = c(&quot;val&quot;, &quot;date&quot;)) plot_api(api_vals = guess, obs = b_col, title = &quot;HSG-B API guessed&quot;) Now we optimize the alpha parameter using scipy curve_fit. par_opt, par_cov = curve_fit(api_model, df[&#39;pcp&#39;], df[&#39;value&#39;], p0=[0.95,100]) print(&#39;Annual mean alpha value is&#39;,round(par_opt[0],2)) ## Annual mean alpha value is 0.98 Now we can calculate the API using the optimized parameter. import numpy as np storage_optimized = api_model(df[&#39;pcp&#39;],par_opt[0],par_opt[1]) # Mean Absolute Error MAE = np.mean(np.abs(df[&#39;value&#39;] - storage_optimized)) print(&#39;Mean annual error =&#39;, round(MAE,1),&#39;cm&#39;) ## Mean annual error = 9.1 cm And save the data for R dates = df[[&#39;date&#39;]] api_optimized = pd.DataFrame(dates.values.tolist(),storage_optimized) api_optimized.to_csv(&quot;model_data/field_sites/output/api_optimized.csv&quot;) And have a look in R: optimized &lt;- read_csv(&quot;model_data/field_sites/output/api_optimized.csv&quot;, show_col_types = F, skip = 1, col_names = c(&quot;val&quot;, &quot;date&quot;)) plot_api(api_vals = optimized, obs = b_col, title = &quot;HSG-B API optimized&quot;) Not great, but also not surprising considering no account for snow melt has been done, nor do we have good logger data. We will define this workflow in a function calc_api.R and run it for the next loggers. calc_api(logger_id = &quot;600409&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set ## HSG Alpha ## &quot;D&quot; &quot;1&quot; This is the logger with broken data, so no surprise that the optimizer failed. calc_api(logger_id = &quot;600411&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set ## HSG Alpha ## &quot;C&quot; &quot;0.99&quot; calc_api(logger_id = &quot;600413&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set ## HSG Alpha ## &quot;C&quot; &quot;0.99&quot; 8.3 Conclusions For HSG B we got an alpha of 0.98, for C we have two samples, both of which got 0.99, and D has two samples as well, but we currently only have data for one. For the one D site we had, the logger data was very poor and the covariance could not be estimated. Overall, the API does not seem to be a great estimator of soil moisture for our specific case. Our logger data is poor and has been cut short due to sensor malfunctions. With a longer time series covering more seasons, it is likely that the curve fitting could be improved upon, and the overall dynamics could be appraised.; Secondly, and more critically: the API does not account for snow melt, which plays a significant role in a Boreal catchments such as CS10. You can see this in action (I think) in March where the logger data steadily goes up, while the API steadily goes down. Snow melt could surely be accounted for by adding another model to estimate it – the question is more if it is worth it, considering the poor underlying data of the loggers. Another complication is, if SWATFarmR does not account for snow melt in its management scheduling, then there is no point in estimating it. And if snow melt is not incorporated into the scheduling, then the operation timing will not be very accurate anyway (since snow melt is such a dominating factor in operation timing). UPDATE post Q+A Session We can modify the output of Micha’s script in Section 10.1 by replacing API with our own custom variable. This variable could be snow melt or something else entirely (like a trafficability thing?) This is pretty straight forward, we just need to add the index to the FarmR using add_variable(). One thing we need to keep in mind is that this needs to be compatible for the climate scenarios (i.e. we can only base it off climate variables which we also have future data on.) Some things to consider: Run SWAT+ for the weather data + scenarios data and use the calculated snow melt Re-create the snow melt model of SWAT+ and use that Use HBV or Persist models Use some tractability index for Norway (Attila?) References "],["model-parameterization.html", "Section 9 Model Parameterization 9.1 Parameter Changes 9.2 Fixing Tiledrain", " Section 9 Model Parameterization These sections are not detailed enough to warrant their own dedicated chapter. 9.1 Parameter Changes These sections mostly involve changing a few parameter values and are therefore in small sub sections without R-code. Some of this might change later. 9.1.1 Channel parameters revised This an ongoing unresolved issue #49. So far nothing has been changed. 9.1.2 Crop parameters verified The crop database has been updated and stored in the following file: &quot;model_data/cs10_setup/swat_input_mod/plants.plt&quot; Discussions to this topic can be found in issue #27. From the whole database, we only use crops defined in our management files: Crop management: oats, barl, wwht, swht, pota, fesc Generic: frst, fesc and others which are minor The parameters for these crops have been updated to reflect the colder growing conditions. 9.1.3 Soil physical parameters in final form This has been mentioned in issue #28, and has been closed without discussion. Seems like the parameters are in final form, but no further documentation exists at this point. 9.1.4 Soil chemical parameters in final form This step will be verified in issue #46. It has been discussed in #19. The following changes have been implemented, but are subject to change: Changes made to soilnut1 of nutrients.sol Parameter Default CS10 lab_p 5 20 nitrate 7 8.5 inorgp 3.5 35.1 watersol_p 0.15 0.4 nutrients_sol &lt;- readLines(&quot;model_data/cs10_setup/swat_input/nutrients.sol&quot;) header &lt;- nutrients_sol[1] nutrients_sol_df &lt;- read.table(&quot;model_data/cs10_setup/swat_input/nutrients.sol&quot;, header = T, skip = 1, sep = &quot;&quot;, fill = T, as.is = T, colClasses = &quot;character&quot;) nutrients_sol_df$lab_p &lt;- 20 nutrients_sol_df$nitrate &lt;- 8.5 nutrients_sol_df$inorgp &lt;- 35.1 nutrients_sol_df$watersol_p &lt;- 0.4 write.table( nutrients_sol_df, &quot;model_data/cs10_setup/swat_input_mod/nutrients.sol&quot;, quote = F, sep = &quot; &quot;, row.names = F, ) header_new &lt;- paste(header, &quot;and modified by the CS10 workflow on&quot;, Sys.time()) nutrients_sol &lt;- readLines(&quot;model_data/cs10_setup/swat_input_mod/nutrients.sol&quot;) writeLines(c(header_new, nutrients_sol), &quot;model_data/cs10_setup/swat_input_mod/nutrients.sol&quot;) Testing if the setup still works: simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) 9.1.5 Impoundment parameters defined Has been postponed by #20, will be dealt with in #54 9.1.6 Water diversions defined Deemed as not relevant for this catchment 9.1.7 Point sources parameters added Is an ongoing issue in #21 9.1.8 Tile drainage parameters defined Is an ongoing issue in #53. old_path &lt;- &quot;model_data/cs10_setup/swat_input/tiledrain.str&quot; new_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/tiledrain.str&quot; tiledrain_str &lt;- readLines(old_path) header &lt;- tiledrain_str[1] header &lt;- paste(header, &quot;then modified by CS10 workflow at&quot;, Sys.time()) tiledrain_df &lt;- read.table( old_path, sep = &quot;&quot;, header = T, skip = 1, fill = T, colClasses = &quot;character&quot; ) tiledrain_df$dp &lt;- 800 # from 1000 tiledrain_df$t_fc &lt;- 12 # from 24 tiledrain_df$lag &lt;- 30 # from 96 tiledrain_df$rad &lt;- 200 # from 100 tiledrain_df$dist &lt;- 8000 # from 30 tiledrain_df$drain &lt;- 40 # from 10 tiledrain_df$pump &lt;- 0 # from 1 tiledrain_df$lat_ksat &lt;- 2 # from 2 (no change) write.table(header, file = new_path, quote = F, col.names = F, row.names = F) write.table( x = tiledrain_df, file = new_path, sep = &quot; &quot;, quote = F, append = T, row.names = F ) ## Warning in write.table(x = tiledrain_df, file = new_path, sep = &quot; &quot;, quote = F, ## : appending column names to file Testing if the setup still works: simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) 9.1.9 Atmospheric deposition defined Has been done in section REF 9.1.10 Additional settings verified Refers to chapter 3.11 in The Protocol and files parameters.bsn codes.bsn Has been discussed in #22 The following changes have been made: Changes made to codes.bsn Parameter Default CS10 pet 1 2 rte_cha 0 1 cn 0 1 tiledrain 0 1 soil_p 0 1 atmo_dep a m old_path &lt;- &quot;model_data/cs10_setup/swat_input/codes.bsn&quot; new_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/codes.bsn&quot; codes_bsn &lt;- readLines(old_path) header &lt;- codes_bsn[1] header &lt;- paste(header, &quot;then modified by CS10 workflow on&quot;, Sys.time()) codes_df &lt;- read.table(old_path, skip = 1, header = T, sep = &quot;&quot;, colClasses = &quot;character&quot;) PET method (pet) The OPTAIN project recommends the PET calculation of Pennman-Monteith, which is code 2. codes_df$pet &lt;- 2 Channel routing network (rte_cha) Recommended is to start with Muskingum (code 1) and apply variable storage method if it causes problems. codes_df$rte_cha &lt;- 1 Stream water quality (wq_cha) Recommended to test both for OPTAIN (1/0) Daily curve number calculation (cn) Code 1 is recommended. (Plant ET) codes_df$cn &lt;- 1 OPTAIN recommends new version codes_df$soil_p &lt;- 1 Lapse rate is being tested in REF Plant growth stress is being testing in REF Tile drainage This should be set to 1, but that causes a crash. We will fix this bug in a dedicated secton (REF) codes_df$tiledrain &lt;- 0 Atmosdep was done in REF and is annual codes_df$atmo_dep &lt;- &quot;y&quot; Writing the changes write.table(header, file = new_path, col.names = F, row.names = F, quote = F) write.table( codes_df, file = new_path, col.names = T, append = T, quote = F, sep = &quot; &quot;, row.names = F ) ## Warning in write.table(codes_df, file = new_path, col.names = T, append = T, : ## appending column names to file Testing if the setup still works: simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) 9.2 Fixing Tiledrain Our problem is that some HRUs are drained, but on the wrong soil type. This leads to drains trying to be placed on soils that are not deep enough for them. This was probably caused by bad overlap of the land use and soil maps. require(mapview) require(sf) require(stringr) require(magrittr) require(dplyr) require(purrr) Let us identify where we have this problem. For this we need the soil data, land use map, and landuse.lum file. basin_shp &lt;- read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;) basin_shp$Id = &quot;CS10 Basin&quot; lu_lum &lt;- read.table(&quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot;, skip = 1, header = T, sep = &quot;&quot;) # need to match IDs lu_lum$type &lt;- lu_lum$name %&gt;% str_remove(&quot;_drn&quot;) %&gt;% str_remove(&quot;_lum&quot;) lu_shp &lt;- read_sf(&quot;model_data/input/land/CS10_LU.shp&quot;) lu_shp2&lt;-left_join(lu_shp, lu_lum, by = &quot;type&quot;) Our drains are set to 80cm depth. Which of our soils are less than that? NOTE: might need to adjust for drain radius! usersoil &lt;- readr::read_csv(&quot;model_data/input/soil/UserSoil_Krakstad.csv&quot;, show_col_types = F) shallow_soils &lt;- usersoil$SNAM[which(usersoil$SOL_ZMX&lt;800)] print(shallow_soils) ## [1] &quot;Mountain&quot; &quot;CMlen-dy&quot; &quot;Antropogenic&quot; &quot;Sea_dep_thin&quot; &quot;Humic&quot; ## [6] &quot;LVlen-sl&quot; &quot;End_moraine&quot; &quot;RGlen-dy-ar&quot; &quot;RGlep-dy&quot; &quot;STlen-dy&quot; ## [11] &quot;STlen-lv-sl&quot; &quot;STlen-rt-sl&quot; &quot;STlen-um-sl&quot; Which of our HRUs use these soils, and are drained? hru_data &lt;- read.table(&quot;model_data/cs10_setup/swat_input/hru-data.hru&quot;, skip = 1, header = T, sep = &quot;&quot;) shallow_hrus &lt;- which(hru_data$soil %in% shallow_soils) drained_hrus &lt;- grepl(x = hru_data$lu_mgt, pattern = &quot;_drn&quot;) %&gt;% which() problem_hrus &lt;- base::intersect(shallow_hrus, drained_hrus) length(problem_hrus) ## [1] 16 Ok. Just 16 HRUs have an issue. Lets get some info on these 16 problem_hru_data &lt;- hru_data[problem_hrus,] # match ID for left join problem_hru_data$type &lt;- problem_hru_data$lu_mgt %&gt;% str_remove(&quot;_drn_lum&quot;) problem_fields &lt;- problem_hru_data$lu_mgt %&gt;% str_remove(&quot;_drn_lum&quot;) problem_shps &lt;- lu_shp %&gt;% select(type, geometry) %&gt;% filter(type %in% problem_fields) problem_shps &lt;- left_join(problem_shps, problem_hru_data, by = &quot;type&quot;, relationship = &quot;many-to-many&quot;) Date processing done, lets have a look: basin_map &lt;- mapview( basin_shp, layer.name = &quot;CS10 Basin&quot;, col.region = &quot;yellow&quot;, alpha.region = 0, lwd = 4, color = &quot;orange&quot;, map.types = &quot;Esri.WorldImagery&quot; ) problem_map &lt;- mapview( problem_shps, color = &quot;red&quot;, lwd = 3, zcol = &quot;soil&quot;, layer.name = &quot;Problem HRUs&quot;, alpha.region = 0 ) full_map &lt;- basin_map + problem_map full_map Resolution to this problem is being discussed in issue #53 "],["swatfarmr-management-schedules.html", "Section 10 SWATFarmR Management Schedules 10.1 Pre-processing crop rotation 10.2 Initializing FarmR 10.3 Calculating Antecedent Precipitation Index 10.4 Scheduling Operations", " Section 10 SWATFarmR Management Schedules WIP: Every field in the OPTAIN SWAT setup needs its own management. We will use the SWATFarmR package to define it. run_this_chapter = FALSE Pre-requirements The creation of management schedules with SWATfarmR requires a SWAT+ model setup created by the SWATbuildR package. We also require the following package and functions: require(DT) require(ggplot2) require(readr) require(stringr) require(magrittr) #remotes::install_github(&quot;chrisschuerz/SWATfarmR&quot;) require(SWATfarmR) require(sf) require(tidyverse) require(lubridate) require(reshape2) require(remotes) require(dplyr) require(data.table) require(DT) source(&#39;model_data/swat_farmR/functions_write_SWATfarmR_input.R&#39;) 10.1 Pre-processing crop rotation This OPTAIN-provided workflow pre-processes our crop rotation data into a SWATfarmR compatible format. Authored by Michael Strauch, modified by Moritz Shore 10.1.1 Input files 10.1.1.1 Land use map with crop information This map has the following requirements: The map must contain the land use of each hru. In case of cropland, the names must be unique for each field (e.g., ‘field_1’, ‘field_2’, etc.) The map must also contain crop infos for the period 1988 to 2020 (or 2021 if crop info available). This requires an extrapolation of the available crop sequences (the sequences derived from remote-sensing based crop classification or local data). The extrapolated crop sequence for 33 years will be also used for running climate scenarios and must not contain any gaps. That means, gaps have to be closed manually! The year columns must be named y_1988, y_1989, etc. The crop infos for each year must match the crop_mgt names in the management operation schedules (provided in a .csv file, see below) see also section 4.1 of the modelling protocol (ref) lu_shp &lt;- &#39;model_data/input/crop/land_with_cr.shp&#39; lu &lt;- st_drop_geometry(read_sf(lu_shp)) datatable( lu %&gt;% head(50), extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE) ) 10.1.1.2 Management operation schedules for each crop ..or, if available, crop-management type. All schedules must be compiled in one csv file (see example in demo data and study also section 4.2 of the modelling protocol) ‘crop_mgt’ must start with the 4-character SWAT crop names (any further management specification is optional). Each schedule must contain a ‘skip’ line to indicate the change of years. The ‘skip’ line should never be the last line of a schedule. mgt_csv &lt;- &#39;model_data/input/management/mgt_crops_CS10.csv&#39; mgt_crop &lt;- read.csv(mgt_csv) datatable(mgt_crop, extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE)) 10.1.1.3 Management operation schedules for generic land-use classes Usually all non-cropland classes with vegetation cover here, all schedules must be provided already in the SWATfarmR input format. lu_generic_csv &lt;- &#39;model_data/input/management/farmR_generic_CS10.csv&#39; # generic land use management .csv table mgt_generic &lt;- read.csv(lu_generic_csv) datatable(mgt_generic) 10.1.2 Settings 10.1.2.1 Simulation Period start_y &lt;- 2010 # starting year (consider at least 3 years for warm-up!) end_y &lt;- 2020 # ending year 10.1.2.2 Prefix of cropland hrus all names of hrus with a crop rotation must begin with this prefix in column ‘lu’ of your land use hru_crops &lt;- &#39;a_&#39; 10.1.2.3 Multi-year farmland grass Did you define any multi-year farmland grass schedules? ‘y’ (yes), ‘n’ (no) m_yr_sch_existing &lt;- &#39;n&#39; If yes, define also the following variables. If not, skip next four lines crop_myr &lt;- &#39;past&#39; # name of your farmland grass Maximum number of years farmland grass can grow before it is killed (should be &lt;8) max_yr &lt;- 5 Do your multi-year farmland grass schedules consider the type of the following crop (summer or winter crop)? (e.g., a ’_1.5yr’ schedule with a kill op in spring allows for planting a summer crop immediately afterwards) If yes, you must define your summer crops crop_s &lt;- c(&#39;sgbt&#39;,&#39;csil&#39;,&#39;barl&#39;) Do your summer crop schedules usually start with an operation in autumn (e.g. tillage)? To combine them with farmland grass, it is necessary that you provide ‘half-year-schedules’ (‘half-year-schedules’ are additional summer crop schedules without operations in autumn) The adapted schedules should be added to the crop management table with suffix ’_0.5yr’ (e.g. ‘csil_0.5yr’) If additional ‘half-year-schedules’ are not needed, because your normal summer crop schedules do not start in autumn, type ‘n’ additional_h_yr_sch_existing &lt;- &#39;n&#39; # &#39;y&#39; (yes), &#39;n&#39; (no) 10.1.3 Checks Check for correct positioning of ‘skip’ line check_skip &lt;- check_skip_position() Check for date conflicts in single crop schedules check_date_conflicts1() Build schedules for crop sequences (Messages disabled) rota_schedules &lt;- build_rotation_schedules() Check for date conflicts in the full rotation schedule. (Messages disabled) check_date_conflicts2() Solve minor date conflicts (where only a few days/weeks are overlapping) (Messages disabled) rota_schedules &lt;- solve_date_conflicts() Check again for date conflicts (Messages disabled) check_date_conflicts2() 10.1.4 Writing input data Write the SWAT farmR input table write_farmR_input() The output of this pre-processing stage is loaded in from this file: farmR_input &lt;- readr::read_csv(&quot;model_data/input/management/farmR_input.csv&quot;, show_col_types = F) datatable( head(farmR_input, 50), extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE) ) TODO: Figure out why this is an issue. Are you using the wrong buildR output as source material for the crop generation? We need to fix the land use name, which entails replacing the _drn_lum with just _lum for all the agricultural fields. This is because land.shp from buildR does not include the _drn for some reason # TODO: Figure out why this is an issue. Are you using the wrong buildR output # as source material for the crop generation? farmR_input2 &lt;- farmR_input %&gt;% filter(grepl(x = land_use, &quot;a_&quot;)) %&gt;% mutate(land_use = str_replace(.$land_use, &quot;_lum&quot;, &quot;_drn_lum&quot;)) farmR_input3 &lt;- farmR_input %&gt;% filter(!grepl(x = land_use, &quot;a_&quot;)) farmR_input4 &lt;- rbind(farmR_input2, farmR_input3) write_csv(farmR_input4, file =&quot;model_data/input/management/farmR_input2.csv&quot;) rm(farmR_input, farmR_input2, farmR_input3, farmR_input4) # Yes I am aware that code is bad and ugly. 10.2 Initializing FarmR If the FarmR has never been initialized, then use new_farmr() and read_management(), otherwise load_farmr(). EVAL FALSE as BuildR script is still broken project_path &lt;- &quot;model_data/cs10_setup/swat_farmr/&quot; SWATfarmR::new_farmr(project_name = &quot;cs10&quot;, project_path = project_path ) #load_farmr(...) cs10$read_management(file = &quot;model_data/input/management/farmR_input2.csv&quot;) 10.3 Calculating Antecedent Precipitation Index TODO loading API from section REF # using temp API # TODO add real API # Load dplyr. We will use functions such as &#39;mutate&#39; and &#39;select&#39;. library(dplyr) # Extract the precipitation from the farmr project pcp &lt;- cs10$.data$variables$pcp # Extract the hydrologic soil group values for all HRUs hsg &lt;- select(cs10$.data$meta$hru_attributes, hru, hyd_grp) # Calculate api values for the hsg classes A to D api_A &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 1) api_B &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 0.8) api_C &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 0.7) api_D &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 0.5) # Bind the data together into one api table and name them with the hsgs api &lt;- bind_cols(api_A, api_B, api_C, api_D) names(api) &lt;- c(&#39;api_A&#39;, &#39;api_B&#39;, &#39;api_C&#39;, &#39;api_D&#39;) # To add the variable to the farmR you have to tell it which variables are # assigned to which HRUs hru_asgn &lt;- mutate(hsg, api = paste0(&#39;api_&#39;, hyd_grp)) %&gt;% select(hru, api) # Add the variable api to the farmR project cs10$add_variable(data = api, name = &#39;api&#39;, assign_unit = hru_asgn, overwrite = T) TODO 10.4 Scheduling Operations cs10$schedule_operations(start_year = 2016, end_year = 2019, n_schedule = 2) 10.4.1 Write Operations We cannot write from 1988 as it is limited to our climate data, which currently only spans back to 2010 cs10$write_operations(start_year = 2016, end_year = 2019) The following files have been updated: file.cio hru-data.hru -landuse.lum management.sch plant.ini These need to overwrite the older files in the run_swat directory. "],["model-verification.html", "Section 11 Model Verification 11.1 Analysis of simulated climate variables 11.2 Simulation of management operations 11.3 Stage 3: Analysis of unconstrained plant growth", " Section 11 Model Verification Preparation Loading required packages, defining paths, and loading objects. # Install/Update SWATdoctR if needed: # remotes::install_git(&#39;https://git.ufz.de/schuerz/swatdoctr&#39;, force = FALSE) require(SWATdoctR) require(DT) Generate Model Run veri_no_stress &lt;- run_swat_verification(project_path = &quot;model_data/cs10_setup/run_swat/&quot;, nostress = 0, keep_folder = F, outputs = &quot;wb&quot;) saveRDS(veri_no_stress, &quot;model_data/swat_doctR/verification_runs/veri_no_stress.rds&quot;) Load Model Run veri_no_stress &lt;- readRDS(&quot;model_data/swat_doctR/verification_runs/veri_no_stress.rds&quot;) 11.1 Analysis of simulated climate variables Modified from the Protocol: The climate variables daily precipitation and daily minimum/maximum temperatures are required inputs of a SWAT+ model setup (more info: protocol section 2.4). We have these present. Further climate inputs such as solar radiation, relative humidity and wind speed are optional input variables and can be essential for the calculation of the potential evapotranspiration (PET). We have these, see issue #42 (link) Climate inputs are grouped to weather stations in a model setup and are assigned to spatial objects (HRUs, channels, reservoirs, etc.) with the nearest neighbor method. We only have 1 weather station so this is not of great relevance. The input of weather data and the assignment of climate variables to spatial objects can be sources for several issues which must be analysed: Data structure of the climate input tables, units of the climate variable, no data flag, etc. was wrong and can result in unrealistically small or large values of the climate variables in the simulation. Does not seem to be the case in our data The nearest neighbor assignment allocates weather stations to spatial objects where the weather records do not represent the actual weather conditions in a spatial object well. This can for example be an issue in complex terrain. Should not be a concern for us (for now) since we only have a single met station The selected method for the calculation of PET results in an under/overestimation of PET when compared to estimates of PET for the region. In such cases other methods for the simulation of PET which are included in SWAT+ should be tested if they better fit the regional conditions and available weather inputs (see more Additional settings). See issue #39 Large implausibilities in the weather inputs can be identified in analyses of annual basin averages of the simulated climate variables. Simulated annual and average values of climate variables must be comparable to observation data and/or region specific literature values. Any larger deviations of precipitation can indicate errors in the input file or an inappropriate assignment of weather stations to spatial units. We have pure observations, so no simulated data. If the lapse rate option is active (Read more in Additional settings), it may be another potential reason for deviations from observations. We are tracking this topic in issue #39 (link). 11.1.1 Lapse rate on: # TODO 11.1.2 Lapse rate off: # TODO Over or underestimated PET can indicate errors in the temperature input files (and if provided in the solar radiation, relative humidity and wind speed inputs). We are tracking this topic in issue #39 (link). SWATdoctR provides the function plot_climate_annual() to analyse the annual simulated basin averages of climate variables. fig1 &lt;- plot_climate_annual(veri_no_stress) plot(fig1) (#fig:doc_plot_climate)Climate plot of CS10 generated by SWATDoctR The first panel shows ET fractions. Current version of SWAT+ often has implausible ET fractions! Are these values plausible? ANSWER: Are they Csilla? The second panel shows the precipitation fractions rainfall (rainfall) and snowfall (snofall). Are these values plausible? ANSWER: resolved in issue #43 (link) – but are they plausible Csilla? The third panel shows the annual temperature values. Are these values plausible? ANSWER: YES Csilla do you agree? The fourth panel shows the relative humidity values. Are these values plausible? ANSWER: resolved in issue #42 (link), but are they plausible? Csilla The fifth panel shows wind speed. Are these values plausible? ANSWER: resolved in #42, Csilla plausible? The sixth panel shows the annual sums of solar radiation. A comparison to literature values of annual solar radiation sums for the region can indicate issues in this input. Are these values plausible? ANSWER: Yes, values for the Oslo area seem to be around 4000 MJ – Csilla do you agree? The analysis of mean monthly precipitation (output variable precip), snowfall (output variable snofall) and snow melt (output variable snomlt) sums and their comparison with region specific information (or in the best case observations) provides insight in seasonal dynamics of the precipitation input. Particularly in snow impacted catchments a first verification of snowfall is valuable to see whether precipitation in solid form is simulated, a snow storage can build up and cause increased spring runoff through snow melt. The hydrological cycle of some catchments may be dominated by spring flood events which must be reflected by the simulated processes. Any observed implausibility in such analysis can indicate issues in the weather inputs or require to pay attention in the calibration of model parameters which control the simulation of snow processes (snofall_tmp, snomelt_tmp, snomelt_lag). CS10, a Boreal catchment, is impacted by snow melt – this is relevant to the verification plot_monthly_snow(sim_verify = veri_no_stress) (#fig:doc_plot_monthly_snow)Precipitation and Snowmelt on a monthly average basis, as generated by SWATDoctR This has been resolved in issue #43. (link). Are these values plausible csilla? In situations where not all required climate inputs are available which are necessary to estimate PET with PM method the estimates will be more uncertain and annual PET sums may differ to regional values. Then the use of a simpler method for the calculation of PET can be a valid solution. We do not need a simpler method since we have the required data STATUS: Waiting on climate verification before continuing to the next step: 11.2 Simulation of management operations Development of management tables is complicated and complex – and thus error prone. Mistakes in this area do not produce any errors or warnings making them easy to miss. All operations which are triggered in a SWAT+ simulation run are written into the file ‘mgt_out.txt’ To verify the operations, we are going to compare scheduled and simulated operations for specific HRUs by random sampling. For this, we need a SWAT+ run with management outputs. If we have already run this code, we do not need to run it again, and can just load it in: veri_mgt &lt;- readRDS(&quot;model_data/swat_doctR/verification_runs/veri_mgt.rds&quot;) Skip this code block, unless you want to re-run SWAT+ veri_mgt &lt;- run_swat_verification( project_path = &quot;model_data/cs10_setup/run_swat&quot;, keep_folder = T, outputs = &quot;mgt&quot; ) saveRDS(veri_mgt, &quot;model_data/swat_doctR/verification_runs/veri_mgt.rds&quot;) From the Protocol: “The function report_mgt() generates an overview report where the scheduled and triggered operations are matched and compared for each management schedule that was implemented in the simulations. The function prepares the scheduled management operations that were written in the input file ‘management.sch’ in tabular form and randomly samples one HRU for each defined schedule from the triggered management operations (from the output file ‘mgt_out.txt’). The comparison is only done for operations that were defined with a fixed date in the management schedule and operations which are triggered by decision tables will be excluded.” “Applying the function report_mgt() for the model verification simulation outputs returns a table with an overview of the operations which were scheduled but not triggered or operations where ‘op_data1’ differs in the scheduled and triggered operations.” mgt_report &lt;- report_mgt(veri_mgt, write_report = TRUE) ## Management OK! No differences between scheduled and triggered managments identified. Seems like there are no issues. I am not sure if this is a good or bad thing :D. “The report_mgt() function is a good starting point to explore the triggered management. But this analysis can be error prone. Still the safest way to analyse the triggered and the scheduled managements is to compare the input and output tables. SWATdoctR provides the function print_triggered_mgt() to print the triggered managements for individual HRUs. For selecting HRUs e.g. with a specific management the helper function get_hru_id_by_attribute() can be useful. This table can be visually compared with the management input table (‘management.sch’)” table &lt;- print_triggered_mgt(sim_verify = veri_mgt, hru_id = 92) ## Triggered managament for ## hru: 92 ## management: a_007f_1_drn_mgt_92_1 datatable(table) To see the operations of a specific management schedule: test &lt;- get_hru_id_by_attribute(veri_mgt, mgt = &quot;a_001f_1_drn_mgt_1731_1&quot;) tables &lt;- print_triggered_mgt(sim_verify = veri_mgt, hru_id = test$id[1]) ## Triggered managament for ## hru: 1731 ## management: a_001f_1_drn_mgt_1731_1 datatable(tables) “Operations which are missing in the simulated management schedules must be checked in the ‘management.sch’ input file. By answering the following questions for the scheduled management operations their proper implementation in the model setup can be verified:” Are the date sequences in the scheduled operations correct and in a right order (mistakes in assigned month and day values)? Answer: Does the variable op_data1 point to the correct entry in the respective input data file? Does the label exist in the input file? E.g. does defined op_data1 exist in ‘tillage.til’ for tillage operations, or does defined op_data1 exist in ‘plant.plt’ for plant operations Answer: Does the variable op_data2 point to the correct entry in the respective operations file (‘.ops’)? E.g. does harvest operation defined with op_data2 exist in ‘harv.ops’. Answer: 11.3 Stage 3: Analysis of unconstrained plant growth "],["references.html", "References", " References "]]
