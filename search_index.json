[["index.html", "OPTAIN CS-10 Introduction", " OPTAIN CS-10 Moritz Shore, Csilla Farkas 2023-09-09 Introduction Figure 0.1: 3D Render of the CS10 Watershed: Kraakstad THIS PAGE IS CURRENTLY A WORK IN PROGRESS The aim of page is to: Document the process of setting up the SWAT+ and SWAP models for Kraakstad, Norway (Case study 10 / CS10) Link and execute entire workflow in parallel with the documentation using Rmarkdown Make the model setup process reproducible Be a reference to the CS10 modelers, as well as other OPTAIN modelers. If all goes well, the compiling of this GitBook will also compile the OPTAIN model setup in tandem, from source input files to final model setup (..and beyond!) The model setup follows the OPTAIN modelling protocols for SWAP (Farkas et al. 2023) and SWAT+ (Schürz et al. 2022). We begin with SWAT+ and the input file preparation for SWATBuildR, in Section 1. Useful links: NIBIO project website NIBIO GitLab hosting model files (restricted access) UFZ GitLab hosting OPTAIN related software and data (restricted access) OPTAIN project website OPTAIN Zenodo hosting various files and documents SWATbuildR (restricted), SWATfarmR, SWATdoctR (restricted), SWATplusR, R-packages core to the OPTAIN workflow svatools, an R-package used in the OPTAIN workflow References "],["input-data-preparation.html", "Section 1 Input Data Preparation 1.1 Digital Elevation Model 1.2 Soil Data 1.3 Watershed Boundary 1.4 Land object input 1.5 Channel object input 1.6 Point sources locations 1.7 Aquifer Objects", " Section 1 Input Data Preparation run_this_chapter = TRUE run_wbt = FALSE This Section covers the input data preperation for the SWATbuildR. Some of the documentation of the original creation of the input data has been lost to time. We will need the following libraries. # Geospatial: require(raster) require(sf) require(terra) require(stars) require(whitebox) # Plotting require(gifski) require(ggplot2) require(ggrepel) require(rayshader) require(mapview) require(rgl) # Data manipulation require(tidyverse) require(DT) whitebox::wbt_init(exe_path = &quot;model_data/code/WBT/whitebox_tools.exe&quot;) # Common ggplot theme for simple features sf_theme &lt;- theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank() ) For our model setup, we need the following layers, Some of which will be created in this section. Figure 1.1: Required vector and raster inputs for a SWAT+ model setup with SWATbuildR Schürz et al. (2022) 1.1 Digital Elevation Model The DEM is sourced from hoydedata.no and has a resolution of 10m. 1m Resolution was available but not used. dem_path &lt;- &quot;model_data/input/elevation/dtm3_ns_v5.tif&quot; dem_rast &lt;- raster(dem_path) plot(dem_rast) Figure 1.2: CS10 Digital elevation model at 10 meter resolution. To our knowledge, it has a 10 meter resolution. 1 meter resolution was available but there seems to have been issues with using it. It is definitely preferable to use the 1m DEM as certain important information can be lost with (max allowed resolution) of 10m. Figure 1.3: An example of a hydrologically effective landscape features being lost due to coarse DEM resolution. From Schürz et al. (2022) 1.1.1 High resolution DEM From hoydedata.no, the 1M DEM tiles which cover our watershed are tile_1 &lt;- &quot;model_data/input/elevation/dtm1_33_124_112.tif&quot; tile_2 &lt;- &quot;model_data/input/elevation/dtm1_33_124_113.tif&quot; Lets have a look (Don’t mind the low plot resolution, it is to save on computer resources) tile1_rast &lt;- rast(tile_1) tile2_rast &lt;- rast(tile_2) tile1_rasterlayer &lt;- as(tile1_rast, &quot;Raster&quot;) tile2_rasterlayer &lt;- as(tile2_rast, &quot;Raster&quot;) tile1_map &lt;- mapview(tile1_rasterlayer, maxpixels = 1000) tile2_map &lt;- mapview(tile2_rasterlayer, maxpixels = 1000) basin_shp &lt;- read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;) basin_map &lt;- mapview(basin_shp) tile1_map+tile2_map+basin_map See that line that splits them? We need to combine them and we can do it with a raster mosaic: rast_collection &lt;- sprc(tile1_rast, tile2_rast) full_dem &lt;- terra::mosaic(rast_collection) Lets have a look: plot(full_dem) Figure 1.4: One meter digital elevation model for the CS10 area. How about that basin coverage? (Don’t mind the low resolution, its to save on memory and computation) dem_layer &lt;- as(full_dem, &quot;Raster&quot;) basin_map+mapview(dem_layer, maxpixels = 1000) Looks good, lets save it. writeRaster(full_dem, &quot;model_data/input/elevation/cs10_dem_1m_utm33n.tif&quot;, overwrite = TRUE) 1.2 Soil Data For BuildR to generate our soil layer, we need the following: a GeoTiff raster layer that defines the spatial locations of the soil classes (with integer ID values). &quot;model_data/input/soil/soil_raster.tif&quot; a lookup table in .csv format, which links the IDs of the soil classes with their names &quot;model_data/input/soil/soil_lookup_26.csv&quot; a usersoil table in .csv format, which provides physical and chemical parameters of all soil layers for each soil class that is defined in the raster layer and the lookup table. An example soil dataset can be acquired e.g. from the ExampleDatasets folder which comes with an installation of SWAT+. &quot;model_data/input/soil/usersoil_26_v2.csv&quot; The documentation for how we created our usersoil is lost to the sands of time, however a OPTAIN-Specific and R-based workflow does exist and can be used. This workflow is based on svatools and is explained here. Some fragments of our usersoil creation documentation can be found in this file &quot;model_data/legacy_setup/input_files/soil/UserSoil_Krakstad.xlsx&quot; 1.2.1 High resolution soil maps A high resolution soil map is required to accurately represent the Hydrologic Response Units (HRUs) used in the OPTAIN project: Figure 1.5: Spatially adequate representation of soil heterogeneity on field plot scale. The different colours represent different soil classes. The black border lines represent the boundaries of spatial objects (e.g. agricultural fields). a) the original soil input layer. b) the dominant soil aggregation as performed by SWATbuildR. Schürz et al. (2022) The use of an extremely detailed soil map is however not recommended: The use of soil information on a scale which is much more detailed than the scale of the spatial objects in a model setup is however not recommended. It might be difficult to provide parameters to the usersoil table for less common soil types. Moreover, by default SWATbuildR uses the dominant soil class for each spatial object in the model setup process (see Figure 2.3b). Thus, a great share of the detailed information would get lost during the model setup process. One exception from that rule may be soil physical data which are available in a raster format 1.2.2 Data source Our soil map is created from merging two separate datasets, the first being Jordsmonn from NIBIO and Loesmasser from NGU. The files have been pre-cropped to watershed extent to save on storage space and computation time. Jordsmonn is located here: jm_path &lt;- &quot;model_data/input/soil/jordsmonn.shp&quot; jm_shp &lt;- read_sf(jm_path) jm_plot &lt;- ggplot(jm_shp) + geom_sf(mapping = aes(fill = WRBGRUPPER)) + sf_theme jm_plot Figure 1.6: Jordsmonn Map for CS10 It only covers agricultural land, and therefore is not enough for complete coverage of the catchment, for this we need to combine it with our with out other map, Loesmasser. lm_path &lt;- &quot;model_data/input/soil/loesmasser_cs10.shp&quot; lm_shp &lt;- read_sf(lm_path) lm_plot &lt;- ggplot(lm_shp) + geom_sf(mapping = aes(fill = jorda_navn)) + sf_theme lm_plot Figure 1.7: Loesmasser Map for CS10 1.2.3 Combining Jordsmonn and Loesmasser Are they in the same CRS? st_crs(lm_shp) == st_crs(jm_shp) ## [1] TRUE We will need a mask of the JM (unite it to a single polygon) jm_mask &lt;- st_union(jm_shp) ggplot(jm_mask) + geom_sf(fill = &quot;orange&quot;) + sf_theme Figure 1.8: Jordsmonn Mask And now we need to clip LM clipped_lm &lt;- st_difference(lm_shp, jm_mask) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries ggplot(clipped_lm) + geom_sf(mapping = aes(fill = jorda_navn)) + sf_theme Figure 1.9: Clipped Loesmasser Map Now we can merge the two maps. But the columns must match: jm_shp_sel &lt;- jm_shp %&gt;% dplyr::select(geometry, WRBGRUPPER) lm_shp_sel &lt;- clipped_lm %&gt;% dplyr::select(geometry, jorda_navn) colnames(jm_shp_sel) &lt;- c(&quot;geometry&quot;, &quot;name&quot;) colnames(lm_shp_sel) &lt;- c(&quot;geometry&quot;, &quot;name&quot;) Merge the names soil_union &lt;- rbind(jm_shp_sel, lm_shp_sel) Rename the absurdly long soil names: soil_union$name &lt;- soil_union$name %&gt;% str_replace(&quot;Hav-, fjord- og strandavsetning, usammenhengende eller tynt dekke over berggrunnen&quot;, &quot;Sea_dep_thin&quot;) %&gt;% str_replace(&quot;Hav- og fjordavsetning, sammenhengende dekke, stedvis med stor mektighet&quot;, &quot;Sea_dep_thick&quot;) %&gt;% str_replace(&quot;Marin strandavsetning, sammenhengende dekke&quot;, &quot;Beach_dep&quot;) %&gt;% str_replace(&quot;Tynt dekke av organisk materiale over berggrunn&quot;, &quot;Humic&quot;) %&gt;% str_replace(&quot;Randmorene/randmorenesone&quot;, &quot;End_moraine&quot;) %&gt;% str_replace(&quot;Bart fjell&quot;, &quot;Mountain&quot;) %&gt;% str_replace(&quot;Torv og myr&quot;, &quot;Bog_organic&quot;) %&gt;% str_replace(&quot;Morenemateriale, usammenhengende eller tynt dekke over berggrunnen&quot;, &quot;moraine_dep&quot;) soil_union$name[which(soil_union$name == &quot;Fyllmasse (antropogent materiale)&quot;)] = &quot;Antropogenic&quot; soil_union$name[which(soil_union$name == &quot;Elve- og bekkeavsetning (Fluvial avsetning)&quot;)] = &quot;river_dep&quot; soil_union$name[which(soil_union$name == &quot;Innsjøavsetning (Lakustrin avsetning)&quot;)] = &quot;lake_dep&quot; soil_union$name[which(soil_union$name == &quot;Breelvavsetning (Glasifluvial avsetning)&quot;)] = &quot;glacier_dep&quot; The map: ggplot(soil_union) + geom_sf(mapping = aes(fill = name)) + sf_theme Figure 1.10: Merged soil map for CS10 Now we need to relate these classes to those we have in the soil data, which is located here: usersoil &lt;- read_csv(&quot;model_data/input/soil/usersoil_26.csv&quot;, show_col_types = F) And contains these soil names: usersoil$SNAM %&gt;% unique() %&gt;% sort() ## [1] &quot;ANT&quot; &quot;ARD&quot; &quot;BEA&quot; &quot;BOG&quot; &quot;CMD&quot; &quot;CME&quot; &quot;END&quot; &quot;GLC&quot; &quot;GLU&quot; &quot;HS&quot; &quot;HUM&quot; &quot;LV&quot; ## [13] &quot;MON&quot; &quot;PL&quot; &quot;PZ&quot; &quot;RG&quot; &quot;SDD&quot; &quot;SDS&quot; &quot;STC&quot; &quot;STE&quot; &quot;STL&quot; &quot;STP&quot; &quot;STR&quot; &quot;STS&quot; ## [25] &quot;STV&quot; &quot;TUM&quot; These are 26 soil classes usersoil$SNAM %&gt;% unique() %&gt;% length() ## [1] 26 Which is more than our soil map has soil_union$name %&gt;% unique() %&gt;% length() ## [1] 25 Therefore, this Jordmonn map is not detailed enough for our desired 26 soil classes. 1.2.4 Detailed Soil Map Thankfully we have a more detailed map which contains more information from NIBIOs internal maps department. This is located here: jm_detailed &lt;- &quot;model_data/input/soil/soil_map_detailed.shp&quot; jm_detailed_shp &lt;- read_sf(jm_detailed) ggplot(jm_detailed_shp) + geom_sf(mapping = aes(fill = kartkode1)) + sf_theme Figure 1.11: Detailed soil map for CS10 from the NIBIO maps department. That is way more. Same CRS? st_crs(jm_detailed_shp) == st_crs(lm_shp) ## [1] TRUE We will now repeat the process with the detailed map: jm_det_mask &lt;- st_union(jm_detailed_shp) clipped_lm &lt;- st_difference(lm_shp, jm_det_mask) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries jm_shp_sel &lt;- jm_detailed_shp %&gt;% dplyr::select(geometry, kartkode1) lm_shp_sel &lt;- clipped_lm %&gt;% dplyr::select(geometry, jorda_navn) colnames(jm_shp_sel) &lt;- c(&quot;geometry&quot;, &quot;name&quot;) colnames(lm_shp_sel) &lt;- c(&quot;geometry&quot;, &quot;name&quot;) soil_union &lt;- rbind(jm_shp_sel, lm_shp_sel) jm_det_select &lt;- jm_detailed_shp %&gt;% dplyr::select(geometry, kartkode1) colnames(jm_det_select) &lt;- c(&quot;geometry&quot;, &quot;name&quot;) soil_union_det &lt;- rbind(jm_det_select, lm_shp_sel) soil_union_det$name &lt;- soil_union_det$name %&gt;% str_replace(&quot;Hav-, fjord- og strandavsetning, usammenhengende eller tynt dekke over berggrunnen&quot;, &quot;Sea_dep_thin&quot;) %&gt;% str_replace(&quot;Hav- og fjordavsetning, sammenhengende dekke, stedvis med stor mektighet&quot;, &quot;Sea_dep_thick&quot;) %&gt;% str_replace(&quot;Marin strandavsetning, sammenhengende dekke&quot;, &quot;Beach_dep&quot;) %&gt;% str_replace(&quot;Tynt dekke av organisk materiale over berggrunn&quot;, &quot;Humic&quot;) %&gt;% str_replace(&quot;Randmorene/randmorenesone&quot;, &quot;End_moraine&quot;) %&gt;% str_replace(&quot;Bart fjell&quot;, &quot;Mountain&quot;) %&gt;% str_replace(&quot;Torv og myr&quot;, &quot;Bog_organic&quot;) %&gt;% str_replace(&quot;Morenemateriale, usammenhengende eller tynt dekke over berggrunnen&quot;, &quot;End_moraine&quot;) ### This will be simplified right now: soil_union_det$name[which(soil_union_det$name == &quot;Fyllmasse (antropogent materiale)&quot;)] = &quot;Antropogenic&quot; soil_union_det$name[which(soil_union_det$name == &quot;Breelvavsetning (Glasifluvial avsetning)&quot;)] = &quot;End_moraine&quot; soil_union_det$name[which(soil_union_det$name == &quot;Innsjøavsetning (Lakustrin avsetning)&quot;)] = &quot;Sea_dep_thick&quot; soil_union_det$name[which(soil_union_det$name == &quot;Elve- og bekkeavsetning (Fluvial avsetning)&quot;)] = &quot;Sea_dep_thick&quot; We now have 79 soil classes. soil_union_det$name %&gt;% unique() %&gt;% length() ## [1] 79 We will save this because it will come in handy later sf::write_sf(soil_union_det, &quot;model_data/input/soil/soil_map_79.shp&quot;) 1.2.5 Reclassification We will rename and simplify the classes like so: soil_reclass &lt;- read_csv(&quot;model_data/input/soil/soil_reclass.csv&quot;,show_col_types = F) datatable(soil_reclass) Reclassification Loop soil_union_det$id = NA for (soil in soil_reclass$old_name) { reclassed &lt;- soil_reclass$new_name[which(soil_reclass$old_name == soil)] ID &lt;- soil_reclass$id[which(soil_reclass$old_name == soil)] soil_union_det$name[which(soil_union_det$name == soil)] &lt;- reclassed soil_union_det$id[which(soil_union_det$name == soil)] &lt;- ID } Now let us map it: soil_complex_plot &lt;- ggplot(soil_union_det) + geom_sf(mapping = aes(fill = name)) + sf_theme soil_complex_plot Figure 1.12: Reclassified detailed map 1.2.6 Simplifying Geometries We can simplify the geometry by aggregating by soil name soil_simple &lt;- soil_union_det %&gt;% group_by(name) %&gt;% summarize() soil_simple_plot &lt;- ggplot(soil_simple) + geom_sf(mapping = aes(fill = name)) + sf_theme soil_simple_plot Figure 1.13: Simplified Geometry of the soil map And a comparison: Figure 1.14: Soil map simplification 1.2.7 Writing vector layer We will need this vector layer later on in the project, so we will save it now sf::write_sf(soil_simple, &quot;model_data/input/soil/cs10_soil_map.shp&quot;) 1.2.8 Lookup Table In its final form, this map needs to be a RASTER with the ID as a value, and we need a lookup table to relate the ID to the soil. Creating the look-up table new_name &lt;- soil_reclass %&gt;% dplyr::select(new_name) %&gt;% dplyr::pull() %&gt;% base::unique() soil_lookup &lt;- data.frame(SOIL_ID = seq_along(new_name), SNAM = new_name) write_csv(x = soil_lookup, file = &quot;model_data/input/soil/soil_lookup_26.csv&quot;) 1.2.9 Rasterization We need to relate these new IDs to our soil map with a left join soil_simple$SNAM &lt;- soil_simple$name soil_simple_id &lt;- left_join(x = soil_simple, y = soil_lookup, by = &quot;SNAM&quot;) ggplot(soil_simple_id) + geom_sf(mapping = aes(fill = SOIL_ID)) + sf_theme Figure 1.15: Soil map by soil ID Now we need to rasterize with this ID as the value. soil_rast &lt;- stars::st_rasterize(soil_simple_id, stars::st_as_stars(sf::st_bbox(soil_simple_id), nx = 3060, # resolution matches the original ny = 5352) # resolution marches the original ) ## Warning in CPL_rasterize(file, driver, st_geometry(sf), values, options, : GDAL ## Message 1: The definition of geographic CRS EPSG:4258 got from GeoTIFF keys is ## not the same as the one from the EPSG registry, which may cause issues during ## reprojection operations. Set GTIFF_SRS_SOURCE configuration option to EPSG to ## use official parameters (overriding the ones from GeoTIFF keys), or to GEOKEYS ## to use custom values from GeoTIFF keys and drop the EPSG code. ## Warning in CPL_read_gdal(as.character(x), as.character(options), ## as.character(driver), : GDAL Message 1: The definition of geographic CRS ## EPSG:4258 got from GeoTIFF keys is not the same as the one from the EPSG ## registry, which may cause issues during reprojection operations. Set ## GTIFF_SRS_SOURCE configuration option to EPSG to use official parameters ## (overriding the ones from GeoTIFF keys), or to GEOKEYS to use custom values ## from GeoTIFF keys and drop the EPSG code. soil_rast &lt;- as(soil_rast, &quot;SpatRaster&quot;) plot(soil_rast) Figure 1.16: Rasterized soil map Check the values: # crop to watershed extent wsmask &lt;- soil_rast %&gt;% terra::crop(., read_sf(&quot;model_data/input/shape/cs10_basin_1m.shp&quot;)) # get frequency of pixels soil_histogram &lt;- raster::freq(wsmask) # convert to tibble soil_hist &lt;- as_tibble(soil_histogram) # get total sum for relative value total_pixels &lt;- soil_hist$count %&gt;% sum() # relative value calculation soil_hist$rel &lt;- (soil_hist$count/total_pixels)*100 # Round to 2 decimals soil_hist$rel&lt;-soil_hist$rel %&gt;% round(2) # left join look up table soil_hist$SOIL_ID &lt;- soil_hist$value soil_hist &lt;- left_join(soil_hist, soil_lookup, by = &quot;SOIL_ID&quot;) # sort by size soil_hist&lt;-soil_hist %&gt;% arrange(rel) # Rearrange and drop duplicate columns soil_hist &lt;- soil_hist %&gt;% dplyr::select(SOIL_ID, SNAM, count, rel) datatable(soil_hist) Pie chart of results: # Get the positions df2 &lt;- soil_hist %&gt;% mutate(csum = rev(cumsum(rev(rel))), pos = rel/2 + lead(csum, 1), pos = if_else(is.na(pos), rel/2, pos)) # Pie chart ggplot(soil_hist, aes(x = &quot;&quot; , y = rel, fill = reorder(SNAM, rel))) + geom_col(width = 1, color = 1) + coord_polar(theta = &quot;y&quot;) + geom_label_repel( data = df2, aes(y = pos, label = paste0(SNAM, &quot; &quot;, rel, &quot;%&quot;)), size = 2, force = 3, nudge_x = 1, show.legend = FALSE, box.padding = 0, seed = 123456, force_pull = 0, direction = &quot;both&quot;, max.overlaps = 26 ) + guides(fill = guide_legend(title = &quot;Soil&quot;)) + theme_void() Figure 1.17: Soil distribtution within the catchment area. And check the CRS print(soil_rast) ## class : SpatRaster ## dimensions : 5352, 3060, 1 (nrow, ncol, nlyr) ## resolution : 4.905229, 5.60725 (x, y) ## extent : 260425, 275435, 6605995, 6636005 (xmin, xmax, ymin, ymax) ## coord. ref. : ETRS89 / UTM zone 33N (EPSG:25833) ## source(s) : memory ## name : SOIL_ID ## min value : 1 ## max value : 26 raster::writeRaster(soil_rast, &quot;model_data/input/soil/soil_raster.tif&quot;, overwrite = T) 1.2.10 Usersoil We need to update our usersoil data now to match the IDs, as we can see there are quite a few differences: usersoil26 &lt;- read_csv(&quot;model_data/input/soil/usersoil_26.csv&quot;, show_col_types = F) US &lt;- usersoil26 %&gt;% select(OBJECTID, SNAM) US &lt;- US %&gt;% arrange(SNAM) compare &lt;- cbind(US, soil_lookup %&gt;% arrange(SNAM)) datatable(compare) A few names are mismatched as well usersoil26$SNAM[which(usersoil26$SNAM == &quot;CMD&quot;)] = &quot;CM1&quot; usersoil26$SNAM[which(usersoil26$SNAM == &quot;CME&quot;)] = &quot;CM2&quot; usersoil26$SNAM[which(usersoil26$SNAM == &quot;SDS&quot;)] = &quot;SDs&quot; usersoil26$SNAM[which(usersoil26$SNAM == &quot;SDD&quot;)] = &quot;SDd&quot; usersoil26$SNAM[which(usersoil26$SNAM == &quot;MON&quot;)] = &quot;Mon&quot; Reclassification loop: for (soil in soil_lookup$SNAM) { new_id &lt;- soil_lookup %&gt;% filter(SNAM == soil) %&gt;% select(SOIL_ID) %&gt;% pull() old_id &lt;- which(usersoil26$SNAM==soil) usersoil26$OBJECTID[old_id] = new_id } usersoil26$OBJECTID ## [1] 2 13 5 6 1 8 9 18 17 10 11 12 3 14 15 7 16 19 20 21 22 23 24 25 4 ## [26] 26 Lets sort by the new order. usersoil_ordered &lt;- usersoil26[order(usersoil26$OBJECTID),] Now we can write the changes with the updated IDs write_csv(usersoil_ordered, &quot;model_data/input/soil/usersoil_26_v2.csv&quot;) 1.3 Watershed Boundary The watershed boundary must be a single polygon GIS vector layer. Presumed source of the data is NVE. It is located here: basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; We will be following this guide to create it using whitebox tools (REF). We will be using a single tile of our 1 DEM to start, since the whole dataset is large dem &lt;- raster(&quot;model_data/input/elevation/cs10_dem_1m_utm33n.tif&quot;) plot(dem, main = &quot;UTM 33N&quot;) 1.3.1 Hydrological preperation Time to breach and fill: wbt_breach_depressions_least_cost( dem = &quot;model_data/input/elevation/cs10_dem_1m_utm33n.tif&quot;, output = &quot;model_data/input/elevation/intermediates/dem_breached.tif&quot;, dist = 5, fill = TRUE) wbt_fill_depressions_wang_and_liu( dem = &quot;model_data/input/elevation/intermediates/dem_breached.tif&quot;, output = &quot;model_data/input/elevation/intermediates/dem_filled_breached.tif&quot; ) Flow Accumulation wbt_d8_flow_accumulation(input = &quot;model_data/input/elevation/intermediates/dem_filled_breached.tif&quot;, output = &quot;model_data/input/elevation/intermediates/D8FA.tif&quot;) And D8 pointer wbt_d8_pointer(dem = &quot;model_data/input/elevation/intermediates/dem_filled_breached.tif&quot;, output = &quot;model_data/input/elevation/intermediates/D8pointer.tif&quot;) 1.3.2 Pour Points Our outlet point is located here: outlet &lt;- &quot;model_data/input/point/cs10_outlet_utm33n.shp&quot; outlet_point &lt;- read_sf(outlet) outlet_map &lt;- mapview(outlet_point) outlet_map We will need to snap it to the channel network. But first we need those streams. 1.3.3 Streams Extracting our streams wbt_extract_streams(flow_accum = &quot;model_data/input/elevation/intermediates/D8FA.tif&quot;, output = &quot;model_data/input/elevation/intermediates/raster_streams.tif&quot;, threshold = 6000) To verify this threshold, lets have a look at the map stream_rast &lt;- rast(&quot;model_data/input/elevation/intermediates/raster_streams.tif&quot;) plot(stream_rast) That does not look good. But if we zoom in: plot(stream_rast, xlim = c(269400,269500), ylim = c(6622100, 6622200)) Lets check if they line up with our actual streams. extent_coord &lt;- data.frame( Plot = c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;), Corner = c(&quot;SW&quot;, &quot;NW&quot;, &quot;NE&quot;, &quot;SE&quot;), Easting = c(268280, # SW 268280, # NW 268480, # NE 268480), # SE Northing = c(6613423, # SW 6613623, # NW 6613623, # NE 6613423) # SE ) extent &lt;- sfheaders::sf_polygon( obj = extent_coord , x = &quot;Easting&quot; , y = &quot;Northing&quot; , polygon_id = &quot;Plot&quot; ) sf::st_crs(extent) &lt;- 25833 mapview(extent)+outlet_map Cropping by our extent, and checking the location of the pixels referenced to a map: stream_crop &lt;- raster::crop(stream_rast, extent) stream_crop &lt;- as(stream_crop, &quot;Raster&quot;) stream_crop_map &lt;- mapview(stream_crop, maxpixels = 2200000) stream_crop_map Looks good. Snapping our pour point to the network wbt_jenson_snap_pour_points( pour_pts = &quot;model_data/input/point/cs10_outlet_utm33n.shp&quot;, streams = &quot;model_data/input/elevation/intermediates/raster_streams.tif&quot;, output = &quot;model_data/input/point/snappedpp.shp&quot;, snap_dist = 35) #careful with this! Know the units of your data Did it work? pp &lt;- shapefile(&quot;model_data/input/point/snappedpp.shp&quot;) streams &lt;- raster(&quot;model_data/input/elevation/intermediates/raster_streams.tif&quot;) stream_crop_map+mapview(pp) It seems to have worked. 1.3.4 Watershed Delineation Finally, we are ready to delineate the watershed. wbt_watershed(d8_pntr = &quot;model_data/input/elevation/intermediates/D8pointer.tif&quot;, pour_pts = &quot;model_data/input/point/snappedpp.shp&quot;, output = &quot;model_data/input/elevation/intermediates/dem_basin.tif&quot;) Preview: ws &lt;- raster::raster(&quot;model_data/input/elevation/intermediates/dem_basin.tif&quot;) mapview(ws) ## Warning in rasterCheckSize(x, maxpixels = maxpixels): maximum number of pixels for Raster* viewing is 5e+05 ; ## the supplied Raster* has 450450100 ## ... decreasing Raster* resolution to 5e+05 pixels ## to view full resolution set &#39;maxpixels = 450450100 &#39; Now we need this in polygon form. wsshape &lt;- stars::st_as_stars(ws) %&gt;% sf::st_as_sf(merge = T) ## Warning in CPL_read_gdal(as.character(x), as.character(options), ## as.character(driver), : GDAL Message 1: The definition of geographic CRS ## EPSG:4258 got from GeoTIFF keys is not the same as the one from the EPSG ## registry, which may cause issues during reprojection operations. Set ## GTIFF_SRS_SOURCE configuration option to EPSG to use official parameters ## (overriding the ones from GeoTIFF keys), or to GEOKEYS to use custom values ## from GeoTIFF keys and drop the EPSG code. ## Warning in CPL_read_gdal(as.character(x), as.character(options), ## as.character(driver), : GDAL Message 1: The definition of geographic CRS ## EPSG:4258 got from GeoTIFF keys is not the same as the one from the EPSG ## registry, which may cause issues during reprojection operations. Set ## GTIFF_SRS_SOURCE configuration option to EPSG to use official parameters ## (overriding the ones from GeoTIFF keys), or to GEOKEYS to use custom values ## from GeoTIFF keys and drop the EPSG code. ## Warning in CPL_polygonize(file, mask_name, &quot;GTiff&quot;, &quot;Memory&quot;, &quot;foo&quot;, options, : ## GDAL Message 1: The definition of geographic CRS EPSG:4258 got from GeoTIFF ## keys is not the same as the one from the EPSG registry, which may cause issues ## during reprojection operations. Set GTIFF_SRS_SOURCE configuration option to ## EPSG to use official parameters (overriding the ones from GeoTIFF keys), or to ## GEOKEYS to use custom values from GeoTIFF keys and drop the EPSG code. ws_1m_map &lt;- mapview(wsshape) ws_1m_map Amazing, so high res. Lets write it sf::write_sf(wsshape, &quot;model_data/input/shape/cs10_basin_1m.shp&quot;) A comparison to the old 10m dem: dem10m &lt;- read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;) ws_1m_map+mapview(dem10m, col.region = &quot;orange&quot;) 1.3.5 Bonus Section Lets make a 3D map for fun dem &lt;- raster::raster(&quot;model_data/input/elevation/cs10_dem_1m_utm33n.tif&quot;) #crop wsmask &lt;- dem %&gt;% terra::crop(.,extent ) %&gt;% terra::mask(., extent) #convert to matrix wsmat &lt;- matrix( terra::extract(wsmask, extent(wsmask)), nrow = ncol(wsmask), ncol = nrow(wsmask)) #create hillshade raymat &lt;- ray_shade(wsmat, sunangle = 115) #render wsmat %&gt;% sphere_shade(texture = &quot;desert&quot;) %&gt;% add_water(detect_water(wsmat), color = &quot;desert&quot;) %&gt;% #add_shadow(raymat) %&gt;% plot_3d( wsmat, zscale = .8, fov = 0, theta = 135, zoom = 0.75, phi = 45, windowsize = c(750, 750) ) rglwidget() 1.4 Land object input Requirements: column id number for each element column type defining land use column drainage containing the id of the channel receiving water from the drained field. Figure 1.18: Land object polygons with attribute table. a) shows a land layer without tile drainage implemented. b) shows a case where the agricultural field with the land id = 2 has tile drainage activated and drains the tile flow into the channel id = 23 Schürz et al. (2022) The creation of the land object map was not documented. It is located here: lo_path &lt;- &quot;model_data/input/land/CS10_LU.shp&quot; A preview: lo_shp &lt;- read_sf(lo_path) lo_plot &lt;- ggplot(lo_shp) + geom_sf(mapping = aes(fill = drainage)) + sf_theme + ggtitle(&quot;CS10 Landuse map&quot;, &quot;Tile drainage indicated by ID of recieving channel&quot;) print(lo_plot) Figure 1.19: CS10 Landuse map 1.4.1 Data sources Only partial documentation is available: For Landuse, the Corine landcover 2018 (CLC) sourced from NIBIOs map service kart8 (details unknown) Field boundaries were sourced from the norwegian property register, hosted on kartverket (details unknown) 1.4.2 Delineation of land objects Currently not documented 1.4.3 Standing water bodies and the landuse type = ‘watr’ Currently not documented 1.4.4 Tile drainage option To determine which of our agricultural fields need drainage, we are going to use the Jordsmonn Map, specifically the Natural drainage capacity (Naturlige dreneringsforhold) described in column “JR_DREN”. Documentation for this dataset can be found here. The datasets have been clipped to the catchment area before this analysis (to reduce file sizes and computation time). Discussion of this topic can be found in Issue #65 jordmonn &lt;- &quot;model_data/input/soil/jordsmonn.shp&quot; jm_shp &lt;- read_sf(jordmonn) There is a lot of junk in this file (89 columns of data), we only need JR_DREN jm_shp_filt &lt;- jm_shp %&gt;% select(geometry, JR_DREN) Classes 1,2 and 3 have the need for tile drains, whereas class 4 has enough natural drainage capacity to forgo tile drainage. jm_drained &lt;- jm_shp_filt %&gt;% filter(JR_DREN != &quot;4&quot;) jm_not_drained &lt;- jm_shp_filt %&gt;% filter(JR_DREN == &quot;4&quot;) A comparison: Figure 1.20: Tile drainage Classes The drainage column of our land use map has already been defined, however every field regardless of its natural drainage class, has been assigned tile drains. We will now remove the tile drains which are located on class 4 soils. First we will drop any unneeded data in our cluttered land object map. lu_map &lt;- lo_shp %&gt;% select(geometry, id, type, drainage) And convert our drainage map to the same projection. jm_shp_filt &lt;- st_transform(jm_shp_filt, st_crs(lu_map)) Then we will join the drainage class attribute spatially. intersect_pct_4 &lt;- st_intersection(lu_map, jm_shp_filt %&gt;% filter(JR_DREN == &quot;4&quot;)) %&gt;% mutate(intersect_4 = st_area(.)) %&gt;% # create new column with shape area dplyr::select(id, intersect_4) %&gt;% # only select columns needed to merge st_drop_geometry() ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries intersect_pct_not_4 &lt;- st_intersection(lu_map, jm_shp_filt %&gt;% filter(JR_DREN != &quot;4&quot;)) %&gt;% mutate(not_4 = st_area(.)) %&gt;% # create new column with shape area dplyr::select(id, not_4) %&gt;% # only select columns needed to merge st_drop_geometry() ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries Total the individual intersectoins class_4 &lt;- intersect_pct_4 %&gt;% group_by(id) %&gt;% summarise(class_4 = sum(intersect_4)) not_class_4 &lt;- intersect_pct_not_4 %&gt;% group_by(id) %&gt;% summarise(not_class_4 = sum(not_4)) Left join them to our land use map by their ID lu_drains &lt;- left_join(lu_map, class_4, by = &quot;id&quot;) lu_drains &lt;- left_join(lu_drains, not_class_4, by = &quot;id&quot;) Set NA values to 0 lu_drains$class_4[which(lu_drains$class_4 %&gt;% is.na())] = 0 lu_drains$not_class_4[which(lu_drains$not_class_4 %&gt;% is.na())] = 0 Predefining our natural drainage attribute as false and defining our generic land use classes which are to be excluded from the manipulation. lu_drains$natural_drain = FALSE generics &lt;- c(&quot;frst&quot;,&quot;rngb&quot;,&quot;urml&quot;,&quot;utrn&quot;,&quot;past&quot;, &quot;watr&quot;, &quot;wetf&quot;) lu_drains$natural_drain[which((lu_drains$class_4 &gt;= lu_drains$not_class_4) &amp; (lu_drains$type %in% generics == FALSE))] = TRUE How many drains are we removing? removed_len &lt;- which((lu_drains$class_4 &gt;= lu_drains$not_class_4) &amp; (lu_drains$type %in% generics == FALSE)) %&gt;% length() removed_len ## [1] 238 Seems like a lot, but how many drained HRUs are there in total? total_drains &lt;- lu_map %&gt;% filter(drainage &gt; 0) %&gt;% pull(drainage) %&gt;% length() total_drains ## [1] 1947 paste(((removed_len / total_drains) * 100) %&gt;% round(1), &quot;% of drained agricultural HRUs will have their drains removed.&quot;) ## [1] &quot;12.2 % of drained agricultural HRUs will have their drains removed.&quot; Time to remove: lu_drains$drainage[which(lu_drains$natural_drain == TRUE)] = NA And drop the columns we don’t need lu_map_v2 &lt;- lu_drains %&gt;% select(geometry, id, type, drainage) Which fields are naturally drained? basin_map + mapview(lu_drains %&gt;% filter(natural_drain), zcol = &quot;type&quot;, legend = FALSE) Our land use map now looks like this. The Number value points to the channel being drained to. lu_v2 &lt;- ggplot(lu_map_v2) + geom_sf(mapping = aes(fill = drainage)) + sf_theme + ggtitle(&quot;CS10 Landuse map&quot;, &quot;Tile drainage indicated by ID of recieving channel&quot;) print(lo_plot) print(lu_v2) Figure 1.21: Tile drainage which was removed Time to save the changes: write_sf(lu_map_v2, &quot;model_data/input/land/CS10_LU_V2.shp&quot;) And cleanup rm(list = ls()) 1.4.5 Existing and potential structures to retain water and nutrients Currently not documented 1.5 Channel object input Limited documentation available: Stream-Reach vector was created using QSWAT 1.5.1 Data sources and data processing Currently not documented 1.6 Point sources locations BuildR can automatically add point source data if it is in the correct format and location. The requirements are as follows: Same folder as point source location layer (shapefile) And the files must be named as &lt;name&gt;_&lt;interval&gt;.csv, where &lt;name&gt; must be the name of a point int the vector layer and &lt;interval&gt; must be one of const, yr, mon, or day depending on the time intervals in the input data. This second point makes little sense but we will try out best. 1.6.1 Point source source data ps &lt;- readr::read_csv(&quot;model_data/input/pointsource/pointsourcedata.csv&quot;, show_col_types = F) # Select the columns we need ps2 &lt;- ps %&gt;% dplyr::select(ID, x, y, tot_P_kg_a) We have a problem. SWAT needs these data in terms of KG / m3 of outflow. We do not have outflow and just have total values. Therefore we need some sort of average outflow to divide our yearly total with. ps2$flo = 1 # TODO # Add columns for data we might add down the line ps2$sed = NA ps2$orgn = NA ps2$sedp = ps2$tot_P_kg_a ps2$no3 = NA ps2$solp = NA ps2$chla = NA ps2$nh3 = NA ps2$no2 = NA ps2$cbod = NA ps2$dox = NA ps2$sand = NA ps2$silt = NA ps2$clay = NA ps2$sag = NA ps2$lag = NA ps2$gravel = NA ps2$tmp = NA # drop the original P column ps2 &lt;- ps2 %&gt;% dplyr::select(!tot_P_kg_a) Our IDs are not unique ps2$ID %&gt;% length() ## [1] 5459 Why? We do not know. No metadata was provided with the file. ps2$ID %&gt;% unique() %&gt;% length() ## [1] 1362 However every coordinate pair is unique paste(ps2$x, ps2$y) %&gt;% unique() %&gt;% length() ## [1] 5429 We will need new IDs, but only for the ones in the catchment: # creating a shape file from the coordinates (UTM32N) and transforming to UTM33N point_source_shape &lt;- ps2 %&gt;% sf::st_as_sf( coords = c(&quot;y&quot;, &quot;x&quot;) ) %&gt;% sf::st_set_crs(25832) #%&gt;% # UTM 32N #sf::st_transform(25833) # UTM 33N # Cropping to DEM extent basin_shp &lt;- read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;)# %&gt;% sf::st_transform(25833) point_source_shape_crop &lt;- point_source_shape %&gt;% sf::st_intersection(., basin_shp) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries point_source_shape_crop$name &lt;- paste0(&quot;pssite_&quot;, seq_along(point_source_shape_crop$ID)) # Write the shape file sf::write_sf(obj = point_source_shape_crop, dsn = &quot;model_data/input/pointsource/cs10-ps-data.shp&quot;) ## Warning in CPL_write_ogr(obj, dsn, layer, driver, ## as.character(dataset_options), : GDAL Message 6: Normalized/laundered field ## name: &#39;Id&#39; to &#39;Id_1&#39; 1.6.2 Point source data files We need to create .csv files in the same directory which contain the point source values. psdata &lt;- point_source_shape_crop %&gt;% st_drop_geometry() %&gt;% dplyr::select(name, flo, sedp) for (i in seq_along(psdata$name)) { line &lt;- psdata[i,] %&gt;% select(-name) file_path = paste0(&quot;model_data/input/pointsource/&quot;,psdata$name[i],&quot;_const.csv&quot;) write.table(x = line, file_path, row.names = F, col.names = T, quote = F, sep = &quot;,&quot;) } 1.7 Aquifer Objects Aquifers are not an important part of the OPTAIN setup, no work needs to be done here: Although SWAT+ would allow the implementation of multiple aquifers in a model setup, the current version of SWATbuildR only adds a single aquifer for the entire catchment to the model setup. All land and water objects will be linked to this single aquifer. As the sizes of the OPTAIN study sites range between approximately 20 to 250 km2 the use of a single aquifer can be justified. References "],["model-setup-with-swatbuildr.html", "Section 2 Model setup with SWATBuildR 2.1 BuildR Setup 2.2 Processing input data 2.3 Calculating Contiguous Object Connectivity 2.4 Generate SWAT+ input", " Section 2 Model setup with SWATBuildR # run the buildR? run_buildr = FALSE if (run_buildr) { print(paste(&quot;Code executed @&quot;, Sys.time())) } else{ datemod &lt;- file.info(&quot;model_data/cs10_setup/optain-cs10/optain-cs10.sqlite&quot;) print(paste(&quot;BuildR last run @&quot;, datemod$mtime)) } ## [1] &quot;BuildR last run @ 2023-08-31 11:02:01.84211&quot; The OPTAIN project is using the COCOA approach (Schürz et al. 2022) and as such, needs to use the SWATBuildR package to build the SWAT+ model setup and calculate the connectivity between HRUs in the catchment. This chapter covers this process. 2.1 BuildR Setup We will need the following packages for this chapter: # not loading most packages because they interfere with the `install_load` # functions of BuildR. this means I need to prefix everything with :: until # Christoph turns BuildR into a proper package with proper scoping. # require(dplyr) # require(ggplot2) # require(mapview) # require(sf) # require(raster) # require(terra) # require(raster) # Common ggplot theme for simple features sf_theme &lt;- ggplot2::theme(axis.text.x=ggplot2::element_blank(), axis.ticks.x=ggplot2::element_blank(), axis.text.y=ggplot2::element_blank(), axis.ticks.y=ggplot2::element_blank() ) We will define the location of our project here, and give it a name: project_path &lt;- &#39;model_data/cs10_setup&#39; project_name &lt;- &#39;optain-cs10&#39; project_layer &lt;- TRUE # load the BuildR source(&#39;model_data/code/swat_buildR/init.R&#39;) # Note the many &quot;maskings&quot; occurring here, dangerous! 2.2 Processing input data SWATBuildR reads input data, performs some checks on it, and saves it in a compatible format for subsequent calculations. Little documentation exists on the creation of these data sets. IF you have any information to add, feel free to do so! 2.2.1 High-Resolution Digital Elevation Model (DEM) The high-resolution DEM is the basis for calculation of water connectivity, among other things. Most of the documentation of the creation of the DEM for CS10 has been lost to the sands of time. All we know is that it is located here: dem_path &lt;- &quot;model_data/input/elevation/dtm3_ns_v5.tif&quot; 2.2.2 Processing the Basin Boundary The basin boundary has presumably been created using the defined outlet point of the catchment and the DEM. No more is currently known about this file other than that it is located here: bound_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; bound_sf &lt;- sf::read_sf(bound_path) basin &lt;- ggplot2::ggplot(bound_sf) + ggplot2::geom_sf() + sf_theme print(basin) Figure 2.1: CS10 Basin, caclulated from the DEM. The following code and commentary is from BuildR Script version 1.5.14, written by Christoph Schuerz. BuildR recommends all layers to be in the same CRS, if we set project_layer to FALSE, it will throw an error when this is not the case: The input layers might be in different coordinate reference systems (CRS). It is recommended to project all layers to the same CRS and check them before using them as model inputs. The model setup process checks if the layer CRSs differ from the one of the basin boundary. By setting ‘proj_layer &lt;- TRUE’ the layer is projected if the CRS is different. If FALSE different CRS trigger an error. project_layer &lt;- TRUE We read in and check the basin boundary and run some checks bound &lt;- read_sf(bound_path) %&gt;% select() set_proj_crs(bound, data_path) check_polygon_topology(layer = bound, data_path = data_path, label = &#39;basin&#39;, n_feat = 1, checks = c(F,T,T,T,F,F,F,F)) 2.2.3 Processing the Land layer Our land layer is located here: land_path &lt;- &quot;model_data/input/land/CS10_LU_V2.shp&quot; Documentation on its creation does not exist. lu_shp &lt;- sf::read_sf(land_path) lu_map &lt;- ggplot2::ggplot(lu_shp) + ggplot2::geom_sf(ggplot2::aes(fill = type)) + sf_theme + ggplot2::theme(legend.position = &quot;none&quot;) print(lu_map) Figure 2.2: Land use map of CS10 by Farm ID We had an issue with the classification of the land uses. For the OPTAIN project, all agricultural fields must have a unique ID, and our land uses only had the ID of the given farm KGB (which had many different fields). To remedy this, new IDs were generated with the format a_###f_# where a_ represents the farm, and f_ represents the respective field of that farm. The farm names needed to be shortened because the SWAT+ model often cannot handle long ID names (longer than 16 characters) Note, the potential measures polygons were not counted as individual fields, which is why SP_ID does not match up with field_ID – This is by design. readr::read_csv(&quot;model_data/input/crop/a_f_id.csv&quot;, show_col_types = F) %&gt;% head() ## # A tibble: 6 × 4 ## KGB type_fr sp_id field_ID ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 213/8/1 a_084 1 a_084f_1 ## 2 213/7/6 a_083 2 a_083f_1 ## 3 213/7/6 a_083 3 a_083f_2 ## 4 213/7/4 a_082 4 a_082f_1 ## 5 213/65/1 a_081 5 a_081f_1 ## 6 213/65/1 a_081 6 a_081f_2 This was done in a simple QGIS workflow of dissolving by farm, splitting from single part to multipart, and then adding an iterating ID per farm field. This workflow could be replicated in R, and then shown here. It is under consideration… BuildR will now run some checks on our land layer. land &lt;- read_sf(land_path) %&gt;% check_layer_attributes(., type_to_lower = FALSE) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;land&#39;, type = &#39;vector&#39;) check_polygon_topology(layer = land, data_path = data_path, label = &#39;land&#39;, area_fct = 0.00, cvrg_frc = 99.9, checks = c(T,F,T,T,T,T,T,T)) BuildR splits the land layer into HRU (land) and reservoir (water) objects split_land_layer(data_path) 2.2.4 Processing the Channels No documentation exists on the creation of the channels layer, all we know is that it is located here: channel_path &lt;- &#39;model_data/input/line/cs10_channels_utm33n.shp&#39; channels &lt;- read_sf(channel_path) %&gt;% dplyr::select(&quot;type&quot;) channel_map &lt;- ggplot2::ggplot() + ggplot2::geom_sf(data = bound_sf) + ggplot2::geom_sf(data = channels, mapping = ggplot2::aes(color = type)) + sf_theme channel_map Figure 2.3: CS10 Channels including surface and subsurface channels BuildR runs some checks: channel &lt;- read_sf(channel_path) %&gt;% check_layer_attributes(., type_to_lower = TRUE) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;channel&#39;, type = &#39;vector&#39;) check_line_topology(layer = channel, data_path = data_path, label = &#39;channel&#39;, length_fct = 0, can_cross = FALSE) BuildR then checks the connectivity between the channels and reservoirs. For this we need to define our id_cha_out and id_res_out. “Variable id_cha_out sets the outlet point of the catchment. Either define a channel OR a reservoir as the final outlet. If channel then assign id_cha_out with the respective id from the channel layer. If reservoir then assign the respective id from the land layer to id_res_out, otherwise leave as NULL” id_cha_out &lt;- 37 id_res_out &lt;- NULL Running connectivity checks between channels and reservoirs: check_cha_res_connectivity(data_path, id_cha_out, id_res_out) Checking if any defined channel ids for drainage from land objects do not exist check_land_drain_ids(data_path) 2.2.5 Processing the DEM BuildR loads and checks the DEM, and saves it. dem &lt;- rast(dem_path) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;dem&#39;, type = &#39;raster&#39;) check_raster_coverage(rst = dem, vct_layer = &#39;land&#39;, data_path = data_path, label = &#39;dem&#39;, cov_frc = 0.95) save_dem_slope_raster(dem = dem, data_path = data_path) Here is the result: map &lt;- raster::raster(&quot;model_data/cs10_setup/optain-cs10/data/raster/dem.tif&quot;) raster::plot(map) Figure 2.4: CS10 DEM cropped to basin 2.2.6 Processing Soil Data Our soil map is located here: soil_layer_path &lt;- &#39;model_data/input/soil/soil_raster.tif&#39; No documentation exists on its creation. map &lt;- raster::raster(soil_layer_path) raster::plot(map) Figure 2.5: CS10 Soil map The soil data and look-up path are located here: soil_lookup_path &lt;- &#39;model_data/input/soil/soil_lookup_26.csv&#39; soil_data_path &lt;- &#39;model_data/input/soil/usersoil_26_v2.csv&#39; Not much documentation exists here either. Some can be found in the excel sheet: ~/model_data/input/soil/swatsoil2.xlsx BuildR reads in the soil data, performs checks, processes, and saves. soil &lt;- rast(soil_layer_path) %&gt;% check_project_crs(layer = ., data_path = data_path, proj_layer = project_layer, label = &#39;soil&#39;, type = &#39;raster&#39;) check_raster_coverage(rst = soil, vct_layer = &#39;hru&#39;, data_path = data_path, label = &#39;soil&#39;, cov_frc = 0.75) save_soil_raster(soil, data_path) BuildR then generates a table with aggregated elevation, slope, soil for HRU units. aggregate_hru_dem_soil(data_path) Read and prepare the soil input tables and a soil/hru id table and write them into data_path/tables.sqlite build_soil_data(soil_lookup_path, soil_data_path, data_path) 2.3 Calculating Contiguous Object Connectivity SWATBuildR follows COCOA. This section contains the calculations. You can read more about it in the protocol. 2.3.1 Calculating land unit connectivity Preparing raster layers based on the DEM and the surface channel objects that will be used in the calculation of the land object connectivity. prepare_terrain_land(data_path) The connection of each land object to neighboring land and water objects is calculated based on the flow accumulation and the D8 flow pointer along the object edge calculate_land_connectivity(data_path) 2.3.1.1 Eliminate land object connections with small flow fractions: For each land object the flow fractions are compared to connection with the largest flow fraction of that land object. Connections are removed if their fraction is smaller than frc_thres relative to the largest one. This is necessary to: Simplify the connectivity network To reduce the risk of circuit routing between land objects. Circuit routing will be checked. If an error due to circuit routing is triggered, then ‘frc_thres’ must be increased to remove connectivities that may cause this issue. frc_thres &lt;- 0.3 The remaining land object connections are analyzed for infinite loop routing. For each land unit the connections are propagated and checked if the end up again in the same unit. # reduce_land_connections(data_path, frc_thres) %&gt;% # check_infinite_loops(., data_path, &#39;Land&#39;) # Analyzing land objects for infinite loop routing: Completed 6185 Land objects # in 10M 44S # # ✘ 150 Land objects identified where water is routed in loops. # # You can resolve this issue in the following ways: # # - Use the layer &#39;land_infinite_loops.gpkg&#39; that was written to # model_data/cs10_setup/optain-cs10/data/vector to identify land polygons that # cause the issue and split them to break the loops This would require to # restart the entire model setup procedure! # # - Increase the value of &#39;frc_thres&#39;. # This reduces the number of connections of each land unit (maybe undesired!) # and can remove the connections that route the water in loops. # # - Continue with the model setup (only recommended for small number of identified units!). # The function &#39;resolve_loop_issues()&#39; will then eliminate a certain number of # connections. What do we do here ? – I think in the past we just removed all the loops, but it says here it is not recommended when there are a lot of them. Should we increase the threshold? Should we try to break up the loops? You can see the problem polygons on the map below: land_inifnite_loops_shp &lt;- sf::read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land_infinite_loops.gpkg&quot;) mapview::mapview(land_inifnite_loops_shp) To me it seems to complicated to try to break up the loops, maybe increase the threshold a bit and see what happens? and delete the rest? Increasing threshold and re-running leads to this: frc_thres &lt;- 0.1 # loops ( mins) frc_thres &lt;- 0.2 # 238 loops (30 mins) frc_thres &lt;- 0.3 # 150 loops (12 mins) frc_thres &lt;- 0.4 # 115 loops ( mins) frc_thres &lt;- 0.5 # 94 loops ( mins) frc_thres &lt;- 0.6 # 57 loops ( mins) We have decided that at a threshold of 0.3 and 150, we are within the “not that many” loops territory, considering out of 6000+ HRUs, it is only around 2%. Therefore we will let BuildR continue and remove them. This has been discussed in Issue #55. frc_thres &lt;- 0.3 # 150 loops reduce_land_connections(data_path, frc_thres) %&gt;% check_infinite_loops(., data_path, &#39;Land&#39;) 2.3.1.2 Resolve infinite loops If infinite loops were identified this routine tries to resolve the issues by selectively removing connections between land units in order to get rid of all infinite loops. resolve_loop_issues(data_path) 2.3.2 Calculating channel/reservoir connectivity 2.3.2.1 Calculating the water object connectivity the function returns the cha and res con_out tables in SWAT+ database format and writes them into data_path/tables.sqlite build_water_object_connectivity(data_path) 2.3.3 Checking the water objects for infinite loops From the cha_res_con_out tables id_from/id_to links are generated and checked for infinite loop routing. prepare_water_links(data_path) %&gt;% check_infinite_loops(., data_path, &#39;Water&#39;, Inf) 2.3.4 Terrain properties Calculate terrain properties such as elevation, slope, catchment area, channel width/depth for channel and reservoir objects and write them into data_path/tables.sqlite prepare_terrain_water(data_path) 2.4 Generate SWAT+ input 2.4.1 Generate land object SWAT+ input tables Build the landuse.lum and a landuse/hru id table and write them into data_path/tables.sqlite build_landuse(data_path) Build the HRU SWAT+ input files and write them into data_path/tables.sqlite build_hru_input(data_path) Add wetlands to the HRUs and build the wetland input files and write them into data_path/tables.sqlite We only use the wetf land use . wetland_landuse &lt;- c(&#39;wehb&#39;, &#39;wetf&#39;, &#39;wetl&#39;, &#39;wetn&#39;) add_wetlands(data_path, wetland_landuse) 2.4.2 Generate water object SWAT+ input tables Build the SWAT+ cha input files and write them into data_path/tables.sqlite build_cha_input(data_path) Build the SWAT+ res input files and write them into data_path/tables.sqlite build_res_input(data_path) Build SWAT+ routing unit con_out based on ‘land_connect_fraction’. build_rout_con_out(data_path) Build the SWAT+ rout_unit input files and write them into data_path/tables.sqlite build_rout_input(data_path) Build the SWAT+ LSU input files and write them into data_path/tables.sqlite build_ls_unit_input(data_path) 2.4.3 Build aquifer input Build the SWAT+ aquifer input files for a single aquifer for the entire catchment. The connectivity to the channels with geomorphic flow must be added after writing the txt input files. This is not implemented in the script yet. build_single_aquifer_files(data_path) 2.4.4 Add point source inputs Point sources have been prepared in Section REF The point source locations are provided with a point vector layer in the path ‘point_path’. point_path &lt;- &#39;model_data/input/pointsource/cs10-ps-data.shp&#39; Maximum distance of a point source to a channel or a reservoir to be included as a point source object (recall) in the model setup: max_point_dist &lt;- 500 # meters Adding the point sources with BuildR: add_point_sources(point_path, data_path, max_point_dist) 2.4.5 Create SWAT+ sqlite database 2.4.5.1 Write the SWAT+Editor project database The database will be located the ‘project_path’. stat &lt;- file.remove(&quot;model_data/cs10_setup/optain-cs10/optain-cs10.sqlite&quot;) if(stat == FALSE){stop(&quot;could not delete database&quot;)} create_swatplus_database(project_path, project_name) References "],["climate-inputs-and-weather-generator.html", "Section 3 Climate inputs and weather generator 3.1 Preparation and loading data 3.2 Creating the weather generator 3.3 Adding weather data to SWAT+ project 3.4 Adding Atmospheric deposition", " Section 3 Climate inputs and weather generator run_this_chapter = FALSE 3.1 Preparation and loading data To add weather and climate data to our SWAT project, we will use svatools (Svajunas, Rets, and Strauch 2023) 3.1.1 Required packages require(euptf2) # devtools::install_github(&quot;tkdweber/euptf2&quot;) require(svatools) # devtools::install_github(&quot;biopsichas/svatools&quot;) require(readr) require(readxl) require(sf) require(mapview) require(ggplot2) require(tidyr) 3.1.2 Load in the file(s) and load svatools template First we load in the template and fill it in with our values and rename it to “cs10_weather_data.xlsx”. This is not done within R. No detailed documentation exists on the source of this data (yet). There has been some discussion in issue #48 #temp_path &lt;- system.file(&quot;extdata&quot;, &quot;weather_data.xlsx&quot;, package = &quot;svatools&quot;) # /// fill out this template and save &quot;cs10_weather_data.xlsx&quot; /// We are using the following projection for this project: # TODO update this when the source files are in UTM33N epgs_code = 25832 Now we can load it in with Svatools. met_lst &lt;- load_template(template_path = &quot;model_data/input/met/cs10_weather_data.xlsx&quot;, epgs_code) ## [1] &quot;Loading data from template.&quot; ## [1] &quot;Reading station ID1 data.&quot; ## [1] &quot;Loading of data is finished.&quot; 3.1.3 Proof the station plot_weather(met_lst, &quot;PCP&quot;, &quot;year&quot;, &quot;sum&quot;) Checking the location of the station: basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; basin &lt;- st_transform(st_read(basin_path), epgs_code) %&gt;% mutate(NAME = &quot;Basin&quot;) ## Reading layer `cs10_basin&#39; from data source ## `T:\\Aktive\\DMN\\2111\\52127_OPTAIN\\WP4\\GIT\\gitlab_mo\\swat-cs10\\model_data\\input\\shape\\cs10_basin.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1 feature and 1 field ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 603439.4 ymin: 6607792 xmax: 610984.4 ymax: 6622017 ## Projected CRS: ETRS89 / UTM zone 32N stations &lt;- st_transform(met_lst$stations, epgs_code) mapview(stations) + mapview(basin) 3.2 Creating the weather generator We only have one weather station, which means only one weather generator. wgn &lt;- prepare_wgn(met_lst, TMP_MAX = met_lst$data$ID1$TMP_MAX, TMP_MIN = met_lst$data$ID1$TMP_MIN, PCP = met_lst$data$ID1$PCP, RELHUM = met_lst$data$ID1$RELHUM, WNDSPD = met_lst$data$ID1$WNDSPD, MAXHHR = met_lst$data$ID1$MAXHHR, SLR = met_lst$data$ID1$SLR) ## [1] &quot;Coordinate system checked and transformed to EPSG:4326.&quot; ## [1] &quot;Working on station ID1:WS_AAS&quot; write.csv(wgn$wgn_st, &quot;model_data/input/met/as_wgn_st.csv&quot;, row.names = FALSE, quote = FALSE) write.csv(wgn$wgn_data, &quot;model_data/input/met/as_wgn_data.csv&quot;, row.names = FALSE, quote = FALSE) 3.3 Adding weather data to SWAT+ project This adds the weather generator and weather stations to our SWAT+ project db_path &lt;- &quot;model_data/cs10_setup/optain-cs10/optain-cs10.sqlite&quot; add_weather(db_path, met_lst, wgn) 3.4 Adding Atmospheric deposition Instructions and documentation can be found here Getting the data: basin_path &lt;- &quot;model_data/input/shape/cs10_basin.shp&quot; df &lt;- get_atmo_dep( basin_path, start_year = 2010, end_year = 2020, t_ext = &quot;year&quot; ) readr::write_csv(df, file = &quot;model_data/input/met/atmodep.csv&quot;) A plot of the results: df &lt;- readr::read_csv(&quot;model_data/input/met/atmodep.csv&quot;, show_col_types = F) ggplot(pivot_longer(df, !DATE, names_to = &quot;par&quot;, values_to = &quot;values&quot;), aes(x = DATE, y = values))+ geom_line()+ facet_wrap(~par, scales = &quot;free_y&quot;)+ theme_bw() Figure 3.1: Atmospheric Deposition data grabbed by svatools Adding the data to the SQLITE db_path &lt;- &quot;model_data/cs10_setup/optain-cs10/optain-cs10.sqlite&quot; add_atmo_dep(df, db_path) Big thank you to Svajunas for his svatools package, making our lives much easier! References "],["writing-swat-input-files.html", "Section 4 Writing SWAT+ Input Files 4.1 Writing Input Files via Python 4.2 Writing the input files with the SWAT Editor", " Section 4 Writing SWAT+ Input Files The SWATBuildR cannot write the actual text files for our model setup, we have to use the SWAT+ Editor for this. UPDATE: A tool has been created (author unknown) which writes our database for us in code form using python. The old section of doing it manually has been left in the doc (REF). 4.1 Writing Input Files via Python run_this_chapter = FALSE The files which execute this operation can be found here: write_db_py &lt;- &quot;model_data/code/write_swat_db.py&quot; write_db_r &lt;- &quot;model_data/code/write_swat_db.r&quot; As you can see, it uses python. The python environment is archived here: py_env &lt;- &quot;model_data/code/env/&quot; We can execute our python script it like so: source(&quot;model_data/code/write_swat_db.R&quot;) IMPORTANT: now that the input files have been written, we will ONLY modify these parameters with R and never edit parameters in the Editor. The reason for this is because changing the values of the text files will not change the values of the sqlite database. If the input files are re-written from, the database, our changes to the text files will be overwritten. Let us test if our model works 4.1.1 SWAT+ Test Run We will copy in our SWAT+ executable, our input files, and our weather files into a folder and run SWAT+ as a test. For some reason, when writing the SWAT+ input files, it does not write the weather station data. I am not sure if this is intended or not. the SWAT+ model still runs fine, however maybe it is using the weather generator? The meteo input files are located in the same directory as the sqlite, probably because svatools put them there? #TODO Our current SWAT+ version is rev60.5.4_64rel.exe but this is subject to change Make a run directory and enable code running. dir.create(&quot;model_data/cs10_setup/run_swat&quot;, showWarnings = F) Copy all required files into this directory: sta_files &lt;- list.files(&quot;model_data/cs10_setup/optain-cs10/&quot;, pattern = &quot;sta_&quot;, full.names = T) cli_files &lt;- list.files(&quot;model_data/cs10_setup/optain-cs10/&quot;, pattern = &quot;.cli&quot;, full.names = T) input_files &lt;- list.files(&quot;model_data/cs10_setup/swat_input/&quot;, full.names = T) path_to_swat &lt;- &quot;model_data/cs10_setup/rev60.5.4_64rel.exe&quot; source_files &lt;- c(sta_files, cli_files, input_files, path_to_swat) status &lt;- file.copy(from = source_files, to = &quot;model_data/cs10_setup/run_swat/&quot;, overwrite = T) Lets run some checks on these files if(any(status == FALSE)){ warning(&quot;Some files were not copied:&quot;) print(source_files[which(status==FALSE)]) } print( paste( length(which(status)), &quot;of&quot;, length(source_files), &quot;files were copied into the run folder, amounting to a size of &quot;, round(sum(file.info(source_files)$size * 1e-6), 2), &quot;megabytes&quot; ) ) Now lets run the model to make sure it works # update time sim time_sim &lt;- readLines(&quot;model_data/cs10_setup/run_swat/time.sim&quot;) time_sim[3] &lt;- &quot; 0 2011 0 2011 0 &quot; writeLines(text = time_sim, con = &quot;model_data/cs10_setup/run_swat/time.sim&quot;) msg &lt;- processx::run(command = &quot;rev60.5.4_64rel.exe&quot;, wd = &quot;model_data/cs10_setup/run_swat/&quot;) simout &lt;- readLines(&quot;model_data/cs10_setup/run_swat/simulation.out&quot;) cat(tail(simout), sep = &quot;\\n&quot;) It will be useful to test SWAT+ along our journey, so we will reuse this code in the function test_swat() source(&quot;model_data/code/test_swat.R&quot;) We can now continue with our final buildR step 4.1.2 Link aquifers and channels with geomorphic flow A SWATBuildR model setup only has one single aquifer (in its current version). This aquifer is linked with all channels through a channel-aquifer-link file (aqu_cha.lin) in order to maintain recharge from the aquifer into the channels using the geomorphic flow option of SWAT+. The required input file cannot be written with the SWAT+Editor. Therefore it has to be generated in a step after writing the model text input files with the SWAT+Editor. Path of the TxtInOut folder (project folder where the SWAT+ text files are written with the SWAT+Editor) txt_path &lt;- &#39;model_data/cs10_setup/swat_input/&#39; Linking the aquifer to the channels project_path &lt;- &#39;model_data/cs10_setup&#39; project_name &lt;- &#39;optain-cs10&#39; source(&#39;model_data/code/swat_buildR/init.R&#39;) link_aquifer_channels(txt_path) This created the file aqu_cha.lin and changed file.cio to point to it (row 16, column 3) Let us test to see if SWAT+ still runs simout &lt;- test_swat() cat(tail(simout), sep = &quot;\\n&quot;) Success. # remove the swat+ run unlink(&quot;model_data/cs10_setup/run_swat/&quot;, recursive = T) 4.2 Writing the input files with the SWAT Editor If for some reason you cannot or do not want to write the input files via code, a step by step guide is below which shows you how to do it with the editor. 4.2.1 Loading the project in SWAT+ Editor Open a project Path to the project sqlite 4.2.2 Writing input files Project Information. The weather generator has been complete by svatools in Section 3 After clicking on the Run Model / Save Scenario button, you arrive at the “Confirm Simulation Settings” page. Here you need to choose where to write your input files. We have chosen cs10_setup/swat_input. We have also changed our simulation time period, but this is not required. Make sure to un-check “Run SWAT” and “Analyze output for visualization” “Save settings &amp; Run Selected” Confirm Simulation Settings. Make sure to follow the instructions carefully here. Your SWAT+ input files will be generated, and you will be prompted to Save Scenario. This is not recommended, because in our testing, the scenario saving would recursively generate new scenarios within the same folder, until all file space had been exhausted on the drive. This extremely deep folder brings windows to its knees, even when trying to delete it using explorer (Use Powershell instead). Option to save scenario. Not Recommended. 4.2.3 Post-write Changes It is important to perform the final BuildR function as in Section REF. "],["crop-map-generation.html", "Section 5 Crop Map Generation 5.1 Calculating Field Area 5.2 Generating the Crop Rotation 5.3 Merging crop rotation with BuildR output", " Section 5 Crop Map Generation As OPTAIN takes into account the individual field, we need to know what is growing on which field and when. Unfortunately our data foundation for this task is quite lackluster, but we will try out best to do so. in the following chapter. require(sf) require(dplyr) require(readr) require(stringr) require(readxl) require(reshape2) require(tibble) require(DT) require(ggplot2) require(gifski) sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) 5.1 Calculating Field Area We have data based on what area certain crops have per farm. In order to relate this to our spatial land use map, we require the area of both the farms, and the fields within these farms. The basis for these calculations comes from the BuildR output from Section lu_sf &lt;- read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land.shp&quot;) lu_map &lt;- ggplot() + geom_sf(lu_sf, mapping = aes(fill = type)) + theme(legend.position = &quot;left&quot;) + sf_theme + theme(legend.position = &quot;none&quot;) print(lu_map) Figure 5.1: Land use map for CS10, colored by type. 5.1.1 Calculate Field Area and Assign IDs This stage has been completed in QGIS. An R-implementation is being considered. farm_area_sf &lt;- read_sf(&quot;model_data/input/crop/buildr_output_dissolved_into_farms.shp&quot;) field_lu_plot &lt;- ggplot() + geom_sf(farm_area_sf, mapping = aes(fill = farm_id)) + theme(legend.position = &quot;none&quot;) + sf_theme print(field_lu_plot) Figure 5.2: Farms in the CS10 catchment, colored by ID 5.1.2 Calculate Field area and assign IDs This stage has been completed in QGIS. An R-implementation is being considered. 5.2 Generating the Crop Rotation The following algorithm takes information from reporting farm, and their respective area in the SWAT+ setup, and combines these data sets to generate a plausible crop rotation. Note, pasture refers to meadow (TODO: fix!) crop_data &lt;- read_excel(&quot;model_data/input/crop/crop_rotation_source_data.xlsx&quot;) datatable(crop_data) 5.2.1 Tidy up the source data: We are converting the source data to tidy format in the following code snippet crop_names &lt;- melt( crop_data, id.vars = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;, &quot;pastrure_area_daa&quot;), measure.vars = c(&quot;crop1&quot;, &quot;crop2&quot;, &quot;crop3&quot;), variable.name = &quot;crop_status&quot;, value.name = &quot;crop&quot; ) %&gt;% tibble() crop_area &lt;- melt( crop_data, id.vars = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;, &quot;pastrure_area_daa&quot;), measure.vars = c(&quot;percent_crop1&quot;, &quot;percent_crop2&quot;, &quot;percent_crop3&quot;), variable.name = &quot;crop_status&quot;, value.name = &quot;area&quot; ) %&gt;% tibble() crop_area$crop_status &lt;- crop_area$crop_status %&gt;% str_remove(&quot;percent_&quot;) # unstable, TODO fix! colnames(crop_area)[6] &lt;- &quot;crop_area_percent&quot; crop_tidy &lt;- left_join(crop_names, crop_area, by = c(&quot;farm_id&quot;, &quot;year&quot;, &quot;area_daa&quot;,&quot;pastrure_area_daa&quot;, &quot;crop_status&quot;)) %&gt;% dplyr::select(-crop_status) # unstable, TODO fix! colnames(crop_tidy)[3] &lt;- &quot;farm_area_daa&quot; crop_tidy$crop_area_percent &lt;- (crop_tidy$crop_area_percent/100) %&gt;% round(2) crop_tidy &lt;- crop_tidy %&gt;% filter(crop %&gt;% is.na() == FALSE) datatable(crop_tidy) 5.2.2 Tidy up the BuildR output The maps shown in Section 5.1 were exported to CSV in QGIS. An R-implementation might be written in here sometime (#TODO). This CSV data needs to be re-formatted to be tidy as well. fields &lt;- read_csv( &quot;model_data/input/crop/buildr_landuse_dissolved_into_fields.csv&quot;, show_col_types = F ) %&gt;% dplyr::select(type, farm_id, field_area_daa) farms &lt;- read_csv( &quot;model_data/input/crop/buildr_landuse_dissolved_into_farms.csv&quot;, show_col_types = F ) %&gt;% dplyr::select(type, farm_id , farm_area_) # fix farm_area_ TODO # unstable, fix! TODO colnames(farms)[3] &lt;- &quot;farm_area_daa&quot; farms$farm_area_daa &lt;- farms$farm_area_daa %&gt;% round(2) fields$field_area_daa &lt;- fields$field_area_daa %&gt;% round(2) reported_data &lt;- left_join(fields, farms, by = &quot;farm_id&quot;) # unstable, fix! TODO colnames(reported_data)[1] &lt;- &quot;field_id&quot; reported_data &lt;- reported_data %&gt;% dplyr::select(field_id, farm_id, field_area_daa, farm_area_daa) datatable(reported_data) 5.2.3 Field classification We are now ready to perform our classification. We will store our results in this predefined dataframe: classed_fields &lt;- tibble( farm_id = NA, field_id = NA, farm_area_daa = NA, field_area_daa = NA, crop = NA, year = NA ) The following reporting farms will be used for the classification: farms &lt;- reported_data$farm_id %&gt;% unique() For the following years (AKA we have data for these years): years &lt;- c(2016, 2017, 2018, 2019) This for loop runs through all the fields and classifies them. The output will not be printed in this file, but will be saved in a log file, that you can dig out of the repository, located here: # TODO add the log file generation The following code is not executed: # For every year for(c_year in years){ # For every farm, for (farm in farms) { # Do the following: # Filter the source and buildR data to the given year and farm crop_filter &lt;- crop_tidy %&gt;% filter(year == c_year) %&gt;% filter(farm_id == farm) hru_filter &lt;- reported_data %&gt;% filter(farm_id == farm) # Behavior if the farm has no reporting data: if(crop_filter$farm_id %&gt;% length() == 0){ # Set all fields to winter wheat. farm_fields &lt;- hru_filter %&gt;% dplyr::select(farm_id, field_id, farm_area_daa, field_area_daa) %&gt;% arrange(desc(field_area_daa)) farm_fields$crop = &quot;wwht&quot; farm_fields$year = c_year # Add them to the results dataframe classed_fields &lt;- rbind(classed_fields, farm_fields) # and skipping to next farm next() } # Behavior, if there is reporting data on the farm&quot; # Find the area of the farm (all elements in vector are the same, so ok # to just grab the first) farm_area_hru &lt;- hru_filter$farm_area_daa[1] # Find the discrepency between &quot;SHOULD BE&quot; crop area and &quot;CURRENTLY IS&quot; # crop area. We want this as an absolute value. area_dis &lt;- (hru_filter$farm_area_daa[1]-crop_filter$farm_area_daa[1]) %&gt;% round(2) %&gt;% abs() # Some farms have no reported area. in this case we set it to some random # negative number. if(area_dis %&gt;% is.na()){area_dis = -999} # Extracting the area of the pasture and farm (all vectors same value, ok # to just take the first) past_area &lt;- crop_filter$pastrure_area_daa[1] farm_area &lt;- crop_filter$farm_area_daa[1] # Extract relevant fields farm_fields &lt;- hru_filter %&gt;% dplyr::select(farm_id, field_id, farm_area_daa, field_area_daa) %&gt;% # and sort by area arrange(desc(field_area_daa)) # pre-define crops column to be NA farm_fields$crop = NA # Bollean check to see if the field has been meadow in a previous year. If # it has, then we want to continue planting meadow on it (This was a # decision we made.) has_meadow &lt;- (classed_fields %&gt;% filter(field_id %in% farm_fields$field_id) %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_id) %&gt;% pull() %&gt;% length() &gt; 0) ### Determining how much land the crops should cover. # This if statement checks if is currently a pasture portion for the reporting # farm, or if there has been in the past. if((past_area %&gt;% is.na() == FALSE) | has_meadow) { # If it does, or did, then this special routine is enacted: # Determines the pasture percentage of the total farm past_percent &lt;- (past_area/farm_area) # If it cannot be calculated, then it is set to 0 if(past_percent %&gt;% is.na()){past_percent = 0} # The crop fractions provided to us in the source data did not account for # the pasture fraction. therefore we need to reduce the other crop # fractions by this amount. adjuster &lt;- past_percent / length(crop_filter$crop) crop_filter$crop_area_percent &lt;- crop_filter$crop_area_percent-adjuster # creating a new entry for pasture and adding it to the now updated crop\\ # fractions list past_row &lt;- tibble( farm_id = farm, year = c_year, farm_area_daa = crop_filter$farm_area_daa[1], pastrure_area_daa = past_area, crop = &quot;meadow&quot;, crop_area_percent = past_percent ) crop_filter &lt;- rbind(crop_filter, past_row) # Creating an updated &quot;SHOULD BE&quot; and &quot;CURRENTLY IS&quot; crop coverage # datafram crop_coverage &lt;- tibble( crop = crop_filter$crop, to_cover = (farm_area_hru*crop_filter$crop_area_percent)) # Setting the initial actual coverage to 0 (&quot;CURRENTLY IS&quot;) crop_coverage$actual &lt;- 0 # This extracts the fields that were previously pasture fields. previously_pasture &lt;- classed_fields %&gt;% filter(field_id %in% farm_fields$field_id) %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_id) %&gt;% pull() if (length(previously_pasture) &gt; 0) { # if some of the fields were previously pasture, they are set to pasture # again: farm_fields$crop[ which(farm_fields$field_id %in% previously_pasture)] = &quot;meadow&quot; # We save the amount of area the meadow crops cover, so that we can # adjust the other &quot;SHOULD BE&quot; fractions correctly. custom_actual &lt;- farm_fields %&gt;% filter(crop == &quot;meadow&quot;) %&gt;% dplyr::select(field_area_daa) %&gt;% pull() %&gt;% sum(na.rm = T) crop_coverage$actual[which(crop_coverage$crop==&quot;meadow&quot;)] = custom_actual } # Now the algorithm continues as normal. # This is the routine that is run, when no meadow is detected: }else{ # creating a tibble which tracks how much land the crops SHOULD cover crop_coverage &lt;- tibble( crop = crop_filter$crop, to_cover = (farm_area_hru * crop_filter$crop_area_percent) ) # pre define crop coverage to 0 (CURRENT) crop_coverage$actual &lt;- 0 } ### Crop classification # Now we know how much land the crops SHOULD cover, it is time to assign # the fields accordingly. We do this with a WHILE loop, which keeps going # until we have classified every field. while(any(farm_fields$crop %&gt;% is.na())){ # run through all the crops in the crop coverage list and determine or # update their current coverage for (c_crop in crop_coverage$crop) { # figure out which index we are in the list crop_index &lt;- which(c_crop == crop_coverage$crop) # Determine/UPDATE the current actual coverage of the crop (sum) crop_coverage$actual[crop_index] &lt;- farm_fields %&gt;% filter(crop == c_crop) %&gt;% dplyr::select(field_area_daa) %&gt;% sum() } # Calculate the difference between crop SHOULD BE coverage and ACTUAL # coverage crop_coverage$diff &lt;- crop_coverage$actual-crop_coverage$to_cover # determine the maximum difference. max_diff &lt;- crop_coverage$diff %&gt;% min(na.rm = T) # determine which crop has the maximum difference max_diff_crop &lt;- crop_coverage$crop[which(crop_coverage$diff == max_diff) %&gt;% min(na.rm = T)] # determine the biggest field left un-classified. biggest_na_field &lt;- farm_fields %&gt;% filter(crop %&gt;% is.na()) %&gt;% arrange(desc(field_area_daa)) %&gt;% nth(1) # Set the field which is still NA, and also the biggest to the crop with # the maximum difference (This code seems weird, and could be # improved?) farm_fields$crop[which((farm_fields$crop %&gt;% is.na() == TRUE) &amp; farm_fields$field_area_daa == biggest_na_field$field_area_daa )] &lt;- max_diff_crop } # After having classified the field, we update the actual coverage value # the respective crop that has been classified. for (c_crop in crop_coverage$crop) { crop_index &lt;- which(c_crop == crop_coverage$crop) crop_coverage$actual[crop_index] &lt;- farm_fields %&gt;% filter(crop == c_crop) %&gt;% dplyr::select(field_area_daa) %&gt;% sum() } # save the year of this crop assignment in the dataframe farm_fields$year = c_year # add the classification to the result dataframe classed_fields &lt;- rbind(classed_fields, farm_fields) # and move onto the next farm } # for every year } # Remove the first NA line classed_fields &lt;- classed_fields[-1, ] Now we can have a look at our results (not evaluated) datatable(classed_fields) We can now extract the crop rotation from this datatable. (not evaluated) # the format of our final dataframe final_df &lt;- tibble(field = NA, y_2016 = NA, y_2017 = NA, y_2018 = NA, y_2019 = NA) # fields we need to extract field_ids &lt;- classed_fields$field_id %&gt;% unique() We will do this with this for loop (note this is bad R-practice. Should be done with something like pivot_longer (TODO)) for(c_field_id in field_ids) { crop_rotation &lt;- tibble(field = c_field_id) crops &lt;- classed_fields %&gt;% filter(field_id == c_field_id) %&gt;% dplyr::select(crop) %&gt;% t() %&gt;% as_tibble(.name_repair = &quot;minimal&quot;) colnames(crops) &lt;- c(&quot;y_2016&quot;, &quot;y_2017&quot;, &quot;y_2018&quot;, &quot;y_2019&quot;) crop_rotation &lt;- cbind(crop_rotation, crops) %&gt;% as_tibble() final_df &lt;- rbind(final_df, crop_rotation) } # Remove the first NA line crop_rotation &lt;- final_df[-1,] # Save the crop rotation in CSV format write_csv(x = crop_rotation, file = &quot;model_data/input/crop/cs10_crop_rotation.csv&quot;) Here is a look at the crop rotation: lu_shp &lt;- &#39;model_data/input/crop/land_with_cr.shp&#39; lu_sf &lt;- read_sf(lu_shp) years &lt;- paste0(&quot;y_&quot;, 2016:2019) for (year in years) { index &lt;- which(colnames(lu_sf) == year) lu_sf_filt &lt;- lu_sf[c(index,length(lu_sf))] plot &lt;- ggplot(data = lu_sf_filt) + geom_sf(color = &quot;black&quot;, aes(fill = get(year)))+ guides(fill=guide_legend(title=year))+ ggtitle(&quot;CS10 Catchment&quot;, &quot;crop rotation&quot;)+ theme(legend.position=&quot;right&quot;)+ theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) print(plot) } Figure 5.3: Generated crop map for years 2016-2019 5.2.4 Extrapolating the Crop Rotation For OPTAIN, the crop rotation needs to span from 1988 to 2020. We currently have 2016 to 2019. crop_rotation &lt;- read_csv(&quot;model_data/input/crop/cs10_crop_rotation.csv&quot;, show_col_types = F) rotation &lt;- crop_rotation %&gt;% dplyr::select(-field) crop_rotation_extrapolated &lt;- cbind(crop_rotation, rotation, rotation, rotation, rotation, rotation, rotation, rotation, rotation$y_2016) # set the column names to be in the correct format colnames(crop_rotation_extrapolated) &lt;- c(&quot;field&quot;, paste0(&quot;y_&quot;, 1988:2020)) 5.3 Merging crop rotation with BuildR output The output of SWATbuildR “land.shp” needs to be connected to the newly generated crop map. lu &lt;- read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/land.shp&quot;) # temporary rename to field, for the left join # unstable, fix! TODO colnames(lu)[2] = &quot;field&quot; # join the crop rotation and land use layer by their field ID lu_cr &lt;- left_join(lu, crop_rotation_extrapolated, by = &quot;field&quot; ) # reset column name. # unstable, fix! TODO colnames(lu_cr)[2] = &quot;lu&quot; # Write the new shape file write_sf(lu_cr, &quot;model_data/input/crop/land_with_cr.shp&quot;) Related Issues Extrapolating the crop rotation #41 "],["landuse.lum-update.html", "Section 6 Landuse.lum update 6.1 Data preperation 6.2 Assigning landuse values 6.3 Writing Changes", " Section 6 Landuse.lum update This section covers the modifications made to the landuse file. For some reason it was written in a tutorial fashion, as if the reader were new to R. require(tidyverse) require(reshape2) require(sf) require(DT) require(dplyr) # require dplyr last to overwrite plyr count() require(ggplot2) ggplot theme(s): sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) 6.1 Data preperation We read in the land use file. We want to set skip = 1 to ignore the SWAT+editor text, and set header = T. We then convert it to a tibble format for better printing to console landuse_lum &lt;- read.table(&quot;model_data/cs10_setup/swat_input/landuse.lum&quot;, skip = 1, header = T) %&gt;% tibble::as_tibble() head(landuse_lum) ## # A tibble: 6 × 14 ## name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a_001f_1_… null nopl_co… null null null null null null mw24… ## 2 a_001f_2_… null nopl_co… null null null null null null mw24… ## 3 a_001f_3_… null nopl_co… null null null null null null mw24… ## 4 a_001f_4_… null nopl_co… null null null null null null mw24… ## 5 a_001f_5_… null nopl_co… null null null null null null mw24… ## 6 a_001f_6_… null nopl_co… null null null null null null mw24… ## # ℹ 4 more variables: sep &lt;chr&gt;, vfs &lt;chr&gt;, grww &lt;chr&gt;, bmp &lt;chr&gt; Our field_id for our cropland does not match name of landuse_lum so we need to parse it out, we can do this many ways, but a safe way is to split it by “_” and combine the first 3 splits with that same underscore: splitted &lt;- landuse_lum$name %&gt;% str_split(&quot;_&quot;) landuse_lum$field_id &lt;- paste(splitted %&gt;% map(1), splitted %&gt;% map(2), splitted %&gt;% map(3), sep = &quot;_&quot;) head(landuse_lum$field_id) ## [1] &quot;a_001f_1&quot; &quot;a_001f_2&quot; &quot;a_001f_3&quot; &quot;a_001f_4&quot; &quot;a_001f_5&quot; &quot;a_001f_6&quot; But wait, this does not work for our “non-fields”. So lets find out which ones they are, and set them to NA not_fields &lt;- which(!grepl(x=landuse_lum$field_id, &quot;a_&quot;)) landuse_lum$field_id[not_fields] &lt;- NA So, what non-fields do we have? landuse_lum$name[not_fields] ## [1] &quot;frst_lum&quot; &quot;past_lum&quot; &quot;rngb_lum&quot; &quot;urml_lum&quot; &quot;utrn_lum&quot; &quot;wetf_lum&quot; Good to know. We’ll keep that in mind. 6.2 Assigning landuse values 6.2.1 Setting cal_group There is no info on this column, so we are going to leave it as null. Dicussion in Issue #18 6.2.2 Setting plnt_com This step will be done by SWATFarmR in Section 11. Dicussed in Issue #15 6.2.3 Setting mgt This step will be done by SWATFarmR in Section 11. Discussed in Issue #14 6.2.4 Setting urb_ro We want to set the urb_ro column for all urban land uses (in our case this would be urml and utrn) to usgs_reg. This has been discussed in Issue #6 We can do this like so: landuse_lum$urb_ro[which(landuse_lum$name %in% c(&quot;urml_lum&quot;, &quot;utrn_lum&quot;))] &lt;- &quot;usgs_reg&quot; ## # A tibble: 2 × 15 ## name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 urml_lum null null null null null null usgs_reg null null ## 2 utrn_lum null null null null null null usgs_reg null null ## # ℹ 5 more variables: sep &lt;chr&gt;, vfs &lt;chr&gt;, grww &lt;chr&gt;, bmp &lt;chr&gt;, ## # field_id &lt;chr&gt; Very good. In our case this was only two – could be done by hand, but that will not be the case for all of our land uses. 6.2.5 Setting urban This one is easy, we set our urban column to an urban parameter set of the same name. The rest we leave as null This has been discussed in Issue #5 landuse_lum$urban[which(landuse_lum$name ==&quot;utrn_lum&quot;)] &lt;- &quot;utrn&quot; landuse_lum$urban[which(landuse_lum$name ==&quot;urml_lum&quot;)] &lt;- &quot;urml&quot; ## # A tibble: 1 × 15 ## name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 urml_lum null null null null null urml usgs_reg null null ## # ℹ 5 more variables: sep &lt;chr&gt;, vfs &lt;chr&gt;, grww &lt;chr&gt;, bmp &lt;chr&gt;, ## # field_id &lt;chr&gt; Lets make sure other land uses still have null: ## # A tibble: 1 × 15 ## name cal_group plnt_com mgt cn2 cons_prac urban urb_ro ov_mann tile ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 frst_lum null nopl_comm null null null null null null null ## # ℹ 5 more variables: sep &lt;chr&gt;, vfs &lt;chr&gt;, grww &lt;chr&gt;, bmp &lt;chr&gt;, ## # field_id &lt;chr&gt; Looks good. 6.2.6 Setting Manning’s n (ovn) This has been discussed in Issue #13 Lets get the easy ones out of the way landuse_lum$ov_mann[which(landuse_lum$name ==&quot;past_lum&quot;)] &lt;- &quot;densegrass&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;rngb_lum&quot;)] &lt;- &quot;rangeland_20cover&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;urml_lum&quot;)] &lt;- &quot;urban_rubble&quot; landuse_lum$ov_mann[which(landuse_lum$name ==&quot;urtn_lum&quot;)] &lt;- &quot;urban_asphalt&quot; Don’t fall asleep yet! For wetf we want forest_heavy but with a higher value. this means we need to add a new entry. And since we are doing this fancy Rmarkdown stuff, we’re going to do it with code! Lets jump in and get at this file. (REMEBER TO REMOVE THE TEMP) ovn_table_path &lt;- &quot;model_data/cs10_setup/swat_input/ovn_table.lum&quot; ovn_table_path_new &lt;- &quot;model_data/cs10_setup/swat_input_mod/ovn_table.lum&quot; ovn_table &lt;- readLines(ovn_table_path) Good, now where is this forest heavy entry? and what does the format look like? index &lt;- grepl(x=ovn_table, &quot;forest_heavy&quot;) %&gt;% which %&gt;% min() ovn_table[c(2, index)] %&gt;% print() ## [1] &quot;name ovn_mean ovn_min ovn_max description&quot; ## [2] &quot;forest_heavy 0.80000 0.70000 0.90000 Forest_heavy&quot; Now, lets make our own and add it in. But only if it doesn’t exist it (Like if the script has been run before…) if(grepl(x = ovn_table, &quot;forest_heavy_cs10&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;forest_heavy_cs10 0.90000 0.80000 0.95000 Forest_heavy_mod&quot; ovn_table &lt;- c(ovn_table, entry) } Did it work? Yes: ovn_table %&gt;% tail() ## [1] &quot;forest_med 0.60000 0.50000 0.70000 Forest_medimum_good&quot; ## [2] &quot;forest_heavy 0.80000 0.70000 0.90000 Forest_heavy&quot; ## [3] &quot;urban_asphalt 0.01100 0.01100 0.01100 Urban_asphalt&quot; ## [4] &quot;urban_concrete 0.01200 0.01200 0.01200 Urban_concrete&quot; ## [5] &quot;urban_rubble 0.02400 0.02400 0.02400 Urban_rubble&quot; ## [6] &quot;forest_heavy_cs10 0.90000 0.80000 0.95000 Forest_heavy_mod&quot; Now lets write this new table writeLines(ovn_table, con = ovn_table_path_new) And now we can enter our wetland class: landuse_lum$ov_mann[which(landuse_lum$name ==&quot;wetf_lum&quot;)] &lt;- &quot;forest_heavy_cs10&quot; For the fields we are going to need the crop rotation info which we created in section 5. crop_rotation &lt;- read_csv(&quot;model_data/input/crop/cs10_crop_rotation.csv&quot;, show_col_types = F) head(crop_rotation) ## # A tibble: 6 × 5 ## field y_2016 y_2017 y_2018 y_2019 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a_025f_1 wwht wwht wwht wwht ## 2 a_025f_3 wwht wwht wwht wwht ## 3 a_025f_4 wwht wwht wwht wwht ## 4 a_025f_2 wwht wwht wwht wwht ## 5 a_105f_1 wwht wwht wwht wwht ## 6 a_105f_4 wwht wwht wwht wwht We have decided to classify ovn based on the degree to which a crop rotation contains meadow, conventional crops (wwht, pota), and conservation crops (all others). To do this we need to analyze the crop rotation. Now it gets a little complicated, but trust me its not actually as bad as it looks. We need to count how many times certain crops show up in the crop rotation of a certain field. Lets do it for conventional crops first. Now, lets get into it conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() %&gt;% dplyr::rename(conv = n) head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field conv ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 What happened here? well we took our crop_rotation, and melted it by field using melt. This function converts the data into tidy format (Wickham 2014). This format makes it easier to apply the following operations on a field basis. What does this look like? crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% arrange(field) %&gt;% head() ## field variable value ## 1 a_001f_1 y_2016 wwht ## 2 a_001f_1 y_2017 wwht ## 3 a_001f_1 y_2018 wwht ## 4 a_001f_1 y_2019 wwht ## 5 a_001f_2 y_2016 wwht ## 6 a_001f_2 y_2017 wwht melt is a function from reshape2 which is why we required it. And what does that “.” mean in there, to the left of \"field\"?? That is a code word for the object to the left of the “pipe” (in this case crop_rotation). the pipe is “%&gt;%” and passes the object to the left of it, into the function to the right of it. look it up! When arranged by field, we can see that every field gets one entry per crop per year. This is a good format to count how many times we have a certain type of crop on a field. Next we group_by the field – this means all the following operations will be done on a field basis. Then we filter our value (which is the crop name). For conventional we needed to filter in any fields with the crop \"wwht\" or \"pota\". What does this look like? crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) ## # A tibble: 1,519 × 3 ## # Groups: field [415] ## field variable value ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 a_025f_1 y_2016 wwht ## 2 a_025f_3 y_2016 wwht ## 3 a_025f_4 y_2016 wwht ## 4 a_025f_2 y_2016 wwht ## 5 a_105f_1 y_2016 wwht ## 6 a_105f_4 y_2016 wwht ## 7 a_105f_2 y_2016 wwht ## 8 a_105f_3 y_2016 wwht ## 9 a_187f_1 y_2016 wwht ## 10 a_206f_1 y_2016 wwht ## # ℹ 1,509 more rows Looks good. We only have crops with winter wheat and potatoes, for every year. Exactly what we need, now we just need to count() them. conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field n ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 And we are back where we started! See, not that complicated. One last thing we need to do is rename n to conv, we do that like so: conv_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value %in% c(&quot;wwht&quot;, &quot;pota&quot;)) %&gt;% dplyr::count() %&gt;% dplyr::rename(conv = n) head(conv_crop_count) ## # A tibble: 6 × 2 ## # Groups: field [6] ## field conv ## &lt;chr&gt; &lt;int&gt; ## 1 a_001f_1 4 ## 2 a_001f_2 4 ## 3 a_001f_3 4 ## 4 a_001f_4 4 ## 5 a_001f_5 4 ## 6 a_001f_6 4 Now lets go ahead and do the same thing for the two other categories: cons andmeadow. meadow_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(value == &quot;meadow&quot; ) %&gt;% dplyr::count() %&gt;% dplyr::rename(meadow = n) cons_crop_count &lt;- crop_rotation %&gt;% melt(. ,&quot;field&quot;) %&gt;% group_by(field) %&gt;% filter(!(value %in% c(&quot;wwht&quot;, &quot;pota&quot;, &quot;meadow&quot;))) %&gt;% dplyr::count() %&gt;% dplyr::rename(cons = n) cons_crop_count %&gt;% head() ## # A tibble: 6 × 2 ## # Groups: field [6] ## field cons ## &lt;chr&gt; &lt;int&gt; ## 1 a_002f_1 2 ## 2 a_002f_2 2 ## 3 a_002f_3 4 ## 4 a_002f_4 4 ## 5 a_002f_5 2 ## 6 a_002f_6 4 meadow_crop_count %&gt;% head() ## # A tibble: 6 × 2 ## # Groups: field [6] ## field meadow ## &lt;chr&gt; &lt;int&gt; ## 1 a_012f_3 4 ## 2 a_012f_6 4 ## 3 a_016f_3 4 ## 4 a_017f_2 4 ## 5 a_017f_3 4 ## 6 a_046f_1 4 meadow was a simple filter, all we have to do was grab crops with the meadow name. For cons crops, we just grabbed the remaining crops that were not conv or meadow. Great. We have these 3 separate, we need to combine them. we can do that with left_join and join by the field column which contains our IDs # create a base dataframe to join to crop_fractions &lt;- crop_rotation %&gt;% dplyr::select(field) %&gt;% distinct() # join our 3 data frames crop_fractions &lt;- left_join(crop_fractions, conv_crop_count, by = &quot;field&quot;) crop_fractions &lt;- left_join(crop_fractions, cons_crop_count, by = &quot;field&quot;) crop_fractions &lt;- left_join(crop_fractions, meadow_crop_count, by = &quot;field&quot;) # lets have a look crop_fractions %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 a_025f_1 4 NA NA ## 2 a_025f_3 4 NA NA ## 3 a_025f_4 4 NA NA ## 4 a_025f_2 4 NA NA ## 5 a_105f_1 4 NA NA ## 6 a_105f_4 4 NA NA That does not look good! when it seems like when the count is 0, it is returned as NA. Lets fix that… crop_fractions &lt;- crop_fractions %&gt;% mutate(cons = ifelse(is.na(cons), 0, cons)) crop_fractions &lt;- crop_fractions %&gt;% mutate(conv = ifelse(is.na(conv), 0, conv)) crop_fractions &lt;- crop_fractions %&gt;% mutate(meadow = ifelse(is.na(meadow), 0, meadow)) What are we doing here? we are mutating the the 3 columns using an ifelse statement. The statement is simple. If the value is.na then we set it to 0. Otherwise, we set it to the same value it had before. did it work? crop_fractions %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a_025f_1 4 0 0 ## 2 a_025f_3 4 0 0 ## 3 a_025f_4 4 0 0 ## 4 a_025f_2 4 0 0 ## 5 a_105f_1 4 0 0 ## 6 a_105f_4 4 0 0 Very good. Now we need to decide what to classify our fields at. lets Define some functions to do that. is_meadow &lt;- function(conv, cons, meadow) { ((meadow &gt; cons) &amp; (meadow &gt; conv)) %&gt;% return() } is_cons &lt;- function(conv, cons, meadow) { ((cons &gt;= meadow) &amp; (cons &gt; conv)) %&gt;% return() } is_conv &lt;- function(conv, cons, meadow) { ((conv &gt;= meadow) &amp; (conv &gt;= cons)) %&gt;% return() } The &amp; sign means that both conditions need to be met. and &gt;= you should know already. Lets use those functions in action. lets do the meadow first. We will create a new dataframe from crop fractions, named field_class. field_class &lt;- crop_fractions %&gt;% mutate(meadow = is_meadow(conv,cons,meadow)) head(field_class) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 a_025f_1 4 0 FALSE ## 2 a_025f_3 4 0 FALSE ## 3 a_025f_4 4 0 FALSE ## 4 a_025f_2 4 0 FALSE ## 5 a_105f_1 4 0 FALSE ## 6 a_105f_4 4 0 FALSE Ok, so none of those first 6 fields are meadow dominated. What about cons? (lets stick with our field_class dataframe and just keep adding on) field_class &lt;- field_class %&gt;% mutate(cons = is_cons(conv,cons,meadow)) head(field_class) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 a_025f_1 4 FALSE FALSE ## 2 a_025f_3 4 FALSE FALSE ## 3 a_025f_4 4 FALSE FALSE ## 4 a_025f_2 4 FALSE FALSE ## 5 a_105f_1 4 FALSE FALSE ## 6 a_105f_4 4 FALSE FALSE Nope, not cons either. Then it must be conv dominant. field_class &lt;- field_class %&gt;% mutate(conv = is_conv(conv,cons,meadow)) field_class %&gt;% head() ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 a_025f_1 TRUE FALSE FALSE ## 2 a_025f_3 TRUE FALSE FALSE ## 3 a_025f_4 TRUE FALSE FALSE ## 4 a_025f_2 TRUE FALSE FALSE ## 5 a_105f_1 TRUE FALSE FALSE ## 6 a_105f_4 TRUE FALSE FALSE Correct! now lets just double check that we didnt have any cases where all are TRUE or where all are FALSE field_class %&gt;% dplyr::select(conv, cons, meadow) %&gt;% isTRUE %&gt;% all() ## [1] FALSE field_class %&gt;% dplyr::select(conv, cons, meadow) %&gt;% isFALSE() %&gt;% all() ## [1] FALSE Looks good to me! carry on: We have our field classifications now, they will be very useful to us later. Lets pull them out of our dataframe to store them as a nice list. cons_fields &lt;- field_class %&gt;% filter(cons) %&gt;% dplyr::select(field) %&gt;% pull() conv_fields &lt;- field_class %&gt;% filter(conv) %&gt;% dplyr::select(field) %&gt;% pull() meadow_fields &lt;- field_class %&gt;% filter(meadow) %&gt;% dplyr::select(field) %&gt;% pull() cons_fields %&gt;% head() ## [1] &quot;a_182f_1&quot; &quot;a_182f_5&quot; &quot;a_182f_3&quot; &quot;a_182f_4&quot; &quot;a_182f_2&quot; &quot;a_182f_7&quot; conv_fields %&gt;% head() ## [1] &quot;a_025f_1&quot; &quot;a_025f_3&quot; &quot;a_025f_4&quot; &quot;a_025f_2&quot; &quot;a_105f_1&quot; &quot;a_105f_4&quot; meadow_fields %&gt;% head() ## [1] &quot;a_182f_6&quot; &quot;a_182f_8&quot; &quot;a_017f_3&quot; &quot;a_017f_2&quot; &quot;a_012f_3&quot; &quot;a_012f_6&quot; Fantastic! Now that was a big task, but it makes the next bit very easy. Lets assign the correct ovn to the types of fields we have classified: 6.2.6.1 Agricultural Fields: Meadow landuse_lum$ov_mann[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;shortgrass&quot; 6.2.6.2 Agricultural Fields: Conventional landuse_lum$ov_mann[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;convtill_nores&quot; 6.2.6.3 Agricultural Fields: Conservational landuse_lum$ov_mann[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;falldisk_res&quot; 6.2.6.4 Forest Initially we wanted certain classes for the forest land use based on how good the soil quality was. But to do this, we would need multiple land uses, which we do not have. We will just use forest_medium for all frst. We can only change this if we go back to buildR and define more generic forest classes. landuse_lum$ov_mann[which(landuse_lum$name ==&quot;frst_lum&quot;)] &lt;- &quot;forest_med&quot; 6.2.6.5 Summary With all that data processing out of the way, lets have a quick look at what we’ve done: landuse_lum$ov_mann[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;densegrass&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;rangeland_20cover&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urban_rubble&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;urtn_lum&quot;)] &lt;- &quot;urban_asphalt&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;forest_med&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;shortgrass&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;convtill_nores&quot; landuse_lum$ov_mann[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;falldisk_res&quot; landuse_lum$ov_mann[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;forest_heavy_cs10&quot; Sweet. Next column.. 6.2.7 Setting cn2: This has been discussed in Issue Issue #1 You know the drill – I am sure you know how to read this code by now. # Brush-brush-weed-grass_mixture_with_brush_the_major_element (poor) landuse_lum$cn2[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;brush_p&quot; # Woods (poor) landuse_lum$cn2[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;wood_p&quot; # Paved_streets_and_roads;_open_ditches_(incl._right-of-way) landuse_lum$cn2[which(landuse_lum$name == &quot;utrn_lum&quot;)] &lt;- &quot;paveroad&quot; # Paved_parking_lots_roofs_driveways_etc_(excl_right-of-way) landuse_lum$cn2[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urban&quot; # Woods (fair) landuse_lum$cn2[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;wood_f&quot; # Pasture_grassland_or_range-continuous_forage_for_grazing landuse_lum$cn2[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;pastg_g&quot; # Meadow-continuous_grass_protected_from_grazing_mowed_for_hay landuse_lum$cn2[which(landuse_lum$field_id %in% meadow_fields)] &lt;- &quot;pasth&quot; # Row_crops landuse_lum$cn2[which(landuse_lum$field_id %in% cons_fields)] &lt;- &quot;rc_conterres_g&quot; # Row_crops landuse_lum$cn2[which(landuse_lum$field_id %in% conv_fields)] &lt;- &quot;rc_strow_p&quot; 6.2.8 Setting cons_prac This has been discussed in Issue #4 For this, we need to add some custom entries to the database: cons_prac_path &lt;- &quot;model_data/cs10_setup/swat_input/cons_practice.lum&quot; cons_prac_path_new &lt;- &quot;model_data/cs10_setup/swat_input_mod/cons_practice.lum&quot; cons_prac &lt;- readLines(cons_prac_path) cons_prac[1:3] %&gt;% print() ## [1] &quot;cons_practice.lum: written by SWAT+ editor v2.1.0 on 2023-05-30 10:39 for SWAT+ rev.60.5.4&quot; ## [2] &quot;name usle_p slp_len_max description&quot; ## [3] &quot;up_down_slope 1.00000 121.00000 Up_and_down_slope&quot; Let us do it in a way so that it is only added if it doesn’t exist yet: if(grepl(x = cons_prac, &quot;agri_conv&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_conv 1.00000 60.00000 no_convervation&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_part_conv&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_part_conv 0.85000 60.00000 75_percent_convential&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_half&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_half 0.70000 50.00000 50_percent_convential&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_part_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_part_cons 0.50000 30.00000 75_percent_consveration&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;agri_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;agri_cons 0.30000 30.00000 100_percent_consveration&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;past_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;past_cons 0.1 60 pasture&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;rngb_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;rngb_cons 0.2 60 rangeland&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;frst_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;frst_cons 0.1 60 forest&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;urml_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;urml_cons 1 60 cs10urban&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;utrn_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;utrn_cons 1 60 road&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_meadow&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_meadow 0.2 60 cs10_meadow&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;wetf_cons&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;wetf_cons 0.05 30 wetlands&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_sed_pond&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_sed_pond 0.1 30 cs10_sed_pond&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_cons_wetl&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_cons_wetl 0.1 30 cs10_cons_wetl&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs10_buff_grass&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs10_buff_grass 0.25 10 cs10_buff_grass&quot; cons_prac &lt;- c(cons_prac, entry) } if (grepl(x = cons_prac, &quot;cs_10_buff_wood&quot;) %&gt;% which %&gt;% length() == 0) { entry &lt;- &quot;cs_10_buff_wood 0.2 10 cs_10_buff_wood&quot; cons_prac &lt;- c(cons_prac, entry) } And write the updated file: writeLines(cons_prac, con = cons_prac_path_new) Now we need to define the 0, 25, 50, 75, 100 percent conventional crops. We can use our old “crop_fractions” dataframe: head(crop_fractions) ## # A tibble: 6 × 4 ## field conv cons meadow ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a_025f_1 4 0 0 ## 2 a_025f_3 4 0 0 ## 3 a_025f_4 4 0 0 ## 4 a_025f_2 4 0 0 ## 5 a_105f_1 4 0 0 ## 6 a_105f_4 4 0 0 Lets derive which fields get classified as what p100_conv &lt;- crop_fractions %&gt;% filter(conv==4) %&gt;% dplyr::select(field) %&gt;% pull() p75_conv &lt;- crop_fractions %&gt;% filter(conv==3) %&gt;% dplyr::select(field) %&gt;% pull() p50_conv &lt;- crop_fractions %&gt;% filter(conv==2) %&gt;% dplyr::select(field) %&gt;% pull() p25_conv &lt;- crop_fractions %&gt;% filter(conv==1) %&gt;% dplyr::select(field) %&gt;% pull() p0_conv &lt;- crop_fractions %&gt;% filter(conv==0) %&gt;% dplyr::select(field) %&gt;% pull() Now we can assign the all the land uses # Agricultural landuse_lum$cons_prac[which(landuse_lum$field_id %in% p100_conv)] &lt;- &quot;agri_conv&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p75_conv)] &lt;- &quot;agri_part_conv&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p50_conv)] &lt;- &quot;agri_half&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p25_conv)] &lt;- &quot;agri_part_cons&quot; landuse_lum$cons_prac[which(landuse_lum$field_id %in% p0_conv)] &lt;- &quot;agri_cons&quot; # Meadow landuse_lum$cons_prac[which(field_class$meadow)] &lt;- &quot;cs10_meadow&quot; # Generic landuse_lum$cons_prac[which(landuse_lum$name == &quot;past_lum&quot;)] &lt;- &quot;past_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;rngb_lum&quot;)] &lt;- &quot;rngb_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;frst_lum&quot;)] &lt;- &quot;frst_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;urml_lum&quot;)] &lt;- &quot;urml_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;utrn_lum&quot;)] &lt;- &quot;utrn_cons&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;wetf_lum&quot;)] &lt;- &quot;wetf_cons&quot; # Measures landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_sed_pond&quot;)] &lt;- &quot;cs10_sed_pond&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_cons_wetl&quot;)] &lt;- &quot;cs10_cons_wetl&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs10_buff_grass&quot;)] &lt;- &quot;cs10_buff_grass&quot; landuse_lum$cons_prac[which(landuse_lum$name == &quot;cs_10_buff_wood&quot;)] &lt;- &quot;cs_10_buff_wood&quot; 6.2.9 Setting tile This column has already been completed by SWATbuildR in 2.4 6.2.10 Setting sep This column has been left as null as discussed in Issue #9 6.2.11 Setting vfs This column has been left as null as discussed in Issue #10 6.2.12 Setting grww This column has been left as null as discussed in Issue #11 6.2.13 Setting bmp This column has been left as null as discussed in Issue #12 6.3 Writing Changes We are done with our modifications however, we need to remove the field_id column landuse_lum &lt;- landuse_lum %&gt;% dplyr::select(-field_id) lets take a look at the final product: datatable(landuse_lum) Lets write out changes in a new directory where we will store any SWAT+ input files which we have modified from their source BuildR form. dir.create(&quot;model_data/cs10_setup/swat_input_mod&quot;, showWarnings = F) new_lum_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot; write.table(landuse_lum, file = new_lum_path, sep = &quot;\\t&quot;, quote = F, row.names = F) But wait – we need to keep that pesky header, otherwise the FarmR will be very angry with us. lum_lines &lt;- readLines(new_lum_path) header &lt;- &quot;header header HEADER, delete me and you will regret it FOREVER!&quot; lum_lines2&lt;- c(header,lum_lines) writeLines(text = lum_lines2, con = new_lum_path) Done! Now that We’ve changed another input file, we should test if our model still runs. We will do that with our custom test_swat_mod() function. This function is basically the same as the previous test_swat() function, only that it incorporates the modified SWAT input files source(&quot;model_data/code/test_swat_mod.R&quot;) simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) Related Milestone landuse.lum update Related minor issues Don’t write rownames! - #50 Update the land use file - #40 References "],["field-site-loggers.html", "Section 7 Field Site Loggers 7.1 Introduction 7.2 Data Processing 7.3 Data quality check 7.4 Data Cleanup", " Section 7 Field Site Loggers 7.1 Introduction This chapter is concerned with preparing the measured data for the OPTAIN field scale models. This will involve loading the data, transforming it into a usable format, and performing a quality check. The end of this chapter will result in data fit for use in SWAP as well as a tangentially related antecedent precipitation index (API) model for use in the SWAT+ modelling work, related to scheduling management operations using SWATFarmR . 7.1.1 Prerequisites The following packages are required for this chapter: require(readr) # for reading in data require(dplyr) # for manipulating data require(stringr) # for manipulating text require(purrr) # for extracting from nested lists require(plotly) # for diagnostic plotting require(lubridate) # for date conversions require(reshape2) # for data conversions require(wesanderson) # plot colors require(DT) # for tables require(mapview) # for plotting require(sf) # for shapefiles show_rows &lt;- 500 # number of rows to show in the tables As well as the following custom functions: source(&quot;model_data/code/plot_logger.R&quot;) 7.1.2 Locations The loggers are described as such: Logger3(600409): deep rocky sand (forest?) Logger4(600410): Tormods farm (agri) Logger2(600411): John Ivar lower area (agri) Logger1(600413): John Ivar behind the barn (agri) And are located here: field_sites_shp &lt;- read_sf(&quot;model_data/field_sites/geospatial/cs10_field_sites.shp&quot;) basin_shp &lt;- read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;) basin_map &lt;- mapview(basin_shp, alpha.regions = .1, legend = FALSE) field_map &lt;- mapview(field_sites_shp, legend = FALSE) basin_map+field_map 7.2 Data Processing 7.2.1 Loading Data To start we need to load in our data from the GroPoint Profile data loggers. The logger is approx. 1m long and has 7 temperature sensors, and 6 soil moisture sensors located along its segments. “GroPoint Profile Multi Segment Soil Moisture &amp; Temperature Profiling Probe Operation Manual” (2023) Figure 7.1: GroPoint Segment Schematic (GroPoint Profile User Manual, Page 6) Figure 7.2: GroPoint Temperature sensor depths. We use GPLP-4 with seven temperature sensors. (GroPoint Profile User Manual, Page 6) The logger data has been modified before import into R. The file names have been changed and column headers have been added. Missing values stored as “Error” have been removed. Columns headers have assigned with variable name, depth, and unit of depth separated by an underscore. path &lt;- &quot;model_data/field_sites/loggers/data/&quot; Loading the data and modifying it is performed with readr and dplyr. # list of files datasheets &lt;- list.files(path, full.names = TRUE) # loading in the data, with an ID column &#39;source&#39; data &lt;- read_csv(datasheets, show_col_types = F, id = &quot;source&quot;) # translating the source text to logger ID data$site &lt;- data$source %&gt;% str_remove(path) %&gt;% str_remove(&quot;.csv&quot;) %&gt;% str_split(&quot;_&quot;) %&gt;% map(1) %&gt;% unlist() # removing the source column data &lt;- data %&gt;% select(-source) # removing duplicates data &lt;- data %&gt;% distinct() Data preview We need to force the date time column into the correct format. We can do this with the following command. (Please note, this has been done for region: United States this code may need to be adjusted to fit your regional settings) data$datetime &lt;- data$datetime %&gt;% as_datetime(format = &quot;%m/%d/%Y %H:%M&quot;, tz = &quot;CET&quot;) 7.2.2 Data Re-structuring For ease of use in R, we will re-structure the data to be in “tidy” format (Read more: (Wickham 2014)). # grab the measured variable column headers mea_var &lt;- colnames(data)[3:15] # melt (tidy) the data by datetime and site. data_melt &lt;- data %&gt;% melt(id.vars = c(&quot;datetime&quot;, &quot;site&quot;), measure.vars = mea_var) # parse out the variable data_melt$var &lt;- data_melt$variable %&gt;% str_split(&quot;_&quot;) %&gt;% map(1) %&gt;% unlist() # parse out the depth of measurement data_melt$depth &lt;- data_melt$variable %&gt;% str_split(&quot;_&quot;) %&gt;% map(2) %&gt;% unlist() # remove duplicates data_melt &lt;- data_melt %&gt;% distinct() Data preview: 7.2.3 Summarize to Daily Means As SWAP is a daily time step model, we do not need the hourly resolution, and can therefore simplify our analysis. We first need to parse out a (daily) date column data_melt$date &lt;- data_melt$datetime %&gt;% as.Date() And then group the data by this date, followed by an averaging function. daily_data &lt;- data_melt %&gt;% group_by(date, var, depth, site) %&gt;% summarise(value = round(mean(value, na.rm = TRUE), 1), .groups = &quot;drop_last&quot;) %&gt;% ungroup() Additional notes: The values are rounded to one decimal place, as this is is the precision limit of the Logger. The averaging is performed per date, variable, depth, and site The functionality of .groups = \"drop_last\" is unknown to me, however it does not change anything other than quieting a warning message. The data is “ungrouped” at the end, to avoid problems with plotting the data (with plotly) later on. Data preview: 7.2.4 Including Air Temperature and Precipitation Data We are going to add the temp / precipitation data from out SWAT+ setup to give us insight on temperature and moisture dynamics for the data quality check. Also important to note, is that all this code is only necessary, because of the strange format SWAT+ uses for its weather input. If it would just use a standard format (like everything else) then all this would not be needed. Reading in the SWAT source precipitation data: swat_pcp &lt;- read.table(&quot;model_data/cs10_setup/optain-cs10/sta_id1.pcp&quot;, skip = 3, header = F, sep = &quot;\\t&quot;) %&gt;% as_tibble() swat_tmp &lt;- read.table(&quot;model_data/cs10_setup/optain-cs10/sta_id1.tmp&quot;, skip = 3, header = F, sep = &quot;\\t&quot;) %&gt;% as_tibble() Data preview: Parsing out the first and last day and year. FIRST_DOY &lt;- swat_pcp[1,][[2]]-1 FIRST_YEAR &lt;- swat_pcp[1,][[1]] LAST_DOY &lt;- swat_pcp[length(swat_pcp$V1),][[2]]-1 LAST_YEAR &lt;- swat_pcp[length(swat_pcp$V1),][[1]] Converting these to an R format. Note that R uses a 0-based index only for dates. (very confusing!). first_day &lt;- as.Date(FIRST_DOY, origin = paste0(FIRST_YEAR, &quot;-01-01&quot;)) last_day &lt;- as.Date(LAST_DOY, origin = paste0(LAST_YEAR, &quot;-01-01&quot;)) date_range &lt;- seq(from = first_day, to = last_day, by = &quot;day&quot;) Creating a dataframe from the source data pcp_data &lt;- tibble(date = date_range, pcp = swat_pcp$V3) And filtering it to only cover the same range as our logger data. pcp_data &lt;- pcp_data %&gt;% filter(date %in% daily_data$date) And we do the same for temperature FIRST_DOY &lt;- swat_tmp[1,][[2]]-1 FIRST_YEAR &lt;- swat_tmp[1,][[1]] LAST_DOY &lt;- swat_tmp[length(swat_tmp$V1),][[2]]-1 LAST_YEAR &lt;- swat_tmp[length(swat_tmp$V1),][[1]] first_day &lt;- as.Date(FIRST_DOY, origin = paste0(FIRST_YEAR, &quot;-01-01&quot;)) last_day &lt;- as.Date(LAST_DOY, origin = paste0(LAST_YEAR, &quot;-01-01&quot;)) date_range &lt;- seq(from = first_day, to = last_day, by = &quot;day&quot;) tmp_data &lt;- tibble(date = date_range, tmp_max = swat_tmp$V3, tmp_min = swat_tmp$V4) tmp_data &lt;- tmp_data %&gt;% filter(date %in% daily_data$date) Figure 7.3: Temperature range and precipitation for CS10 Note: this does (yet) include data from 2023, even though the loggers do. 7.3 Data quality check In this section we are proofing the quality of the data. 7.3.1 Logger 600409 “Deep Rocky Sand” plot_logger(daily_data, &quot;600409&quot;, pcp_data, tmp_data) Figure 7.4: Logger 600409 7.3.1.1 Soil Temperature Remove first few days with unrealistic data 7.3.1.2 Soil Moisture Content First few days need to be removed 150mm depth seems usable before Sept2022 – However Attila (instrument installer) mentioned this sensor might be in the air (which would explain low moisture levels), so potentially useless 350mm depth seems usable, breaks around/after Sept2022 450mm depth seems useless, constantly getting wetter for an entire year, spiking post Sept2022 600mm depth seems useless, constantly getting wetter for an entire year, spiking post Sept2022 750mm depth seems useless, constantly getting wetter for an entire year. 900mm depth seems useless, constantly getting wetter for an entire year. 7.3.2 Logger 600410 “Tormods farm” plot_logger(daily_data, &quot;600410&quot;, pcp_data, tmp_data) Figure 7.5: Logger 600410 7.3.2.1 Soil Temperature Remove first few days with unrealistic data 7.3.2.2 Soil Moisture Content First few days need to be removed Depth 150 probably useless, as sensor is in free air. (According to Attila) Logger seems to have broken Sometime after Sept1 2022, for all depth levels. Data pre-Sept22 seems usable (Csilla ?) 7.3.3 Logger 600411 “John Ivar Lower Area” plot_logger(daily_data, &quot;600411&quot;, pcp_data, tmp_data) Figure 7.6: Logger 600411 7.3.3.1 Soil Temperature Need to remove the first few days Need to remove the malfunction in April-May 2022 7.3.3.2 Soil Moisture Content First few days need to be removed Need to remove the malfunction in April-May 2022 Depths 150 is out of soil, must be removed. Logger seems to have broken August 16th, 2022, data seems useful before that date (Csilla, agree?) 7.3.4 Logger 600413 “John Ivar behind the barn” plot_logger(daily_data, &quot;600413&quot;, pcp_data, tmp_data) Figure 7.7: Logger 600413 7.3.4.1 Soil Temperature Remove the first few days with unrealistic data 7.3.4.2 Soil Moisture Content Remove first few days Remove 150mm, seems out of earth Hard to tell when sensor broke, earliest June 24th, latest Sept.12 2022, what do you think Csilla ? 7.4 Data Cleanup 7.4.1 General cleanup Removing the first few days of the simulation start_date &lt;- &quot;2021-11-01&quot; data_clean &lt;- daily_data %&gt;% filter(date &gt; start_date) To be on the safe side, I am setting the logger cut-off date to XXXXXXXXX This could be tuned “per logger” later on if one feels confident. cut_off_date &lt;- &quot;2022-08-14&quot; data_clean &lt;- data_clean %&gt;% filter(date &lt; cut_off_date) And we will do the same for the climate data # TODO dont actually do this. pcp_data_clean &lt;- pcp_data %&gt;% filter(date &gt; start_date) pcp_data_clean &lt;- pcp_data_clean %&gt;% filter(date &lt; cut_off_date) tmp_data_clean &lt;- tmp_data %&gt;% filter(date &gt; start_date) tmp_data_clean &lt;- tmp_data_clean %&gt;% filter(date &lt; cut_off_date) We will remove the 150 mm depth SMC measurement, as it was not submerged properly in the soil, and is therefore of little use. The temperature reading might be useful, so we will keep it. data_clean &lt;- data_clean %&gt;% filter(depth != &quot;150&quot;) 7.4.2 Logger specific cleanup 7.4.2.1 Logger 600409 No logger specific cleanup required 7.4.2.2 Logger 600410 No logger specific cleanup required 7.4.2.3 Logger 600411 Logger 600411 needs temperature and SMC data removed from 2022-04-21 to 2022-05-10. It seems like the logger was removed from the soil. # define time range to remove remove_dates &lt;- seq(from = as.Date(&quot;2022-04-21&quot;), to = as.Date(&quot;2022-05-10&quot;), by = &quot;day&quot;) # set values in that site and date range to NA data_clean &lt;- data_clean %&gt;% mutate(value = case_when((site == &quot;600411&quot; &amp; date %in% remove_dates) ~ NA, .default = value)) plot_logger(data_clean, &quot;600411&quot;, pcp_data_clean, tmp_data_clean) Figure 7.8: Logger 600411, cleaned data 7.4.2.4 Logger 600413 No logger specific cleanup required 7.4.3 Saving the cleaned data We can now save our cleaned up data. We will do so in CSV form write_csv(data_clean, file = &quot;model_data/field_sites/loggers/clean/logger_data_daily_clean.csv&quot;) write_csv(pcp_data_clean, file = &quot;model_data/field_sites/loggers/clean/pcp_data_clean.csv&quot;) References "],["field-site-properties.html", "Section 8 Field Site Properties 8.1 Determining the HSG group of the logger sites", " Section 8 Field Site Properties For various use-cases across the modelling project, we will require certain information about the locations of the field sites. This section gathers that information and makes it available to other sections. run_this_chapter = TRUE 8.1 Determining the HSG group of the logger sites require(sf) require(readr) require(DT) require(dplyr) require(mapview) require(ggplot2) sf_theme &lt;- theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks ) We load the soil map of the agricultural area. soil_map &lt;- &quot;model_data/input/soil/soil_map_79.shp&quot; soil_map_shp &lt;- read_sf(soil_map) We will crop it by the watershed. Therefore we load the watershed cs10_basin_path &lt;- &quot;model_data/input/shape/cs10_basin_1m.shp&quot; cs10_basin &lt;- read_sf(cs10_basin_path) # reproject cs10_basin &lt;- st_transform(cs10_basin, st_crs(soil_map_shp)) And crop by by basin. (current not sure of the effect of constant geometry) st_agr(soil_map_shp) = &quot;constant&quot; st_agr(cs10_basin) = &quot;constant&quot; soil_map_shp &lt;- st_intersection(soil_map_shp, cs10_basin) ggplot()+geom_sf(soil_map_shp, mapping = aes(fill = name))+sf_theme Figure 8.1: Soil map of CS10, in shapefile format We need information on the soil itself. This is found in our user table. We only need the hydrologic soil group (HSG) HYDGRP, but we will keep the soil depth SOL_ZMX and soil texture TEXTURE just in case # TODO update this path usersoil_path &lt;- &quot;model_data/input/soil/usersoil_80.csv&quot; usersoil &lt;- read_csv(usersoil_path, show_col_types = F) usersoil &lt;- usersoil %&gt;% dplyr::select(OBJECTID, SNAM, SOL_ZMX, HYDGRP, TEXTURE) We can combine the two data sets with a left join by soil name usersoil$name &lt;- usersoil$SNAM soil_propery_map &lt;- dplyr::left_join(soil_map_shp, usersoil, by = &quot;name&quot;) ggplot()+geom_sf(soil_propery_map, mapping = aes(fill = HYDGRP))+sf_theme Figure 8.2: HSG of CS10 Now we need the locations of the field sites. These are stored as points in the following file: cs10_field_sites_path &lt;- &quot;model_data/field_sites/geospatial/cs10_field_sites.shp&quot; cs10_field_sites &lt;- read_sf(cs10_field_sites_path) We are going to join the attributes using a spatial join cs10_field_sites &lt;- st_transform(cs10_field_sites, crs = st_crs(soil_propery_map)) field_sites_attr &lt;- st_join(cs10_field_sites, soil_propery_map) Here is the result: We have two sites covering C and D, and one site covering B. This covers all existing HSGs in the catchment, which is good. We can save this information in a dataframe. (UPDATE: with new soil data this is no longer the case) field_site_attr_df &lt;- st_drop_geometry(field_sites_attr) datatable(field_site_attr_df) We will write this into our output folder write_csv(x = field_site_attr_df, file = &quot;model_data/field_sites/output/field_site_attr_df.csv&quot;) And save our soil property map, and field site points (with attributes) # TODO fix the weird ID column mess you made write_sf(field_sites_attr, &quot;model_data/field_sites/output/field_site_attr_map.shp&quot;) write_sf(soil_propery_map, &quot;model_data/field_sites/output/soil_propery_map.shp&quot;) "],["antecedent-precipitation-index.html", "Section 9 Antecedent Precipitation Index 9.1 Introduction 9.2 Antecedent Precipitation Index 9.3 Conclusions", " Section 9 Antecedent Precipitation Index 9.1 Introduction SWATFarmR cannot account for operations based on values modeled by SWAT+, such as soil moisture. This is because FarmR does not run the model itself, and creates and pre-defines all the management schedules based on measured climate data. This means we need to relate our measured climate data to our soil moisture (Because soil moisture is an important factor in planning agricultural operations). One method to do this is called the Antecedent Precipitation Index. Which is what we will creating for CS10 in this part. As a heads up, this section requires Python! 9.1.1 Pre-requirements Required R packages require(readr) require(dplyr) require(hydroGOF) library(reticulate) # Required Python packages: &quot;pandas&quot;, &quot;numpy&quot;, &quot;scipy&quot; source(&quot;model_data/code/plot_api.R&quot;) source(&quot;model_data/code/calc_api.R&quot;) 9.1.2 Setting up Python for R/Rmarkdown A quick (non-evaluated) code block to show how a python environment can be created for use in Rmarkdown/Reticulate on your local machine. # install miniconda install_miniconda() # create conda environment for your project conda_create(&quot;cs10_env&quot;, # environment name packages = c(&quot;pandas&quot;, &quot;numpy&quot;, &quot;scipy&quot;), # required packages conda = &quot;auto&quot;) conda_python(&quot;cs10_env&quot;) # returns the .exe python path for your environment # you need to change your R environment path in this file usethis::edit_r_environ() # RETICULATE_PYTHON=&quot;YOUR ENVIRONMENT PATH/python.exe&quot; # restart your R session. py_config() # If everything is right, you should get something like: # python: YOUR ENVIRONMENT PATH/python.exe libpython: YOUR ENVIRONMENT # PATH/python39.dll pythonhome: YOUR ENVIRONMENT PATH version: 3.9.7 (default, # Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] Architecture: 64bit numpy: # YOUR ENVIRONMENT PATH\\lib\\site-packages\\numpy numpy_version: 1.21.2 # After this it should work :) 9.2 Antecedent Precipitation Index From (Patrignani 2020) The API is a well-known, parsimonious, recursive model for predicting soil moisture solely based on precipitation records. The API is commonly implemented using daily precipitation records, but it is possible to work at finer temporal scales (e.g. hourly) if both precipitation (model input) and soil moisture (for validation purposes) are available. The equation describing the simple version of the model is: \\[API_{t} = \\alpha API_{t-1} + P_t\\] \\(API_t\\): Soil water content at time \\(t\\) (today) \\(API_{t-1}\\): Soil water content at time \\(t-1\\) (yesterday) \\(\\alpha\\): Loss coefficient. Range between 0 and 1 \\(P_t\\): Precipitation at time \\(t\\) (today) 9.2.1 Loading data We will load out data from the previous Section 7.4.3 logger_data &lt;- read_csv(&quot;model_data/field_sites/loggers/clean/logger_data_daily_clean.csv&quot;, show_col_types=F) # we only need SMC for this part logger_data = logger_data %&gt;% filter(var == &quot;smc&quot;) site_attributes &lt;- read_csv(&quot;model_data/field_sites/output/field_site_attr_df.csv&quot;, show_col_types = F) pcp_data &lt;- read_csv(&quot;model_data/field_sites/loggers/clean/pcp_data_clean.csv&quot;, show_col_types = F) head(site_attributes) ## # A tibble: 5 × 8 ## id name dem_basin OBJECTID SNAM SOL_ZMX HYDGRP TEXTURE ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 600413 STlv-sl 1 70 STlv-sl 1000 D silty_clay_loam ## 2 600411 STrt-sl 1 73 STrt-sl 1000 D silty_clay_loam ## 3 600410 STeu-sl-pp 1 62 STeu-sl-pp 1000 D loam ## 4 600409 Humic 1 31 Humic 250 D silt_loam ## 5 600408 Humic 1 31 Humic 250 D silt_loam We will calculate the model per Hydrologic soil group (HSG), and start with logger 600410. log_data &lt;- logger_data %&gt;% filter(site == &quot;600410&quot;) 9.2.2 Calculating the total water content We need to calculate the total water content of the profile from the depth measurements from the following depths: unique(log_data$depth) ## [1] &quot;300&quot; &quot;450&quot; &quot;600&quot; &quot;750&quot; &quot;900&quot; Following code is modified from Patrignani (2020) (read more) (This calculation could be improved by considering the soil horizons) a &lt;- log_data %&gt;% filter(depth == &quot;300&quot;) %&gt;% dplyr::select(value) b &lt;- log_data %&gt;% filter(depth == &quot;450&quot;) %&gt;% dplyr::select(value) c &lt;- log_data %&gt;% filter(depth == &quot;600&quot;) %&gt;% dplyr::select(value) d &lt;- log_data %&gt;% filter(depth == &quot;750&quot;) %&gt;% dplyr::select(value) e &lt;- log_data %&gt;% filter(depth == &quot;900&quot;) %&gt;% dplyr::select(value) total_water_content &lt;- a * .35 + # from 0 to 35 b * .15 + # from 35 to 50 c * .15 + # from 55 to 65 d * .15 + # from 65 to 80 e * .20 # from 80 to 100 log_col &lt;- tibble(log_data %&gt;% filter(depth == &quot;300&quot;) %&gt;% dplyr::select(date), total_water_content) And we can add the precipitation data from 7.2.4 log_col &lt;- left_join(log_col, pcp_data, by = &quot;date&quot;) plot_api(obs = log_col, title = &quot;600410&quot;) Figure 9.1: Total water content for logger 600410 Saving the data write_csv(log_col, &quot;model_data/field_sites/output/twc.csv&quot;) 9.2.3 Defining API model The value is capped at the maximum value of the source data (50). This could be improved by capping site specific. def api_model(P,alpha=0.97,ini=36.8): api = [ini] for t in range(1,len(P)): append_val = api[t-1]*alpha + P[t] if append_val &gt; 50: append_val = 50 api.append(append_val) return api Load in our data: import pandas as pd df = pd.read_csv(&#39;model_data/field_sites/output/twc.csv&#39;) df = df[[&#39;date&#39;,&#39;value&#39;,&#39;pcp&#39;]] df.head() ## date value pcp ## 0 2021-11-02 36.880 0.3 ## 1 2021-11-03 36.930 1.1 ## 2 2021-11-04 36.995 0.0 ## 3 2021-11-05 37.025 0.0 ## 4 2021-11-06 37.030 0.7 As a test, we will calculate the predictions with an alpha of 0.95: import pandas as pd from scipy.optimize import curve_fit # guess storage_guessed = api_model(df[&#39;pcp&#39;], alpha=0.95, ini=36.8) # write to text dates = df[[&#39;date&#39;]] guess = pd.DataFrame(dates.values.tolist(),storage_guessed) guess.to_csv(&quot;model_data/field_sites/output/first_guess.csv&quot;) Now we can have a look at it in R guess &lt;- read_csv(&quot;model_data/field_sites/output/first_guess.csv&quot;, show_col_types = F, skip = 1, col_names = c(&quot;val&quot;, &quot;date&quot;)) plot_api(api_vals = guess, obs = log_col, title = &quot;API guessed&quot;) Figure 9.2: API for guessed alpha of HSG B Now we optimize the alpha parameter using scipy curve_fit. par_opt, par_cov = curve_fit(api_model, df[&#39;pcp&#39;], df[&#39;value&#39;], p0=[0.95,100]) print(&#39;Annual mean alpha value is&#39;,round(par_opt[0],2)) ## Annual mean alpha value is 0.98 Now we can calculate the API using the optimized parameter. import numpy as np storage_optimized = api_model(df[&#39;pcp&#39;],par_opt[0],par_opt[1]) # Mean Absolute Error MAE = np.mean(np.abs(df[&#39;value&#39;] - storage_optimized)) print(&#39;Mean annual error =&#39;, round(MAE,1),&#39;cm&#39;) ## Mean annual error = 9.1 cm And save the data for R dates = df[[&#39;date&#39;]] api_optimized = pd.DataFrame(dates.values.tolist(),storage_optimized) api_optimized.to_csv(&quot;model_data/field_sites/output/api_optimized.csv&quot;) And have a look in R: optimized &lt;- read_csv(&quot;model_data/field_sites/output/api_optimized.csv&quot;, show_col_types = F, skip = 1, col_names = c(&quot;val&quot;, &quot;date&quot;)) plot_api(api_vals = optimized, obs = log_col, title = &quot;API optimized&quot;) Figure 9.3: API with an optimized alpha for HSG B Not great, but also not surprising considering no account for snow melt has been done, nor do we have good logger data. We will define this workflow in a function calc_api.R and run it for the next loggers. # TODO fix warning calc_api(logger_id = &quot;600409&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set Figure 9.4: Optimized API for logger 600409 ## HSG Alpha ## &quot;D&quot; &quot;1&quot; This is the logger with broken data, so no surprise that the optimizer failed. calc_api(logger_id = &quot;600411&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set Figure 9.5: Optimized API for logger 600411 ## HSG Alpha ## &quot;D&quot; &quot;0.99&quot; calc_api(logger_id = &quot;600413&quot;) ## Warning in py_to_r.pandas.core.frame.DataFrame(x): index contains duplicated ## values: row names not set Figure 9.6: Optimized API for logger 600413 ## HSG Alpha ## &quot;D&quot; &quot;0.99&quot; 9.3 Conclusions UPDATE: all our sites are classed as D now For HSG B we got an alpha of 0.98, for C we have two samples, both of which got 0.99, and D has two samples as well, but we currently only have data for one. For the one D site we had, the logger data was very poor and the covariance could not be estimated. Overall, the API does not seem to be a great estimator of soil moisture for our specific case. Our logger data is poor and has been cut short due to sensor malfunctions. With a longer time series covering more seasons, it is likely that the curve fitting could be improved upon, and the overall dynamics could be appraised.; Secondly, and more critically: the API does not account for snow melt, which plays a significant role in a Boreal catchments such as CS10. You can see this in action (I think) in March where the logger data steadily goes up, while the API steadily goes down. Snow melt could surely be accounted for by adding another model to estimate it – the question is more if it is worth it, considering the poor underlying data of the loggers. Another complication is, if SWATFarmR does not account for snow melt in its management scheduling, then there is no point in estimating it. And if snow melt is not incorporated into the scheduling, then the operation timing will not be very accurate anyway (since snow melt is such a dominating factor in operation timing). UPDATE post Q+A Session We can modify the output of Micha’s script in Section 11.1 by replacing API with our own custom variable. This variable could be snow melt or something else entirely (like a trafficability thing?) This is pretty straight forward, we just need to add the index to the FarmR using add_variable(). One thing we need to keep in mind is that this needs to be compatible for the climate scenarios (i.e. we can only base it off climate variables which we also have future data on.) Some things to consider: Run SWAT+ for the weather data + scenarios data and use the calculated snow melt Re-create the snow melt model of SWAT+ and use that Use HBV or Persist models Use some tractability index for Norway (Attila?) References "],["model-parameterization.html", "Section 10 Model Parameterization 10.1 Parameter Changes 10.2 Fixing tiledrain 10.3 Setting Soil Phosphorous Initial Conditions", " Section 10 Model Parameterization These sections are not detailed enough to warrant their own dedicated chapter. 10.1 Parameter Changes These sections mostly involve changing a few parameter values and are therefore in small sub sections without R-code. Some of this might change later. 10.1.1 Channel parameters revised This an ongoing unresolved issue #49. So far nothing has been changed. 10.1.2 Crop parameters verified The crop database has been updated and stored in the following file: &quot;model_data/cs10_setup/swat_input_mod/plants.plt&quot; Discussions to this topic can be found in issue #27. From the whole database, we only use crops defined in our management files: Crop management: oats, barl, wwht, swht, pota, fesc Generic: frst, fesc and others which are minor The parameters for these crops have been updated to reflect the colder growing conditions. 10.1.3 Soil physical parameters in final form This has been mentioned in issue #28, and has been closed without discussion. Seems like the parameters are in final form, but no further documentation exists at this point. 10.1.4 Soil chemical parameters in final form This step will be verified in issue #46. It has been discussed in #19. The following changes have been implemented, but are subject to change: Changes made to soilnut1 of nutrients.sol Parameter Default CS10 lab_p 5 20 nitrate 7 8.5 inorgp 3.5 35.1 watersol_p 0.15 0.4 nutrients_sol &lt;- readLines(&quot;model_data/cs10_setup/swat_input/nutrients.sol&quot;) header &lt;- nutrients_sol[1] nutrients_sol_df &lt;- read.table(&quot;model_data/cs10_setup/swat_input/nutrients.sol&quot;, header = T, skip = 1, sep = &quot;&quot;, fill = T, as.is = T, colClasses = &quot;character&quot;) nutrients_sol_df$lab_p &lt;- 20 nutrients_sol_df$nitrate &lt;- 8.5 nutrients_sol_df$inorgp &lt;- 35.1 nutrients_sol_df$watersol_p &lt;- 0.4 write.table( nutrients_sol_df, &quot;model_data/cs10_setup/swat_input_mod/nutrients.sol&quot;, quote = F, sep = &quot; &quot;, row.names = F, ) header_new &lt;- paste(&quot;modified by the CS10 workflow in section 9.1&quot;) nutrients_sol &lt;- readLines(&quot;model_data/cs10_setup/swat_input_mod/nutrients.sol&quot;) writeLines(c(header_new, nutrients_sol), &quot;model_data/cs10_setup/swat_input_mod/nutrients.sol&quot;) Testing if the setup still works: simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) 10.1.5 Impoundment parameters defined Has been postponed by #20, will be dealt with in #54 10.1.6 Water diversions defined Deemed as not relevant for this catchment 10.1.7 Point sources parameters added Is an ongoing issue in #21 10.1.8 Tile drainage parameters defined This has been discussed in Issue #7 and is an ongoing issue in #53. old_path &lt;- &quot;model_data/cs10_setup/swat_input/tiledrain.str&quot; new_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/tiledrain.str&quot; tiledrain_str &lt;- readLines(old_path) header &lt;- paste(&quot;modified by CS10 workflow in section 9.1&quot;) tiledrain_df &lt;- read.table( old_path, sep = &quot;&quot;, header = T, skip = 1, fill = T, colClasses = &quot;character&quot; ) tiledrain_df$dp &lt;- 800 # from 1000 tiledrain_df$t_fc &lt;- 12 # from 24 tiledrain_df$lag &lt;- 30 # from 96 tiledrain_df$rad &lt;- 200 # from 100 tiledrain_df$dist &lt;- 8000 # from 30 tiledrain_df$drain &lt;- 40 # from 10 tiledrain_df$pump &lt;- 0 # from 1 tiledrain_df$lat_ksat &lt;- 2 # from 2 (no change) write.table(header, file = new_path, quote = F, col.names = F, row.names = F) write.table( x = tiledrain_df, file = new_path, sep = &quot; &quot;, quote = F, append = T, row.names = F ) ## Warning in write.table(x = tiledrain_df, file = new_path, sep = &quot; &quot;, quote = F, ## : appending column names to file Testing if the setup still works: simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) 10.1.9 Atmospheric deposition defined Has been done in Section 3.4 10.1.10 Additional settings verified Refers to chapter 3.11 in The Protocol and files parameters.bsn codes.bsn Has been discussed in #22 The following changes have been made: Changes made to codes.bsn Parameter Default CS10 pet 1 2 rte_cha 0 1 cn 0 1 tiledrain 0 1 soil_p 0 1 atmo_dep a m old_path &lt;- &quot;model_data/cs10_setup/swat_input/codes.bsn&quot; new_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/codes.bsn&quot; codes_bsn &lt;- readLines(old_path) header &lt;- codes_bsn[1] header &lt;- paste(&quot;modified by CS10 workflow in section 9.1&quot;) codes_df &lt;- read.table(old_path, skip = 1, header = T, sep = &quot;&quot;, colClasses = &quot;character&quot;) PET method (pet) The OPTAIN project recommends the PET calculation of Pennman-Monteith, which is code 2. codes_df$pet &lt;- 2 Channel routing network (rte_cha) Recommended is to start with Muskingum (code 1) and apply variable storage method if it causes problems. codes_df$rte_cha &lt;- 1 Stream water quality (wq_cha) Recommended to test both for OPTAIN (1/0) Daily curve number calculation (cn) Code 1 is recommended. (Plant ET) codes_df$cn &lt;- 1 OPTAIN recommends new version codes_df$soil_p &lt;- 1 Lapse rate is being tested in ?? Plant growth stress is being testing in ?? Tile drainage This should be set to 1, but that causes a crash. We will fix this bug in a dedicated section codes_df$tiledrain &lt;- 0 Atmospheric Deposition atmodep was done in Section 3.4 and is annual codes_df$atmo_dep &lt;- &quot;y&quot; Writing the changes write.table(header, file = new_path, col.names = F, row.names = F, quote = F) write.table( codes_df, file = new_path, col.names = T, append = T, quote = F, sep = &quot; &quot;, row.names = F ) ## Warning in write.table(codes_df, file = new_path, col.names = T, append = T, : ## appending column names to file Testing if the setup still works: simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) 10.2 Fixing tiledrain run_this_chapter = FALSE Problem: tiledrain=1 in codes.bsn causes crash. Reason: Some HRUs have drains, but are on a shallow soil type. This causes the drains to be below the soil, leading to a (nondescript) error. Likely source of error: Misalignment of the soil and land use map. Discussion of this issue can be found in #53 require(mapview) require(sf) require(stringr) require(dplyr) require(purrr) require(DT) 10.2.1 Finding the problem HRUs Our drains are set to 80cm depth. Which of our soils are less than that? usersoil &lt;- readr::read_csv(&quot;model_data/input/soil/usersoil_26_v2.csv&quot;, show_col_types = F) shallow_soils &lt;- usersoil$SNAM[which(usersoil$SOL_ZMX &lt; 800)] print(shallow_soils) ## [1] &quot;ANT&quot; &quot;END&quot; &quot;HUM&quot; &quot;Mon&quot; &quot;RG&quot; &quot;SDs&quot; Which of our HRUs use these soils, and are drained? hru_data &lt;- read.table( &quot;model_data/cs10_setup/swat_input/hru-data.hru&quot;, skip = 1, header = T, sep = &quot;&quot;) shallow_hrus &lt;- which(hru_data$soil %in% shallow_soils) drained_hrus &lt;- grepl(x = hru_data$lu_mgt, pattern = &quot;_drn&quot;) %&gt;% which() # Intersection of HRUs which are both shallow and drained problem_hrus &lt;- base::intersect(shallow_hrus, drained_hrus) length(problem_hrus) ## [1] 6 Just 6 HRUs have an issue. Lets get some info on these 6: problem_hru_data &lt;- hru_data[problem_hrus,] datatable(problem_hru_data) We need to link the soil type to the hru vector file hru_shp &lt;- read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/hru.shp&quot;) hru_soil &lt;- hru_data %&gt;% dplyr::select(soil, name) hru_shp &lt;- left_join(hru_shp, hru_soil, by = &quot;name&quot;) And separate out our problematic HRUs and affected LUs problem_shps &lt;- hru_shp %&gt;% filter(name %in% problem_hru_data$name) problem_hru_ids &lt;- problem_hru_data$lu_mgt %&gt;% str_remove(&quot;_drn_lum&quot;) problem_lus &lt;- hru_shp %&gt;% filter(type %in% problem_hru_ids) Next we remove the problem HRUs and affect LUs from our hru_shp (this is for the upcoming map, to remove overlaps). hru_shp &lt;- hru_shp %&gt;% filter((hru_shp$name %in% problem_lus$name) == FALSE) hru_shp &lt;- hru_shp %&gt;% filter((hru_shp$name %in% problem_shps$name) == FALSE) We also remove the problem HRUs from the affected LUs problem_lus &lt;- problem_lus %&gt;% filter((problem_lus$name %in% problem_shps$name) == FALSE) Data processing done, lets have a look: hru_map &lt;-mapview( hru_shp, map.types = &quot;Esri.WorldImagery&quot;, color = &quot;green&quot;, lwd = 1, legend = FALSE, zcol = &quot;soil&quot;, alpha.region = 0 ) problem_map &lt;- mapview( problem_shps, color = &quot;red&quot;, lwd = 3, legend = FALSE, zcol = &quot;name&quot;, layer.name = &quot;Problem HRUs&quot;, alpha.region = 0 ) problem_lus_map &lt;- mapview( problem_lus, color = &quot;orange&quot;, lwd = 1, legend = FALSE, zcol = &quot;name&quot;, layer.name = &quot;Problem LUs&quot;, alpha.region = 0 ) full_map &lt;- hru_map + problem_lus_map+ problem_map full_map Resolution to this problem has been discussed in issue #53 We will extract the hru and landuse file to modify further. lum_fixed &lt;- read.table(&quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot;, skip = 1, sep = &quot;&quot;, header = T) %&gt;% tibble() hru_fixed &lt;- read.table(&quot;model_data/cs10_setup/swat_input/hru-data.hru&quot;, skip = 1, sep = &quot;&quot;, header = T) %&gt;% tibble() 10.2.2 Fixing the problem HRUs hru0704 has the wrong soil type for its landuse a_037f_2_drn_lum, which should be STV instead of SDs hru_fixed$soil[which(hru_fixed$name==&quot;hru0704&quot;)] = &quot;STV&quot; hru1157 has the wrong soil type for its landuse a_067f_1, which should be STR instead of SDs hru_fixed$soil[which(hru_fixed$name==&quot;hru1157&quot;)] = &quot;STR&quot; hru1233 has the wrong soil type for its landuse a_074f_1, which should be STV instead of Mon hru_fixed$soil[which(hru_fixed$name==&quot;hru1233&quot;)] = &quot;STV&quot; hru1755 has the wrong land use, while it was classified as a field, it should actually be a road. This is due to misaligned LU maps and should be fixed at the source hru_fixed$lu_mgt[which(hru_fixed$name==&quot;hru1755&quot;)] = &quot;utrn_lum&quot; hru3523 has the wrong soil type for its landuse a_053f_1, which should be STR instead of RG hru_fixed$soil[which(hru_fixed$name==&quot;hru3523&quot;)] = &quot;STR&quot; hru5861 of landuse a_210_f2 does not share the same soil (end moraine) as the rest of its field. hru_fixed$soil[which(hru_fixed$name==&quot;hru5861&quot;)] = &quot;STV&quot; 10.2.2.1 Legacy problems The following HRUs were problematic before we updated our soil map (see issue #68), now they are no longer problematic, therefore the code has been commented out. hru0218 with landuse a_008f_2_drn_lum – Remove the drains since it is an isolated LU #lum_fixed$tile[which(lum_fixed$name==&quot;a_008f_2_drn_lum&quot;)] = &quot;null&quot; hru3637 with landuse a_148f_1_drn_lum – Remove the drains since it is an isolated LU. #lum_fixed$tile[which(lum_fixed$name==&quot;a_148f_1_drn_lum&quot;)] = &quot;null&quot; hru3686 of landuse a_115f_1_drn_lum has the wrong soil type, changing it to \"STlv-sl” of its connected field. #hru_fixed$soil[which(hru_fixed$name==&quot;hru3686&quot;)] = &quot;STlv-sl&quot; hru4880 of landuse a_182f_5_drn_lum has the wrong soil type for the land use. Changing to the “ARdy” of neighboring fields #hru_fixed$soil[which(hru_fixed$name==&quot;hru4880&quot;)] = &quot;ARdy&quot; hru5033 with isolated landuse a_112f_1_drn_lum can have its drains removed. #lum_fixed$tile[which(lum_fixed$name==&quot;a_112f_1_drn_lum&quot;)] = &quot;null&quot; hru0692 of isolated land use a_037f_1_drn_lum can safely has its drains removed #lum_fixed$tile[which(lum_fixed$name==&quot;a_037f_1_drn_lum&quot;)] = &quot;null&quot; hru3314 of landuse a_060f_2_drn_lum has the wrong soil type. It should be STlv-sl, not STlen-rt-sl. The same is true for hru3376. #hru_fixed$soil[which(hru_fixed$name==&quot;hru3314&quot;)] = &quot;STlv-sl&quot; #hru_fixed$soil[which(hru_fixed$name==&quot;hru3376&quot;)] = &quot;STlv-sl&quot; hru2784 (and the within hru3367) is a sizable field with landuse a_072f_1_drn_lum and soil RGlen-dy-ar. Tough call here, but best bet is probably to assign soil of the other major field it shares a landuse with (STlv-sl) #hru_fixed$soil[which(hru_fixed$name==&quot;hru2784&quot;)] = &quot;STlv-sl&quot; #hru_fixed$soil[which(hru_fixed$name==&quot;hru3367&quot;)] = &quot;STlv-sl&quot; hru3465 of LU a_146f_2_drn_lum has the wrong soil for the greater field. Changing it from RGlep-dy to STrt-sl #hru_fixed$soil[which(hru_fixed$name==&quot;hru3465&quot;)] = &quot;STrt-sl&quot; hru5214 of isolated land use a_020f_4_drn_lum can safely have its drains removed. #lum_fixed$tile[which(lum_fixed$name==&quot;a_020f_4_drn_lum&quot;)] = &quot;null&quot; hru5926 of landuse a_211f_1_drn_lum has the wrong soil type (Sea-dep-thin), changing to that of the greater field (STrt-sl) #hru_fixed$soil[which(hru_fixed$name==&quot;hru5926&quot;)] = &quot;STlv-sl&quot; hru2633 of isolated LU a_131f_1_drn_lum can safely have its drains removed. #lum_fixed$tile[which(lum_fixed$name==&quot;a_131f_1_drn_lum&quot;)] = &quot;null&quot; 10.2.3 Checking changes Do we now have 0 problem HRUs? shallow_hrus &lt;- which(hru_fixed$soil %in% shallow_soils) drained_hrus &lt;- grepl(x = hru_fixed$lu_mgt, pattern = &quot;_drn&quot;) %&gt;% which() # Intersection of HRUs which are both shallow and drained problem_hrus &lt;- base::intersect(shallow_hrus, drained_hrus) length(problem_hrus) ## [1] 0 Yes 10.2.4 Writing modified changes Removing the appended columns, and duplicates from the left-join, adding header and writing to modified input files directory. lu_header &lt;- paste(&quot;lu table after fixing up by OPTAIN-CS10 workflow in section 9.2&quot;) write.table(lu_header, &quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot;, quote = F, row.names = F, col.names = F ) write.table(lum_fixed, file = &quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot;, sep = &quot;\\t&quot;, quote = F, append = T, row.names = F, col.names = T) ## Warning in write.table(lum_fixed, file = ## &quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot;, : appending column names to ## file Six drains have been removed. hru_header &lt;- paste(&quot;header header header, delete me and regret it forever! fixed soil types by cs10 workflow in section 9.2&quot;) write.table(hru_header, &quot;model_data/cs10_setup/swat_input_mod/hru-data.hru&quot;, quote = F, row.names = F, col.names = F ) write.table(hru_fixed, file = &quot;model_data/cs10_setup/swat_input_mod/hru-data.hru&quot;, sep = &quot;\\t&quot;, quote = F, append = T, row.names = F, col.names = T) ## Warning in write.table(hru_fixed, file = ## &quot;model_data/cs10_setup/swat_input_mod/hru-data.hru&quot;, : appending column names ## to file Ten soil types have been changed. 10.2.5 Re-enabling tiledrain Load from modified input files old_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/codes.bsn&quot; new_path &lt;- &quot;model_data/cs10_setup/swat_input_mod/codes.bsn&quot; codes_bsn &lt;- readLines(old_path) header &lt;- codes_bsn[1] header &lt;- paste(&quot;modified by CS10 workflow in section 9.2&quot;) codes_df &lt;- read.table(old_path, skip = 1, header = T, sep = &quot;&quot;, colClasses = &quot;character&quot;) Re-enable by setting to 1 codes_df$tiledrain &lt;- 1 Writing to modified input files directory. write.table(header, file = new_path, col.names = F, row.names = F, quote = F) write.table( codes_df, file = new_path, col.names = T, append = T, quote = F, sep = &quot; &quot;, row.names = F ) ## Warning in write.table(codes_df, file = new_path, col.names = T, append = T, : ## appending column names to file 10.2.6 Testing SWAT+ Now we can see if the model runs source(&quot;model_data/code/test_swat_mod.R&quot;) simout &lt;- test_swat_mod() cat(tail(simout), sep = &quot;\\n&quot;) Does it work? No. Of course not because tiledrain=1 is current issue in SWAT+. 10.3 Setting Soil Phosphorous Initial Conditions # require(dplyr) # require(sf) # require(mapview) # require(ggplot2) # require(stringr) # require(DT) require(magrittr) ## Loading required package: magrittr ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## The following objects are masked from &#39;package:terra&#39;: ## ## extract, inset ## The following object is masked from &#39;package:raster&#39;: ## ## extract sf_theme &lt;- ggplot2::theme(axis.text.x=ggplot2::element_blank(), #remove x axis labels axis.ticks.x=ggplot2::element_blank(), #remove x axis ticks axis.text.y=ggplot2::element_blank(), #remove y axis labels axis.ticks.y=ggplot2::element_blank() #remove y axis ticks ) In this section we will set the initial value for labile phosphorous in the soil. SWAT+ reads in inital P values from lab_p which is in a file called nutrients.sol which is read by the nutrients column in the file soil_plant.ini which is read by the soil_plant_init column of the hru-data.hru. We have a map containing the initial values, created by Dominika using P-AL method, here: pal &lt;- sf::read_sf(&quot;model_data/input/nutrients/cs10_p_al.shp&quot;) ggplot2::ggplot()+ggplot2::geom_sf(pal, mapping = ggplot2::aes(fill = pal))+sf_theme It does not have full coverage of the basin: ws &lt;- sf::read_sf(&quot;model_data/input/shape/cs10_basin.shp&quot;) ggplot2::ggplot() + ggplot2::geom_sf(ws, mapping = ggplot2::aes(fill = &quot;Watershed&quot;))+ ggplot2::geom_sf(pal, mapping = ggplot2::aes(fill = &quot;P map&quot;)) + sf_theme Now we need to calculate which HRUs have which amount of lab_p . hru_shp &lt;- sf::read_sf(&quot;model_data/cs10_setup/optain-cs10/data/vector/hru.shp&quot;) # set correct CRS pal &lt;- sf::st_transform(pal, sf::st_crs(hru_shp)) # join hru_pal &lt;- sf::st_intersection(hru_shp, pal) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries # average per hru hru_pal_avg &lt;- hru_pal %&gt;% dplyr::group_by(id) %&gt;% dplyr::summarize(p_al_mean = mean(pal)) mapview::mapview(hru_pal_avg, zcol = &quot;p_al_mean&quot;) We are finished with the spatial work. We now just need to extract the ID and the lab_p. # drop geometry and grab only the ID and lab_p hru_p_df_raw &lt;- hru_pal_avg %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(id, p_al_mean) # left join to a complete dataframe hru_full &lt;- tibble::tibble(id = hru_shp$id) hru_p_df &lt;- dplyr::left_join(hru_full, hru_p_df_raw , by = &quot;id&quot;) # sort by ID hru_p_df &lt;- dplyr::arrange(hru_p_df, id) We have 84 hrus with no data. which(hru_p_df$p_al_mean %&gt;% is.na()) %&gt;% length() ## [1] 84 We will fix this by giving the average value of the land use. Calculating average values of land use: lu_shp &lt;- sf::read_sf(&quot;model_data/input/land/CS10_LU.shp&quot;) hru_lu &lt;- sf::st_intersection(hru_pal_avg, lu_shp) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries generics &lt;- c(&quot;frst&quot;, &quot;past&quot;, &quot;rngb&quot;, &quot;urml&quot;, &quot;utrn&quot;, &quot;watr&quot;, &quot;wetf&quot;) hru_lu_generic &lt;- hru_lu %&gt;% dplyr::filter(type %in% generics) hru_lu_agri &lt;- hru_lu %&gt;% dplyr::filter((type %in% generics)==FALSE) %&gt;% dplyr::summarize (mean = mean(p_al_mean)) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::pull() %&gt;% round(2) generic_p_avg &lt;- hru_lu_generic %&gt;% dplyr::group_by(type) %&gt;% dplyr::summarise(type_p = mean(p_al_mean)) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::mutate(type_p = round(type_p, 2)) lu_p_avg_df &lt;- rbind(generic_p_avg, c(&quot;agri&quot;, hru_lu_agri)) DT::datatable(lu_p_avg_df) Now we need to find the types of the hrus with the missing data and give them the average value, then add those values to the full dataframe. missing &lt;- hru_p_df$id[which(hru_p_df$p_al_mean %&gt;% is.na())] missing_df &lt;- hru_p_df %&gt;% dplyr::filter(id %in% missing) missing_df &lt;- missing_df %&gt;% dplyr::arrange(id) types &lt;- hru_shp %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(id %in% missing) %&gt;% dplyr::arrange(id) types$type[which((types$type %in% generics) == FALSE)] = &quot;agri&quot; missing_df$type &lt;- types$type missing_df &lt;- dplyr::left_join(missing_df, lu_p_avg_df, by = &quot;type&quot;) %&gt;% dplyr::mutate(p_al_mean = type_p) %&gt;% dplyr::select(id, p_al_mean) hru_p_df_no_na &lt;- hru_p_df %&gt;% dplyr::filter(p_al_mean %&gt;% is.na() == FALSE) hru_p_df &lt;- rbind(hru_p_df_no_na, missing_df) %&gt;% dplyr::arrange(id) Now we need a soil_plant_ini class for each hru, as well as a nutrient class for each hru. The soil_plant.ini file requires the following columns: hru_names &lt;- paste0(&quot;spi_hru&quot;,hru_p_df$id) sw_frac &lt;- &quot;0.00000&quot; nutrients &lt;- paste0(&quot;sn_hru&quot;, hru_p_df$id) pest &lt;- &quot;null&quot; path &lt;- &quot;null&quot; hmet &lt;- &quot;null&quot; salt &lt;- &quot;null&quot; soil_plant_ini &lt;- tibble::tibble(name = hru_names, sw_frac = sw_frac, nutrients = nutrients, pest = pest, path = path, hmet = hmet, salt = salt) write.table(x = soil_plant_ini, file = &quot;model_data/cs10_setup/swat_input_mod/soil_plant.ini&quot;, quote = F, row.names = F) Now we need to add a header and reformat the column names. header = &quot;soil_plant.ini: last updated by CS10 workflow section XX&quot; txt &lt;- readLines(&quot;model_data/cs10_setup/swat_input_mod/soil_plant.ini&quot;) txt &lt;- c(header, txt) writeLines(txt, &quot;model_data/cs10_setup/swat_input_mod/soil_plant.ini&quot;) And now we need to modify the nutrients.sol file name &lt;- paste0(&quot;sn_hru&quot;, hru_p_df$id) lab_p = hru_p_df$p_al_mean cpath &lt;- &quot;model_data/cs10_setup/swat_input_mod/nutrients.sol&quot; nutrients_df &lt;- read.table(cpath, skip = 1, fill = T, header = T, colClasses = rep(&quot;character&quot;, 13)) %&gt;% tibble::tibble() nutrients_df &lt;- nutrients_df[1,] nutrients_df_add &lt;- tibble::tibble( name = name, exp_co = nutrients_df$exp_co, lab_p = lab_p, nitrate = nutrients_df$nitrate, fr_hum_act = nutrients_df$fr_hum_act, hum_c_n = nutrients_df$hum_c_n, hum_c_p = nutrients_df$hum_c_p, inorgp = nutrients_df$inorgp, watersol_p = nutrients_df$watersol_p, h3a_p = nutrients_df$h3a_p, mehlich_p = nutrients_df$mehlich_p, bray_strong_p = nutrients_df$bray_strong_p, description = &quot;&quot; ) nutrients_df_full &lt;- rbind(nutrients_df, nutrients_df_add) write.table(nutrients_df_full, file = cpath, quote = F, row.names = F) header = &quot;nutrients.sol: last updated by CS10 workflow section XX&quot; txt &lt;- readLines(cpath) txt &lt;- c(header, txt) writeLines(txt, cpath) Now we need to assign the HRU table in hru-data.hru the correct soil_plant_init column from. cpath &lt;- &quot;model_data/cs10_setup/swat_input_mod/hru-data.hru&quot; hru_data &lt;- read.table(cpath, fill = T, header = T, skip = 1) %&gt;% tibble::tibble() hru_data$soil_plant_init &lt;- hru_names write.table(hru_data, file = cpath, quote = F, row.names = F) txt &lt;- readLines(cpath) txt &lt;- c(&quot;hru-data last modified by CS10 workflow @ section XX&quot;, txt) writeLines(txt, con = cpath) DT::datatable(head(hru_data)) Related issues: #88, #46, #19 "],["swatfarmr-management-schedules.html", "Section 11 SWATFarmR Management Schedules 11.1 Pre-processing crop rotation 11.2 Initializing FarmR 11.3 Calculating Antecedent Precipitation Index 11.4 Scheduling Operations", " Section 11 SWATFarmR Management Schedules WIP: Every field in the OPTAIN SWAT setup needs its own management. We will use the SWATFarmR package to define it. run_this_chapter = FALSE Pre-requirements The creation of management schedules with SWATfarmR requires a SWAT+ model setup created by the SWATbuildR package. We also require the following package and functions: require(DT) require(ggplot2) require(readr) require(stringr) require(magrittr) #remotes::install_github(&quot;chrisschuerz/SWATfarmR&quot;) require(SWATfarmR) require(sf) require(tidyverse) require(lubridate) require(reshape2) require(remotes) require(dplyr) require(data.table) require(DT) source(&#39;model_data/code/functions_write_SWATfarmR_input.R&#39;) 11.1 Pre-processing crop rotation This OPTAIN-provided workflow pre-processes our crop rotation data into a SWATfarmR compatible format. Authored by Michael Strauch, modified by Moritz Shore 11.1.1 Input files 11.1.1.1 Land use map with crop information This map has the following requirements: The map must contain the land use of each hru. In case of cropland, the names must be unique for each field (e.g., ‘field_1’, ‘field_2’, etc.) The map must also contain crop infos for the period 1988 to 2020 (or 2021 if crop info available). This requires an extrapolation of the available crop sequences (the sequences derived from remote-sensing based crop classification or local data). The extrapolated crop sequence for 33 years will be also used for running climate scenarios and must not contain any gaps. That means, gaps have to be closed manually! The year columns must be named y_1988, y_1989, etc. The crop infos for each year must match the crop_mgt names in the management operation schedules (provided in a .csv file, see below) see also section 4.1 of the modelling protocol (Schürz et al. 2022) lu_shp &lt;- &#39;model_data/input/crop/land_with_cr.shp&#39; lu &lt;- st_drop_geometry(read_sf(lu_shp)) datatable( lu %&gt;% head(50), extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE) ) 11.1.1.2 Management operation schedules for each crop ..or, if available, crop-management type. All schedules must be compiled in one csv file (see example in demo data and study also section 4.2 of the modelling protocol) ‘crop_mgt’ must start with the 4-character SWAT crop names (any further management specification is optional). Each schedule must contain a ‘skip’ line to indicate the change of years. The ‘skip’ line should never be the last line of a schedule. mgt_csv &lt;- &#39;model_data/input/management/mgt_crops_CS10.csv&#39; mgt_crop &lt;- read.csv(mgt_csv) datatable(mgt_crop, extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE)) 11.1.1.3 Management operation schedules for generic land-use classes Usually all non-cropland classes with vegetation cover here, all schedules must be provided already in the SWATfarmR input format. lu_generic_csv &lt;- &#39;model_data/input/management/farmR_generic_CS10.csv&#39; # generic land use management .csv table mgt_generic &lt;- read.csv(lu_generic_csv) datatable(mgt_generic) 11.1.2 Settings 11.1.2.1 Simulation Period start_y &lt;- 2010 # starting year (consider at least 3 years for warm-up!) end_y &lt;- 2020 # ending year 11.1.2.2 Prefix of cropland hrus all names of hrus with a crop rotation must begin with this prefix in column ‘lu’ of your land use hru_crops &lt;- &#39;a_&#39; 11.1.2.3 Multi-year farmland grass Did you define any multi-year farmland grass schedules? ‘y’ (yes), ‘n’ (no) m_yr_sch_existing &lt;- &#39;n&#39; If yes, define also the following variables. If not, skip next four lines crop_myr &lt;- &#39;past&#39; # name of your farmland grass Maximum number of years farmland grass can grow before it is killed (should be &lt;8) max_yr &lt;- 5 Do your multi-year farmland grass schedules consider the type of the following crop (summer or winter crop)? (e.g., a ’_1.5yr’ schedule with a kill op in spring allows for planting a summer crop immediately afterwards) If yes, you must define your summer crops crop_s &lt;- c(&#39;sgbt&#39;,&#39;csil&#39;,&#39;barl&#39;) Do your summer crop schedules usually start with an operation in autumn (e.g. tillage)? To combine them with farmland grass, it is necessary that you provide ‘half-year-schedules’ (‘half-year-schedules’ are additional summer crop schedules without operations in autumn) The adapted schedules should be added to the crop management table with suffix ’_0.5yr’ (e.g. ‘csil_0.5yr’) If additional ‘half-year-schedules’ are not needed, because your normal summer crop schedules do not start in autumn, type ‘n’ additional_h_yr_sch_existing &lt;- &#39;n&#39; # &#39;y&#39; (yes), &#39;n&#39; (no) 11.1.3 Checks Check for correct positioning of ‘skip’ line check_skip &lt;- check_skip_position() Check for date conflicts in single crop schedules check_date_conflicts1() Build schedules for crop sequences (Messages disabled) rota_schedules &lt;- build_rotation_schedules() Check for date conflicts in the full rotation schedule. (Messages disabled) check_date_conflicts2() Solve minor date conflicts (where only a few days/weeks are overlapping) (Messages disabled) rota_schedules &lt;- solve_date_conflicts() Check again for date conflicts (Messages disabled) check_date_conflicts2() 11.1.4 Writing input data Write the SWAT farmR input table write_farmR_input() The output of this pre-processing stage is loaded in from this file: farmR_input &lt;- readr::read_csv(&quot;model_data/input/management/farmR_input.csv&quot;, show_col_types = F) datatable( head(farmR_input, 50), extensions = &quot;Scroller&quot;, options = list(scrollY = 200, scroller = TRUE) ) Micha’s preperation script prints out land use names which will not work for us, we need to fix them. This is how our land-use names look: lus &lt;- read.table(&quot;model_data/cs10_setup/swat_input_mod/landuse.lum&quot;, skip = 1, header = T) datatable(lus) As we can see, we have land uses with either the suffix _drn_lum or just _lum. However, our farmR input only has _lum. We need to fix that: fix &lt;- farmR_input fix$land_use &lt;- farmR_input$land_use %&gt;% str_remove_all(&quot;_lum&quot;) # remove duplicates #fix2 &lt;- distinct(fix) # does this work? or are they read sequentially? drained_fields &lt;- lus$name[which(grepl(x = lus$name, &quot;_drn_&quot;))] %&gt;% str_remove(&quot;_drn&quot;) %&gt;% str_remove(&quot;_lum&quot;) non_drained_fields &lt;- lus$name[which(!(grepl(x = lus$name, &quot;_drn_lum&quot;)))] %&gt;% str_remove(&quot;_drn&quot;) %&gt;% str_remove(&quot;_lum&quot;) fixed_drained &lt;- fix[which(fix$land_use %in% drained_fields),] fixed_non_drained &lt;- fix[which(fix$land_use %in% non_drained_fields),] fixed_non_drained$land_use &lt;- paste0(fixed_non_drained$land_use, &quot;_lum&quot;) fixed_drained$land_use &lt;- paste0(fixed_drained$land_use, &quot;_drn_lum&quot;) full_fix &lt;- rbind(fixed_non_drained, fixed_drained) datatable(full_fix) write_csv(full_fix, file =&quot;model_data/input/management/farmR_input2.csv&quot;, na = &quot;&quot;) 11.2 Initializing FarmR We will create a dedicated directory for our SWATfarmR and copy in our latest text input files. dir.create(&quot;model_data/cs10_setup/swat_farmR/&quot;, showWarnings = F) sta_files &lt;- list.files(&quot;model_data/cs10_setup/optain-cs10/&quot;, pattern = &quot;sta_&quot;, full.names = T) cli_files &lt;- list.files(&quot;model_data/cs10_setup/optain-cs10/&quot;, pattern = &quot;.cli&quot;, full.names = T) input_files_base &lt;- list.files(&quot;model_data/cs10_setup/swat_input/&quot;, full.names = T) source_files &lt;- c(sta_files, cli_files, input_files_base) status &lt;- file.copy(from = source_files, to = &quot;model_data/cs10_setup/swat_farmR/&quot;, overwrite = T) # overwriting the base files with our modifications input_files_mod &lt;- list.files(&quot;model_data/cs10_setup/swat_input_mod/&quot;, full.names = T) status_mod &lt;- file.copy(from = input_files_mod, to = &quot;model_data/cs10_setup/swat_farmR/&quot;, overwrite = T) If the FarmR has never been initialized, then use new_farmr() and read_management(), otherwise load_farmr(). project_path &lt;- &quot;model_data/cs10_setup/swat_farmr/&quot; file.remove(&quot;model_data/cs10_setup/swat_farmR/cs10.farm&quot;) new_farmr(project_name = &quot;cs10&quot;, project_path = project_path ) #load_farmr(...) cs10$read_management(file = &quot;model_data/input/management/farmR_input2.csv&quot;) 11.3 Calculating Antecedent Precipitation Index TODO loading API from section REF # using temp API # TODO add real API # Load dplyr. We will use functions such as &#39;mutate&#39; and &#39;select&#39;. library(dplyr) # Extract the precipitation from the farmr project pcp &lt;- cs10$.data$variables$pcp # Extract the hydrologic soil group values for all HRUs hsg &lt;- select(cs10$.data$meta$hru_attributes, hru, hyd_grp) # Calculate api values for the hsg classes A to D api_A &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 1) api_B &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 0.8) api_C &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 0.7) api_D &lt;- variable_decay(variable = pcp, n_steps = -5, decay_rate = 0.5) # Bind the data together into one api table and name them with the hsgs api &lt;- bind_cols(api_A, api_B, api_C, api_D) names(api) &lt;- c(&#39;api_A&#39;, &#39;api_B&#39;, &#39;api_C&#39;, &#39;api_D&#39;) # To add the variable to the farmR you have to tell it which variables are # assigned to which HRUs hru_asgn &lt;- mutate(hsg, api = paste0(&#39;api_&#39;, hyd_grp)) %&gt;% select(hru, api) # Add the variable api to the farmR project cs10$add_variable(data = api, name = &#39;api&#39;, assign_unit = hru_asgn, overwrite = T) TODO 11.4 Scheduling Operations cs10$schedule_operations(start_year = 2011, end_year = 2020, n_schedule = 2, replace = &quot;all&quot;) 11.4.1 Write Operations We cannot write from 1988 as it is limited to our climate data, which currently only spans back to 2010 (2011?) cs10$write_operations(start_year = 2011, end_year = 2020) “Writing management files: - Loading scheduled operations: 100% - Preparing ‘hru-data.hru’ - Preparing ‘landuse.lum’ - Preparing ‘schedule.mgt’ - Preparing ‘plant.ini’ - Writing files - Updating ‘time.sim’ - Updating ‘file.cio’ Finished writing management files in 5M 40S” Make sure to use the files in the SWATfarmR directory going forward. Test to see if the setup still works: # No it does not # TODO Temp fix: ### Change tiledrain to 0 Related Issues FarmR 3.1 Issues UFZ #28 FarmR 3.1 Fix UFZ #27 on the UFZ gitlab FarmR 3.0 issues – #64 Update FarmR to 3.0 - #56 No Plant ET - #48 Schedule Ops 1988-2020 - #51 Re-extrapolate the crop rotation - #47 References "],["references.html", "References", " References "]]
