---
editor_options: 
  markdown: 
    wrap: 72
---

# Antecedent Precipitation Index

```{r knitengines, include=FALSE}
knitr::knit_engines$set(python = reticulate::eng_python)
```

## Introduction

SWATFarmR cannot account for operations based on values modeled by
SWAT+, such as soil moisture. This is because FarmR does not run the
model itself, and creates and predefines all the management schedules
based on measured climate data. This means we need to relate our
measured climate data to our soil moisture (Because soil moisture is an
important factor in planning agricultural operations). One method to do
this is called the Antecedent Precipitation Index. Which is what we will
creating for CS10 in this part.

As a heads up, this section requires Python!

### Pre-requirements

Required R packages

```{r, message=FALSE}
require(readr)
require(dplyr)
require(hydroGOF)
library(reticulate) # Required Python packages: "pandas", "numpy", "scipy"

source("model_data/code/plot_api.R")
source("model_data/code/calc_api.R")

```

### Setting up Python for R/Rmarkdown

A quick (non-evaluated) code block to show how a python environment can
be created for use in Rmarkdown/Reticulate on your local machine.

```{r, include = TRUE, eval=FALSE}
# install miniconda 
install_miniconda() 

# create conda environment for your project
conda_create("cs10_env", # environment name 
              packages = c("pandas", "numpy", "scipy"), # required packages
              conda = "auto")


conda_python("cs10_env") # returns the .exe python path for your environment

# you need to change your R environment path in this file
usethis::edit_r_environ()

# RETICULATE_PYTHON="YOUR ENVIRONMENT PATH/python.exe"

# restart your R session.

py_config()

# If everything is right, you should get something like:

# python: YOUR ENVIRONMENT PATH/python.exe libpython: YOUR ENVIRONMENT
# PATH/python39.dll pythonhome: YOUR ENVIRONMENT PATH version: 3.9.7 (default,
# Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] Architecture: 64bit numpy:
# YOUR ENVIRONMENT PATH\lib\site-packages\numpy numpy_version: 1.21.2

# After this it should work :)
```

## Antecedent Precipitation Index

From [@patrignani2020]

The API is a well-known, parsimonious, recursive model for predicting
soil moisture solely based on precipitation records. The API is commonly
implemented using daily precipitation records, but it is possible to
work at finer temporal scales (e.g. hourly) if both precipitation (model
input) and soil moisture (for validation purposes) are available. The
equation describing the simple version of the model is:

$$API_{t} = \alpha API_{t-1} + P_t$$\
$API_t$: Soil water content at time $t$ (today)

$API_{t-1}$: Soil water content at time $t-1$ (yesterday)

$\alpha$: Loss coefficient. Range between 0 and 1

$P_t$: Precipitation at time $t$ (today)

### Loading data

We will load out data from the previous steps. (ref)

```{r}
logger_data <- read_csv("model_data/field_sites/loggers/clean/logger_data_daily_clean.csv",
                        show_col_types=F)

# we only need SMC for this part
logger_data = logger_data %>% filter(var == "smc")

site_attributes <-
  read_csv("model_data/field_sites/output/field_site_attr_df.csv",
           show_col_types = F)

pcp_data <- read_csv("model_data/field_sites/loggers/clean/pcp_data_clean.csv",
                     show_col_types = F)

head(site_attributes)
```

We will calculate the model per Hydrologic soil group (HSG), and start
with B, which was measured by logger 600410.

```{r}
b_data <- logger_data %>% filter(site == "600410")
```

### Calculating the total water content

We need to calculate the total water content of the profile from the
depth measurements from the following depths:

```{r}
unique(b_data$depth)
```

Following code is modified from @patrignani2020 ([[read
more]{.underline}](https://soilwater.github.io/pynotes-agriscience/notebooks/antecedent_precipitation_index.html))

(This calculation could be improved by considering the soil horizons)

```{r}
a <- b_data %>% filter(depth == "300") %>% select(value)
b <- b_data %>% filter(depth == "450") %>% select(value)
c <- b_data %>% filter(depth == "600") %>% select(value)
d <- b_data %>% filter(depth == "750") %>% select(value)
e <- b_data %>% filter(depth == "900") %>% select(value)

total_water_content <- 
  a * .35 + # from 0 to 35
  b * .15 + # from 35 to 50
  c * .15 + # from 55 to 65
  d * .15 + # from 65 to 80
  e * .20   # from 80 to 100

b_col <- tibble(b_data %>% filter(depth == "300") %>% select(date), total_water_content)

```

And we can add the precipitation data from
\@ref(including-air-temperature-and-precipitation-data)

```{r, hsg_b_twc_plot, fig.align='center', fig.cap="Total water content for logger 600410 (HSG B)"}
b_col <- left_join(b_col, pcp_data, by = "date")

plot_api(obs = b_col, title = "HSG B")

```

Saving the data

```{r}
write_csv(b_col, "model_data/field_sites/output/twc.csv")
```

### Defining API model

The value is capped at the maximum value of the source data (50). This
could be improved by capping site specific.

```{python}
def api_model(P,alpha=0.97,ini=36.8):
    api = [ini]
    for t in range(1,len(P)):
      append_val = api[t-1]*alpha + P[t]
      if append_val > 50:
        append_val = 50
      api.append(append_val)
    
    return api
```

Load in our data:

```{python}
import pandas as pd
df = pd.read_csv('model_data/field_sites/output/twc.csv')
df = df[['date','value','pcp']]
df.head()
```

As a test, we will calculate the predictions with an alpha of 0.95:

```{python}
import pandas as pd
from scipy.optimize import curve_fit

# guess
storage_guessed = api_model(df['pcp'], alpha=0.95, ini=36.8)

# write to text
dates = df[['date']]
guess = pd.DataFrame(dates.values.tolist(),storage_guessed)
guess.to_csv("model_data/field_sites/output/first_guess.csv")

```

Now we can have a look at it in R

```{r}
guess <- read_csv("model_data/field_sites/output/first_guess.csv",
                  show_col_types = F,
                  skip = 1, col_names = c("val", "date"))

plot_api(api_vals = guess, obs = b_col, title = "HSG-B API guessed")
```

Now we optimize the alpha parameter using `scipy` `curve_fit`.

```{python}
par_opt, par_cov = curve_fit(api_model, df['pcp'], df['value'], p0=[0.95,100])

print('Annual mean alpha value is',round(par_opt[0],2))
```

Now we can calculate the API using the optimized parameter.

```{python}
import numpy as np

storage_optimized = api_model(df['pcp'],par_opt[0],par_opt[1])

# Mean Absolute Error
MAE = np.mean(np.abs(df['value'] - storage_optimized))
print('Mean annual error =', round(MAE,1),'cm')
```

And save the data for R

```{python}
dates = df[['date']]
api_optimized = pd.DataFrame(dates.values.tolist(),storage_optimized)
api_optimized.to_csv("model_data/field_sites/output/api_optimized.csv")
```

And have a look in R:

```{r}
optimized <- read_csv("model_data/field_sites/output/api_optimized.csv",
                      show_col_types = F,
                      skip = 1, col_names = c("val", "date"))

plot_api(api_vals = optimized, obs = b_col, title = "HSG-B API optimized")
```

Not great, but also not surprising considering no account for snow melt
has been done, nor do we have good logger data.

We will define this workflow in a function `calc_api.R` and run it for
the next loggers.

```{r}
calc_api(logger_id = "600409")
```

This is the logger with broken data, so no surprise that the optimizer
failed.

```{r}
calc_api(logger_id = "600411")
```

```{r}
calc_api(logger_id = "600413")
```

## Conclusions

For HSG B we got an alpha of **0.98**, for C we have two samples, both
of which got **0.99**, and D has two samples as well, but we currently
only have data for one. For the one D site we had, the logger data was
very poor and the **covariance could not be estimated**.

Overall, the API does not seem to be a great estimator of soil moisture
**for our specific case**. Our logger data is poor and has been cut
short due to sensor malfunctions. With a **longer time series** covering
more seasons, it is likely that the curve fitting could be improved
upon, and the overall dynamics could be appraised.;

Secondly, and more critically: the API does not account for **snow
melt**, which plays a significant role in a Boreal catchments such as
CS10. You can see this in action (I think) in March where the logger
data steadily goes up, while the API steadily goes down. Snow melt could
surely be accounted for by adding *another* model to estimate it -- the
question is more if it is worth it, considering the poor underlying data
of the loggers.

Another complication is, if SWATFarmR does not account for snow melt in
its management scheduling, then there is no point in estimating it. And
if snow melt is not incorporated into the scheduling, then the operation
timing will not be very accurate anyway (since snow melt is such a
dominating factor in operation timing).

Is what I wrote here correct Csilla?
